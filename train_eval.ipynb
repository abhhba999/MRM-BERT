{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;import pandas as pd;import os,sys;\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "from utils import read_data,seq2kmer,read_fasta;from collections import Counter\n",
    "from sklearn.utils import shuffle;from pycaret.classification import *\n",
    "import torch;from torch import nn, Tensor;import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE=128\n",
    "mod_type=\"m6Am\"# just for exsample\n",
    "train_type=\"normal\"\n",
    "random_seed=123\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_fp16=\"--fp16\"#if you don't install apex,please delete this line, if you want to use apex,please install apex first following the instructions on DNABERT GitHub Repository\n",
    "#Please Download the model file from the url in readme.md and put it in the same directory as this script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fintune Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目录已存在\n",
      "12/20/2023 22:23:35 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: True\n",
      "12/20/2023 22:23:35 - INFO - transformers.configuration_utils -   loading configuration file ./models/bert/m6Am/config.json\n",
      "12/20/2023 22:23:35 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": \"dnaprom\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"num_rnn_layer\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"rnn\": \"lstm\",\n",
      "  \"rnn_dropout\": 0.0,\n",
      "  \"rnn_hidden\": 768,\n",
      "  \"split\": 0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 69\n",
      "}\n",
      "\n",
      "============================================================\n",
      "<class 'transformers.tokenization_dna.DNATokenizer'>\n",
      "12/20/2023 22:23:45 - INFO - transformers.tokenization_utils -   loading file https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-3/vocab.txt from cache at /root/.cache/torch/transformers/e1e7221d086d0af09215b2c6ef3ded41de274c79ace1930c48dfce242a7b36fa.b24b7bce4d95258cccdbc46b651c8283db3a0f1324fb97567c8b22b19970f82c\n",
      "12/20/2023 22:23:45 - INFO - transformers.modeling_utils -   loading weights file ./models/bert/m6Am/pytorch_model.bin\n",
      "12/20/2023 22:23:47 - INFO - __main__ -   finish loading model\n",
      "12/20/2023 22:23:48 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, attention_probs_dropout_prob=0.1, beta1=0.9, beta2=0.999, cache_dir='', config_name='', data_dir='./dataset/bert/classic/m6Am', device=device(type='cuda'), do_ensemble_pred=False, do_eval=True, do_lower_case=False, do_predict=False, do_train=True, do_visualize=False, early_stop=10, eval_all_checkpoints=False, evaluate_during_training=True, fp16=True, fp16_opt_level='O1', gradient_accumulation_steps=1, hidden_dropout_prob=0.1, learning_rate=3e-05, local_rank=-1, logging_steps=100, max_grad_norm=1.0, max_seq_length=101, max_steps=-1, model_name_or_path='./models/bert/m6Am', model_type='dna', n_gpu=1, n_process=40, no_cuda=False, num_rnn_layer=2, num_train_epochs=25.0, output_dir='./results/your_models/bert/classic/m6Am', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=32, per_gpu_pred_batch_size=8, per_gpu_train_batch_size=32, predict_dir=None, predict_scan_size=1, result_dir=None, rnn='lstm', rnn_dropout=0.0, rnn_hidden=768, save_steps=100, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, task_name='dnaprom', tokenizer_name='dna3', visualize_data_dir=None, visualize_models=None, visualize_train=False, warmup_percent=0.1, warmup_steps=0, weight_decay=0.01)\n",
      "12/20/2023 22:23:48 - INFO - __main__ -   Creating features from dataset file at ./dataset/bert/classic/m6Am\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   LOOKING AT ./dataset/bert/classic/m6Am/train.tsv\n",
      "finish loading examples\n",
      "number of processes for converting feature: 40\n",
      "1 processor started !\n",
      "2 processor started !\n",
      "3 processor started !\n",
      "4 processor started !\n",
      "5 processor started !\n",
      "6 processor started !\n",
      "7 processor started !\n",
      "8 processor started !\n",
      "9 processor started !\n",
      "10 processor started !\n",
      "11 processor started !\n",
      "12 processor started !\n",
      "13 processor started !\n",
      "14 processor started !\n",
      "15 processor started !\n",
      "16 processor started !\n",
      "17 processor started !\n",
      "18 processor started !\n",
      "19 processor started !\n",
      "20 processor started !\n",
      "21 processor started !\n",
      "22 processor started !\n",
      "23 processor started !\n",
      "24 processor started !\n",
      "25 processor started !\n",
      "26 processor started !\n",
      "27 processor started !\n",
      "28 processor started !\n",
      "29 processor started !\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "30 processor started !\n",
      "31 processor started !\n",
      "32 processor started !\n",
      "33 processor started !\n",
      "34 processor started !\n",
      "35 processor started !12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "\n",
      "36 processor started !\n",
      "37 processor started !\n",
      "38 processor started !\n",
      "39 processor started !\n",
      "40 processor started !\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 44 35 64 51 63 46 44 35 64 51 63 48 50 59 32 51 63 45 39 15 48 52 66 59 29 40 19 63 47 48 51 64 51 64 49 56 20 68 68 66 59 32 52 67 64 50 58 27 29 38 10 27 31 46 44 35 63 48 51 62 44 35 63 48 51 63 48 51 63 48 51 63 48 51 63 48 49 56 20 66 59 31 47 48 51 61 39 15 45 40 19 63 45 38 12 36 67 64 51 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-94\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 60 36 68 68 67 64 52 68 68 66 59 32 51 61 40 19 61 37 8 20 67 62 42 27 31 47 47 46 43 30 41 23 15 47 46 43 30 43 30 44 36 68 67 63 46 43 30 43 29 39 13 37 5 7 16 51 62 44 33 56 19 63 47 47 48 51 63 47 47 48 51 62 44 33 56 20 67 63 46 44 34 59 30 44 35 61 40 17 53 6 11 31 45 39 13 40 19 61 37 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-187\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 5 7 13 37 6 12 36 66 58 27 31 46 41 23 15 47 45 37 5 5 5 6 12 33 53 8 19 62 41 24 17 54 9 21 6 9 21 6 9 24 17 53 6 9 23 13 38 11 29 38 11 29 37 8 17 55 13 38 10 25 23 14 44 33 53 7 13 39 15 45 39 16 50 58 27 30 41 22 12 36 66 57 21 5 7 13 39 14 44 33 55 13 38 12 36 65 54 10 26 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-280\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 54 10 27 29 40 18 59 29 40 17 56 18 58 25 22 10 27 31 47 46 42 27 29 37 7 14 44 35 62 42 27 29 40 19 61 37 5 6 12 36 65 53 5 7 15 46 42 27 29 40 19 61 37 6 11 32 51 63 45 40 18 58 26 25 23 15 45 40 19 61 40 17 55 14 42 27 30 43 29 39 13 38 12 33 54 12 33 56 18 60 35 61 38 10 28 34 59 30 41 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 36 66 60 36 66 58 26 27 31 46 42 26 26 28 34 59 30 43 31 46 42 26 25 22 10 25 23 13 38 12 36 68 65 55 14 43 30 44 33 55 13 38 12 34 60 35 63 47 45 38 9 24 19 62 41 24 20 68 66 60 33 55 13 40 18 57 24 20 65 54 11 30 41 23 15 47 48 49 54 9 24 18 57 24 20 68 66 60 36 67 61 40 18 57 24 20 65 54 11 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-373\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 32 52 67 63 46 43 31 46 41 21 5 8 18 60 35 62 44 36 68 65 54 10 25 23 13 40 20 67 64 50 60 33 56 19 63 45 39 14 44 35 61 39 15 47 48 52 67 63 45 37 7 14 42 26 26 27 30 41 22 10 26 27 30 42 26 26 25 24 17 56 18 59 29 40 18 58 26 26 28 36 66 60 36 66 58 26 26 26 28 33 55 14 42 28 36 65 53 6 10 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-95\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 47 47 47 45 40 19 64 51 64 52 67 63 48 51 64 50 59 30 43 31 46 43 31 47 46 43 32 52 67 64 50 58 28 34 59 31 48 51 64 52 67 64 51 64 49 56 19 63 45 39 13 40 19 64 51 64 51 64 52 68 68 67 64 49 56 19 63 45 40 19 64 49 56 17 56 20 68 67 64 51 64 49 56 19 64 52 67 64 52 67 64 51 62 44 35 63 46 44 35 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-466\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 40 18 57 22 10 26 25 22 12 35 63 46 42 27 32 50 59 29 37 5 5 7 13 40 17 53 6 11 30 43 30 43 31 46 42 27 29 37 5 5 5 5 8 18 58 25 23 15 45 38 10 26 28 36 65 54 9 21 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 6 10 26 26 26 26 26 26 26 26 26 26 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-188\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 64 51 62 44 36 68 67 64 52 68 67 63 47 48 50 59 32 51 62 41 22 9 22 9 21 8 20 67 61 40 20 67 64 51 64 52 68 68 68 66 60 36 67 64 51 64 50 59 29 40 18 58 28 35 62 42 27 29 40 19 64 50 59 31 47 48 52 66 60 34 60 36 67 62 44 34 60 35 63 48 50 58 28 36 66 59 31 46 44 34 60 35 64 52 66 59 29 39 14 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-559\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-281\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 8 17 54 9 21 6 10 26 26 28 34 60 34 58 26 26 26 26 26 26 26 26 26 26 26 25 21 5 7 13 38 11 29 40 18 58 26 26 25 21 6 12 35 63 47 46 42 27 29 40 18 57 21 7 14 42 28 36 66 58 27 29 38 10 25 21 5 5 5 5 5 5 5 5 5 5 5 5 5 5 8 20 67 62 44 33 53 8 18 57 21 5 5 5 8 17 56 17 55 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 24 17 54 10 27 31 48 52 68 67 64 52 66 59 32 52 65 56 19 63 48 51 63 48 52 68 65 56 19 62 44 34 57 24 18 58 27 30 43 31 47 48 51 64 52 67 62 43 29 40 17 56 17 53 8 18 57 24 20 67 61 40 17 56 17 56 19 64 52 65 55 15 46 44 36 67 64 52 67 63 48 52 68 67 61 40 19 61 38 12 36 67 64 52 68 68 67 62 44 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 65 56 18 59 32 49 53 8 19 62 44 33 55 14 43 29 37 5 5 7 16 50 57 21 7 16 52 67 63 47 45 37 6 10 28 34 59 31 46 42 28 33 56 17 55 14 42 27 29 38 10 27 31 47 47 45 40 19 61 37 8 19 62 43 29 40 19 64 50 60 34 57 21 7 16 50 60 35 64 51 62 41 22 12 36 65 56 19 63 48 49 53 5 8 18 59 32 51 61 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-652\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 52 68 68 68 67 64 52 68 68 67 63 45 40 20 67 62 41 22 9 21 5 7 15 48 51 63 48 52 66 58 25 24 20 68 68 67 63 48 51 63 45 38 11 31 47 47 46 43 29 40 17 56 19 64 50 59 32 52 68 65 54 9 22 11 32 52 68 66 60 36 67 64 52 67 62 43 32 52 68 65 55 16 52 65 56 20 65 55 16 51 64 51 62 41 24 18 60 34 60 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-374\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-96\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 44 33 53 7 16 51 62 44 36 68 68 68 65 56 20 65 56 18 60 35 61 38 12 36 68 68 65 56 20 68 68 67 64 52 67 63 47 46 43 29 37 5 7 16 52 68 66 59 29 38 10 28 35 63 45 38 10 25 21 6 9 24 17 56 17 55 15 46 43 29 37 5 7 13 39 15 48 51 63 46 44 35 62 41 21 5 5 5 6 9 23 15 47 48 49 55 14 44 36 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 61 40 18 60 34 60 36 66 60 33 56 19 64 49 56 19 63 48 49 53 8 19 64 51 64 52 68 65 53 8 19 61 40 19 62 43 30 42 28 34 60 36 65 54 11 31 46 43 29 40 18 60 36 67 64 52 65 56 20 67 62 43 32 52 66 59 29 39 15 47 48 52 68 66 60 33 56 18 59 31 47 47 45 39 14 43 31 48 52 67 62 44 35 63 47 48 51 64 50 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-745\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 47 46 42 27 32 51 64 51 64 51 63 45 38 10 28 36 66 59 31 45 38 11 31 48 51 63 46 41 24 19 61 37 7 13 40 19 63 48 51 64 52 67 63 48 52 65 55 15 45 37 6 12 36 68 65 56 19 63 48 51 64 49 53 8 19 64 52 67 63 48 52 67 63 47 47 48 51 62 43 31 45 40 17 53 5 7 14 42 28 36 67 62 44 35 64 52 65 56 17 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-467\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-189\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 44 35 63 45 39 15 45 38 12 35 63 47 48 52 67 62 41 21 5 6 10 26 26 28 34 57 22 10 26 26 26 25 24 18 57 24 17 56 17 55 13 40 19 61 38 10 26 27 29 39 15 45 38 9 22 10 28 35 61 40 20 67 62 44 36 66 59 30 43 32 49 53 7 14 43 31 46 41 21 8 19 62 43 29 38 12 33 54 11 30 41 23 15 47 48 51 63 46 42 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 45 37 6 10 28 36 68 66 60 33 53 7 13 38 11 30 42 28 33 53 8 17 56 19 62 44 36 66 60 34 57 21 5 5 8 17 55 13 40 18 58 28 34 57 21 8 19 63 45 40 20 68 66 58 25 21 5 5 8 17 53 5 8 20 66 58 26 26 25 24 18 58 27 31 45 37 8 17 54 11 29 37 5 8 20 67 63 45 37 5 7 13 40 20 68 66 57 21 6 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-838\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-282\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 19 62 44 33 53 7 16 49 53 8 18 59 30 42 26 26 28 34 58 26 28 33 55 13 37 5 5 6 12 35 63 46 44 33 53 8 18 58 25 22 9 21 7 14 41 22 10 27 29 38 10 26 28 33 53 8 18 60 35 62 42 26 28 35 62 44 35 62 41 24 18 60 36 66 58 26 28 36 65 56 18 57 21 8 18 57 22 10 28 34 57 21 5 6 9 21 8 17 53 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-560\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 12 33 55 14 44 36 67 64 52 67 62 44 36 66 60 35 63 47 45 37 8 19 61 39 16 49 56 20 65 56 20 65 56 20 67 63 46 42 27 29 39 15 48 51 64 50 58 27 29 39 16 51 63 48 52 67 64 51 63 48 52 65 53 8 17 55 13 40 19 63 46 44 36 67 63 46 43 31 48 50 60 35 63 48 50 57 23 15 47 48 51 63 46 43 30 43 31 46 43 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 68 67 62 44 36 67 61 39 14 41 23 15 45 38 12 35 61 38 12 36 66 60 35 63 47 46 42 28 34 59 31 47 46 44 35 63 45 38 11 32 51 63 45 38 11 29 38 11 29 38 10 27 29 37 8 19 62 43 31 47 46 44 35 62 43 31 45 38 11 30 42 28 35 61 40 19 63 46 43 29 38 10 26 27 30 42 25 24 19 62 44 34 57 21 5 8 18 59 30 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-4\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 62 41 21 6 10 26 26 28 33 53 5 6 10 26 26 26 28 34 57 24 17 56 20 67 64 49 55 16 50 59 30 42 28 35 62 44 34 60 34 58 28 35 63 47 45 37 8 19 61 40 20 66 59 30 42 25 21 5 5 7 14 43 31 46 44 33 55 15 46 43 29 37 8 18 60 33 54 11 30 43 31 46 44 36 67 62 42 28 36 67 63 46 43 31 47 45 37 5 8 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-931\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 9 21 5 5 7 14 44 33 53 6 10 26 25 23 14 43 30 43 29 38 10 26 25 21 6 11 29 40 18 57 23 15 45 39 14 44 35 63 46 41 22 10 26 27 31 46 44 35 61 38 10 26 26 26 27 31 47 46 43 29 40 19 61 40 19 63 47 47 45 38 10 25 22 10 25 22 11 31 46 43 30 44 33 54 10 26 27 29 40 20 65 53 5 5 8 19 63 45 37 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-653\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-375\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 54 10 28 36 67 62 43 32 52 68 65 56 20 67 64 51 61 38 12 35 64 51 64 52 65 56 17 56 19 63 47 48 50 59 30 43 32 49 56 19 63 45 39 15 48 51 63 47 45 38 9 22 11 31 47 47 46 43 31 47 45 39 16 52 66 59 30 43 30 41 24 18 58 27 32 51 64 50 58 25 22 12 36 67 63 48 51 62 44 35 63 48 51 64 52 67 62 44 34 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-97\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 35 63 48 49 55 15 48 52 68 66 60 34 59 32 52 65 53 5 5 8 20 65 56 20 67 64 52 65 53 8 19 64 52 68 68 65 56 20 65 56 19 63 48 51 62 44 35 63 45 40 17 56 19 63 45 40 17 55 14 44 35 61 38 11 31 48 51 63 48 51 64 52 67 64 50 59 30 43 31 45 38 12 36 67 61 39 16 49 55 15 47 46 44 36 67 63 46 43 31 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 10 28 33 55 13 38 12 35 62 42 26 28 34 60 36 68 65 53 8 20 67 62 42 27 30 43 29 40 20 68 68 65 53 5 5 6 12 36 68 68 65 53 7 14 44 36 66 59 29 39 13 37 7 13 39 14 44 36 65 53 5 7 15 46 43 29 40 17 56 17 53 6 12 34 60 34 60 33 55 14 41 22 10 27 31 45 37 7 13 39 13 38 12 33 54 12 36 65 53 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1024\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 31 47 48 50 59 29 38 12 35 64 51 63 48 51 64 51 61 40 19 61 37 7 16 51 63 46 44 35 64 51 61 39 14 44 33 56 19 64 51 62 44 36 66 58 28 35 63 47 45 38 12 35 64 52 67 63 47 46 41 24 20 68 67 62 44 36 68 65 56 19 64 51 64 52 67 64 51 63 48 51 62 43 30 43 31 48 51 62 44 35 64 52 68 68 68 65 56 20 67 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-746\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-468\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 55 15 47 45 39 15 48 51 64 51 63 46 44 36 67 63 45 37 8 19 62 44 34 58 26 28 35 62 42 26 27 30 42 25 23 15 45 38 10 27 29 39 13 38 10 26 26 27 29 39 14 44 36 65 53 6 9 24 19 61 39 14 42 27 31 45 37 6 10 26 27 31 46 42 27 29 37 8 17 56 19 62 42 26 26 27 30 42 26 26 28 34 57 22 10 26 25 23 13 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-190\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 30 44 36 66 58 26 26 25 21 6 10 28 33 54 9 21 6 11 30 44 35 62 43 29 39 14 43 30 41 23 13 40 17 54 12 34 58 28 33 53 5 5 6 11 30 43 31 47 45 40 18 57 24 20 65 54 11 30 44 34 60 35 62 42 25 22 12 33 53 5 5 6 9 23 14 42 26 28 33 54 9 21 5 6 9 22 10 26 27 29 40 17 56 20 65 54 10 26 28 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 12 35 62 42 28 36 65 53 5 6 11 29 39 14 42 25 24 19 61 39 13 38 10 28 35 63 46 44 36 66 57 23 13 38 9 24 18 57 23 14 44 33 54 10 25 23 14 43 29 37 6 9 21 5 6 12 34 58 26 28 33 54 12 35 62 41 21 6 10 25 22 11 29 38 10 25 21 6 10 27 30 44 33 54 9 22 9 21 5 6 9 22 10 26 25 21 5 6 10 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1117\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 60 35 63 47 45 40 19 63 47 47 48 51 64 50 59 30 44 35 62 41 24 17 55 16 49 54 9 21 8 20 66 60 36 68 67 63 46 43 31 47 47 47 48 52 66 60 35 63 45 37 7 13 37 7 15 48 49 56 18 59 30 43 29 40 20 65 56 20 68 67 64 52 68 67 62 44 36 68 65 56 19 63 47 46 44 36 68 68 67 62 42 27 31 47 47 45 39 14 44 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-283\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-839\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-561\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 37 7 14 42 26 27 30 43 30 41 21 5 7 14 41 21 6 9 24 18 58 26 26 25 21 5 8 17 53 7 14 42 25 21 5 8 17 53 6 9 22 10 25 21 5 8 19 63 45 39 15 46 42 26 26 25 24 17 56 18 59 30 43 30 44 34 60 33 54 12 35 62 44 34 57 24 19 61 39 15 45 39 13 38 11 29 37 6 9 24 17 53 5 5 6 9 21 6 12 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 65 54 10 26 27 31 48 51 63 46 43 31 47 46 43 29 39 13 40 17 53 8 20 67 61 40 18 59 29 39 14 44 35 61 37 7 16 50 60 35 64 50 60 36 67 63 46 43 29 40 18 58 28 35 64 50 59 29 38 9 22 11 31 48 52 67 63 47 46 42 28 35 64 49 54 11 29 40 20 68 67 62 42 28 33 56 20 65 53 7 15 47 48 51 64 51 63 45 38 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 67 62 42 27 30 43 29 38 12 33 54 12 36 67 61 40 17 53 5 6 9 22 10 27 31 45 37 5 5 8 17 56 19 61 37 8 17 56 19 63 46 41 22 12 35 62 44 35 61 37 8 20 66 59 30 43 30 42 28 36 65 53 7 15 45 37 5 6 11 30 43 29 40 17 53 7 13 38 11 29 39 14 42 27 29 37 7 13 40 18 58 27 30 41 23 15 45 38 12 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-5\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1210\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 21 5 6 11 29 38 10 28 34 58 26 26 25 21 5 6 10 26 28 35 61 38 10 27 31 48 50 58 25 21 6 9 21 6 12 33 53 6 12 33 56 20 67 62 44 33 56 19 61 38 11 30 42 26 26 25 21 6 9 22 10 25 22 10 28 36 67 62 41 22 10 26 25 22 9 22 10 27 30 42 26 27 30 41 22 9 21 5 6 10 28 35 62 42 25 22 10 27 29 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 63 47 46 43 29 39 16 51 63 47 47 48 50 59 31 47 47 46 43 31 46 43 30 42 28 33 55 16 50 60 36 67 61 40 17 56 20 67 64 52 67 64 51 63 45 40 19 63 45 38 12 34 58 28 36 67 63 48 52 68 67 64 52 65 56 20 66 59 30 42 27 32 51 62 44 33 56 20 67 63 47 48 52 68 68 67 64 52 68 68 66 60 36 67 64 51 63 45 39 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1303\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-654\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-932\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-376\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 20 67 62 41 24 20 67 63 46 41 23 16 52 65 53 8 19 62 44 36 68 66 59 30 42 27 30 42 28 35 62 44 34 60 33 56 20 66 59 32 51 64 50 58 27 31 47 47 45 40 18 60 34 58 25 23 16 52 65 56 20 68 66 59 31 46 42 28 33 56 20 67 61 40 20 65 56 18 60 33 53 5 5 6 10 28 36 68 66 59 30 44 36 68 68 68 66 58 25 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-98\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 56 20 68 67 62 43 31 45 40 17 54 11 31 47 45 40 19 61 40 19 62 44 33 56 19 63 47 45 37 7 13 40 17 56 17 54 12 34 60 34 60 34 59 31 47 46 44 35 61 40 20 67 63 46 43 29 37 7 13 39 15 45 40 17 56 20 65 53 7 14 44 36 68 66 60 36 68 65 53 7 13 38 12 36 67 61 40 18 58 25 22 9 23 14 43 31 45 39 15 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 40 18 57 21 6 10 26 27 31 45 39 13 40 17 53 5 5 8 20 67 61 40 20 66 57 23 14 43 30 42 28 35 62 41 22 10 27 31 45 37 8 19 62 44 33 53 8 19 61 38 10 26 27 29 37 6 12 34 59 29 39 13 40 17 55 14 41 21 5 8 18 58 28 36 66 58 26 27 30 42 26 27 31 47 46 42 27 30 44 34 58 28 36 66 59 31 45 38 11 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 65 56 20 67 63 47 47 48 51 63 47 46 42 26 27 31 48 50 57 24 17 54 9 22 11 30 43 30 41 24 17 53 5 8 19 63 48 51 64 51 63 48 52 65 56 19 63 47 45 37 5 5 5 7 13 37 8 20 65 55 14 44 35 64 51 61 39 16 51 64 51 64 52 67 64 52 67 61 37 8 20 67 63 47 48 52 68 67 61 38 10 26 26 28 35 62 44 35 64 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 44 33 54 10 26 26 25 21 7 13 40 20 65 54 9 24 17 54 10 27 29 40 17 56 20 67 61 39 13 39 13 40 18 58 28 35 63 45 39 13 40 19 62 42 26 28 36 67 61 37 8 19 63 47 45 37 5 8 19 62 44 35 62 42 26 28 34 58 27 31 45 40 18 57 24 18 57 24 19 63 46 42 25 24 17 53 6 11 30 42 25 21 6 10 28 35 63 46 44 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1396\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-747\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 12 34 58 26 27 30 43 29 40 17 56 19 61 37 7 14 41 22 11 30 44 33 54 10 27 30 42 28 33 55 15 47 46 43 30 44 34 58 27 31 45 38 10 25 24 17 55 15 45 38 10 25 21 7 14 43 30 44 36 68 65 53 8 20 65 53 5 8 17 53 8 18 57 21 5 7 15 46 44 33 55 14 42 25 22 10 27 29 38 10 26 28 34 57 22 11 30 43 30 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-469\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-191\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 42 26 25 23 16 52 67 62 44 35 63 46 43 30 43 31 47 45 40 17 55 14 41 24 17 56 19 63 47 47 48 52 65 56 19 64 51 63 47 45 39 15 48 51 63 48 50 59 29 40 19 63 48 51 64 50 59 30 42 27 29 37 8 19 62 41 24 18 59 32 52 67 63 47 47 48 51 64 52 65 56 17 56 19 64 49 56 17 56 17 55 15 47 46 44 35 64 51 61 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 65 56 20 65 56 20 65 55 16 52 68 68 65 53 8 20 65 55 16 52 65 56 19 63 48 49 56 19 63 48 51 64 52 67 62 44 35 63 46 43 31 47 46 43 32 51 62 43 29 39 14 43 31 47 46 43 32 51 64 51 61 39 14 43 32 51 63 47 48 51 63 47 47 47 46 43 31 47 46 43 31 47 46 43 31 47 47 46 43 31 47 46 42 27 31 47 47 48 52 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 21 6 9 23 13 37 5 5 5 8 20 68 66 58 28 36 66 58 28 36 68 65 54 12 36 65 56 17 53 5 8 17 56 18 58 25 21 5 8 18 60 36 68 65 54 10 26 27 29 40 20 68 66 60 36 65 53 6 10 28 33 56 19 62 43 31 47 45 39 14 42 26 27 29 38 11 29 40 17 54 12 34 58 26 27 29 37 6 9 22 11 30 43 29 38 10 25 21 5 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1025\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 56 18 58 26 28 34 57 22 10 26 27 30 43 32 51 64 49 56 17 55 14 42 27 29 39 15 48 50 59 30 43 30 44 34 59 30 42 26 26 27 31 46 43 31 46 42 27 31 45 40 18 60 35 63 47 48 52 67 64 50 58 27 31 46 43 31 47 46 42 27 31 47 47 47 46 43 32 51 63 48 51 63 48 49 55 15 48 49 56 18 58 27 30 42 27 31 46 42 26 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1489\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1118\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-840\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-284\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1211\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 20 66 60 33 55 13 39 15 47 46 42 26 27 29 40 20 68 68 67 64 50 59 31 47 46 42 26 25 22 12 35 61 40 17 56 19 62 44 33 55 13 39 15 47 46 42 26 27 29 40 20 68 68 67 64 50 59 31 47 46 42 26 25 22 12 35 61 40 17 56 19 62 44 33 55 13 39 15 47 46 42 26 27 29 40 18 57 24 19 64 50 58 26 28 36 65 56 18 58 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 8 17 56 18 60 35 61 37 5 5 5 6 11 31 45 40 18 59 29 40 20 66 57 22 9 22 10 26 28 36 65 53 5 5 8 20 65 54 11 30 44 35 62 42 25 22 10 27 29 39 16 49 55 13 39 13 37 6 12 35 63 48 49 53 5 8 17 55 14 42 26 26 26 25 21 5 5 5 8 19 62 44 33 54 9 23 14 42 28 33 55 13 37 8 17 54 10 26 26 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-562\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 47 47 48 51 63 47 47 45 38 12 35 61 38 10 28 36 67 61 39 15 47 48 52 65 56 20 68 68 67 62 43 29 40 19 62 44 33 55 16 51 63 48 51 61 39 14 42 27 29 39 16 50 60 33 55 16 51 63 48 52 66 60 35 63 48 52 68 67 64 49 53 7 13 38 12 36 67 64 52 67 64 52 67 63 45 39 15 48 52 68 66 60 33 56 18 60 35 62 44 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 25 22 10 26 28 34 58 26 25 22 10 27 29 37 7 13 37 7 14 41 22 10 26 25 22 10 28 36 65 55 13 38 10 26 25 23 14 43 30 44 34 60 34 59 29 40 20 67 61 39 13 37 6 12 35 62 41 21 8 18 60 35 62 41 21 8 20 65 54 12 34 60 33 53 8 17 54 9 21 5 6 11 30 43 30 44 35 62 43 30 41 23 14 44 33 53 6 10 26 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-933\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 56 19 64 51 64 51 63 46 44 35 64 51 62 43 30 41 22 9 22 9 22 9 23 13 40 19 64 52 67 63 47 46 44 35 63 47 45 40 20 67 64 51 62 44 35 62 42 27 29 38 11 32 52 67 64 51 62 42 26 28 35 63 45 39 14 42 28 34 57 23 15 47 48 49 56 18 58 26 26 26 28 33 54 10 27 30 43 29 37 7 13 38 12 34 59 31 48 49 56 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1582\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 57 24 17 56 17 53 6 10 27 30 41 24 20 66 58 25 24 17 53 5 5 7 13 38 10 26 26 27 30 42 26 27 29 40 17 53 6 12 34 59 32 50 58 28 35 62 43 31 45 38 10 28 34 58 26 26 25 22 9 24 18 58 26 27 31 45 40 18 58 26 26 28 33 54 12 36 66 57 23 14 41 21 5 8 17 55 15 48 49 54 12 35 63 45 38 10 27 30 44 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-655\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-377\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1304\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 46 41 24 20 65 53 7 15 48 51 62 44 35 64 51 64 51 62 43 29 39 14 44 33 54 10 28 36 67 62 44 33 54 11 32 51 62 44 35 62 43 29 40 19 63 48 51 63 45 38 10 26 26 27 29 37 5 6 11 31 47 47 48 52 65 54 12 33 55 16 52 67 64 52 66 60 36 67 64 52 67 62 44 35 61 40 18 59 31 48 51 62 44 33 55 13 40 20 67 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 61 40 20 67 61 39 14 44 35 63 47 46 43 30 44 33 53 7 14 44 36 65 53 7 13 37 5 5 6 12 35 61 37 7 14 42 27 31 48 52 66 58 28 36 65 56 18 59 29 39 14 43 32 52 67 63 45 40 20 67 64 51 63 48 52 67 64 49 55 15 46 44 33 56 20 68 68 65 56 17 56 20 68 65 53 7 16 51 61 40 19 62 44 33 53 5 8 19 64 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1675\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 62 44 33 53 5 5 6 12 33 54 12 33 54 9 22 9 22 10 27 30 44 33 53 8 17 56 17 54 9 21 8 18 60 33 56 18 60 33 55 15 46 44 36 65 53 7 15 47 45 38 12 34 57 21 5 7 13 40 20 65 56 17 54 12 35 62 44 36 68 66 58 28 35 61 40 18 58 27 30 44 36 65 53 7 15 45 39 13 38 9 22 12 34 57 21 6 10 25 22 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 52 68 67 61 39 13 40 17 56 19 63 47 45 37 8 17 55 15 45 39 16 50 58 25 24 18 60 35 63 45 38 10 26 25 21 5 8 18 60 35 62 44 34 58 25 24 18 59 29 39 13 39 14 43 30 42 25 24 19 64 52 66 60 36 67 62 41 24 17 56 18 60 36 65 54 9 23 14 41 24 20 65 54 10 25 21 5 5 5 8 20 68 67 61 39 13 40 17 53 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 43 31 45 39 16 50 60 36 66 59 30 41 21 5 7 14 44 35 61 37 8 17 56 18 57 24 19 61 37 5 5 5 6 9 21 5 6 9 21 5 5 7 13 40 18 57 24 19 61 38 9 21 5 6 9 22 11 29 40 19 62 43 31 46 43 31 47 46 42 26 25 24 19 63 46 42 27 31 46 44 36 66 59 29 38 11 30 43 30 44 34 60 33 54 11 31 45 39 13 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1768\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 55 15 47 47 45 39 15 47 47 46 41 24 20 65 53 5 8 19 62 42 25 22 9 21 5 8 20 67 62 42 26 27 31 46 43 30 42 28 36 66 58 27 30 43 30 42 26 27 29 37 6 11 30 42 25 22 10 26 26 28 34 58 28 35 64 49 56 20 66 58 27 30 44 33 56 19 64 50 58 28 34 59 30 44 34 60 34 58 26 25 21 7 15 46 42 28 33 54 10 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 68 65 56 20 68 67 64 50 60 36 67 63 45 40 19 61 39 16 51 63 46 42 26 26 27 31 48 51 62 41 24 18 59 32 51 63 47 47 48 51 62 43 30 41 22 11 31 47 45 38 9 24 18 59 30 43 32 51 62 44 35 63 47 46 44 33 56 19 63 46 43 31 47 48 50 60 35 63 48 52 67 63 48 52 67 63 48 52 67 63 48 52 68 68 68 65 53 7 13 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1397\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1026\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 64 49 54 10 28 36 67 62 44 35 63 48 50 60 33 53 8 19 64 49 53 5 8 17 56 20 67 64 52 68 65 55 13 37 5 6 10 28 35 63 48 51 64 49 56 18 58 27 29 40 18 60 35 63 48 51 63 45 39 14 44 36 65 53 7 15 45 40 20 65 56 17 54 12 35 64 52 67 64 51 61 40 20 65 56 19 62 44 34 59 32 51 62 44 34 60 34 58 26 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-470\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 41 21 6 10 26 26 26 26 26 26 26 28 34 57 22 10 26 26 26 25 24 18 58 28 33 56 17 54 12 36 68 68 66 58 26 27 29 39 15 45 38 12 34 58 28 36 67 63 45 40 20 65 54 12 36 66 59 30 43 30 41 22 11 30 43 30 42 28 33 55 15 46 41 24 18 60 33 54 11 30 44 35 63 46 44 35 63 46 42 25 24 19 63 46 43 31 47 45 37 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 64 51 62 43 30 43 29 40 20 66 60 36 65 56 20 67 63 48 50 60 35 63 47 47 48 51 62 43 31 48 51 64 51 62 43 29 39 16 49 53 8 19 62 44 35 64 50 59 29 39 14 42 27 31 48 52 67 64 50 60 34 60 35 64 50 59 30 44 36 67 64 50 59 31 48 51 64 51 64 51 62 44 35 61 39 13 37 6 12 36 67 64 52 67 62 43 30 44 33 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-748\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 31 47 45 40 19 62 43 31 48 51 62 43 32 52 68 67 61 40 20 67 62 43 29 40 17 55 14 43 31 45 40 18 58 27 32 51 61 38 11 29 39 15 47 48 51 64 50 59 29 40 19 61 40 18 59 30 41 21 7 13 39 16 50 60 35 62 42 26 25 21 6 10 28 36 65 53 6 12 34 59 29 38 11 29 39 13 40 19 61 40 20 67 63 48 52 67 63 47 47 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1212\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1119\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1490\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 31 48 52 66 59 32 52 68 68 65 53 8 20 68 68 68 65 53 8 17 53 8 20 66 57 21 7 14 42 27 31 48 52 66 60 33 55 16 52 68 68 66 58 28 35 61 38 11 29 39 14 42 27 31 46 43 30 43 29 37 8 19 62 42 28 36 67 64 50 58 26 28 34 58 26 28 36 66 60 36 68 68 66 58 25 23 13 39 16 51 64 52 68 66 58 27 29 37 7 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 37 5 5 8 20 66 59 29 39 15 46 44 33 54 10 28 33 56 17 53 6 9 22 10 26 25 21 5 6 9 22 12 34 58 27 30 42 27 29 38 10 27 31 46 41 22 12 35 61 37 5 5 5 6 9 22 10 25 22 9 22 10 26 26 26 26 28 34 58 27 30 41 22 11 30 42 27 31 45 39 13 37 5 8 18 60 33 53 5 6 10 26 25 22 12 33 56 20 67 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-563\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 40 20 67 63 48 52 65 53 5 7 13 39 13 40 17 56 17 53 7 14 44 34 58 26 27 31 48 52 68 66 59 32 52 68 68 68 68 67 61 40 20 65 56 19 63 45 39 15 45 38 11 32 49 56 19 62 42 27 32 51 64 52 68 68 66 58 28 35 62 44 36 68 67 62 44 33 55 16 52 65 54 11 31 48 51 64 52 68 67 63 48 52 67 61 38 11 30 44 33 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 47 46 41 24 20 65 56 17 53 5 8 20 67 62 43 29 38 10 26 26 27 30 42 26 26 26 26 26 27 30 43 30 43 30 43 30 43 30 43 30 42 27 31 47 47 48 51 63 45 39 15 47 46 42 25 22 10 26 26 25 22 10 26 26 27 29 38 10 26 26 25 22 11 31 46 43 30 44 34 58 27 30 42 27 29 38 9 23 16 52 68 67 62 42 26 28 36 66 59 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-841\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1583\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1305\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2326\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 51 64 51 63 48 51 64 52 67 63 48 51 64 51 63 46 44 35 64 51 61 40 18 60 34 57 24 19 63 48 52 68 66 59 29 40 19 62 44 36 65 55 13 40 20 68 66 59 29 38 11 31 46 44 33 56 20 68 66 60 35 64 49 55 14 43 31 48 51 63 48 51 64 49 54 12 36 66 60 33 55 15 47 48 52 66 58 27 31 46 44 36 68 67 63 47 45 39 16 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 29 39 14 43 30 44 34 58 28 35 63 47 45 40 17 55 14 44 36 65 56 18 60 35 61 40 18 60 36 67 61 39 16 49 54 11 30 42 28 36 67 62 43 29 39 14 44 35 61 37 7 14 42 27 30 44 35 63 46 43 31 47 45 40 20 66 58 27 29 37 8 19 64 49 54 10 27 30 43 31 46 44 35 63 46 43 29 40 19 63 46 43 31 47 48 49 56 18 57 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 28 36 68 65 53 7 15 45 40 19 62 42 26 26 27 30 43 30 43 30 44 35 64 51 61 38 12 35 64 51 63 48 51 63 47 47 48 51 63 47 45 39 14 42 27 32 51 63 45 38 10 26 26 27 31 46 43 31 48 52 65 53 8 18 60 35 64 52 65 54 11 31 47 45 40 19 64 52 67 64 52 66 59 32 50 60 34 57 24 19 62 44 33 56 19 61 40 20 67 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2047\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1861\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 59 30 43 32 51 61 38 11 30 43 30 43 31 48 51 62 43 31 47 46 44 35 62 43 30 43 30 43 31 46 43 31 46 42 26 26 26 27 31 47 47 47 48 51 63 46 43 31 45 38 10 26 26 28 34 58 27 32 51 64 52 65 55 16 51 62 44 36 68 68 65 55 16 52 66 60 36 68 65 56 19 61 40 17 54 11 31 45 38 10 26 27 31 48 52 68 66 58 28 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2512\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2233\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1676\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1398\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1954\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 52 65 56 19 62 43 31 46 44 35 64 51 62 43 31 47 47 46 42 27 30 43 31 45 39 15 47 47 47 46 43 31 48 52 67 62 44 34 60 34 60 33 56 17 56 20 66 59 29 40 19 61 40 17 56 20 68 68 67 64 52 66 59 30 44 35 64 52 68 68 65 55 13 37 7 13 37 6 12 36 67 64 52 68 68 66 58 27 30 44 36 68 66 59 32 52 68 65 55 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1027\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 22 11 31 47 45 40 17 53 6 11 31 46 43 29 40 17 56 17 53 8 20 65 56 18 57 24 19 64 51 64 50 58 27 32 50 60 35 64 50 59 31 46 41 24 18 58 27 31 45 40 18 57 23 13 40 19 64 50 60 36 65 56 20 68 66 58 26 25 24 20 67 61 40 19 64 50 60 34 58 27 30 44 33 54 10 27 30 42 26 28 35 64 52 68 65 55 16 52 67 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 63 47 47 48 51 63 47 47 47 47 47 45 40 17 55 15 47 48 50 59 29 37 5 5 7 13 37 6 10 28 35 63 46 43 32 49 56 19 64 52 67 61 40 19 63 45 38 11 29 38 12 36 65 54 10 26 25 21 5 8 20 68 68 67 61 40 18 57 23 15 48 52 67 61 37 8 17 56 19 64 52 67 61 40 19 63 45 39 13 39 15 48 52 67 61 40 17 54 10 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 59 30 44 35 62 43 30 44 35 64 51 64 50 58 27 31 45 38 11 32 49 56 18 58 26 27 30 43 32 52 67 63 45 38 11 32 51 64 51 64 51 63 46 44 35 64 51 63 45 38 10 28 36 68 67 62 44 34 59 29 40 18 59 29 40 17 56 20 67 64 52 67 64 50 60 36 65 56 17 54 11 32 51 62 44 36 68 65 56 19 64 52 66 58 28 35 64 52 67 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 49 54 10 28 36 67 63 48 52 67 63 48 51 64 52 67 64 51 63 47 48 51 62 43 31 47 45 40 17 53 6 12 35 61 40 19 64 51 61 38 12 36 67 64 51 64 50 59 29 38 10 28 33 53 8 17 56 17 56 17 55 15 45 38 12 33 54 12 36 67 64 52 67 64 52 67 64 51 62 44 36 68 68 67 63 47 47 47 45 40 17 53 8 18 60 33 54 11 32 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 17 55 16 52 65 56 18 59 30 43 30 42 25 23 15 46 42 28 34 59 29 39 15 47 45 40 17 55 14 41 24 17 56 18 60 35 61 40 18 60 36 67 61 38 9 21 6 11 29 38 12 36 67 62 43 29 39 14 44 35 61 40 19 63 46 43 32 49 55 15 47 47 47 46 41 24 20 67 62 43 29 37 8 18 60 33 54 11 31 46 42 27 31 45 39 15 46 43 29 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-934\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-656\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 9 22 12 36 65 55 13 38 10 28 35 62 42 25 22 10 26 27 30 43 31 47 45 40 19 61 40 19 61 37 6 12 34 57 22 12 33 55 13 40 19 61 38 12 35 61 38 11 29 37 8 18 57 22 10 28 35 63 45 37 7 15 45 40 20 68 65 53 8 19 62 43 29 39 14 44 33 56 19 63 46 42 25 21 6 12 34 58 27 29 40 20 68 66 58 26 26 26 25 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 35 64 49 53 5 8 19 62 43 30 42 26 25 21 6 10 25 24 17 54 11 31 47 45 38 12 34 60 34 59 31 45 38 10 26 26 26 28 35 62 42 26 26 28 34 58 28 35 61 37 6 10 28 35 62 42 26 26 28 33 54 12 34 58 26 26 27 30 42 27 29 38 12 33 53 5 6 11 30 42 26 28 35 63 46 41 22 12 35 63 46 41 22 12 34 59 31 46 44 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2140\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1769\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2605\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   Writing example 0/120\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 24 20 67 61 38 11 30 44 36 65 54 10 26 27 31 47 47 47 48 50 57 24 19 63 47 46 43 30 42 26 27 29 39 15 47 47 47 47 47 47 47 47 47 47 48 52 67 63 45 38 10 25 23 15 48 49 53 8 19 64 52 65 54 12 33 53 5 5 7 13 37 5 7 13 39 14 41 21 7 16 49 54 12 36 67 64 52 67 64 51 63 48 52 68 65 53 8 19 64 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2419\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 54 11 30 42 27 31 47 46 42 25 24 17 54 11 30 42 28 34 58 27 30 44 34 58 26 27 29 38 11 30 42 27 30 42 26 26 25 23 14 44 34 59 29 39 13 39 14 43 29 38 9 21 5 5 6 10 27 32 52 67 63 45 40 19 64 50 58 25 21 8 17 54 12 34 57 24 18 60 33 56 18 58 28 34 60 33 55 15 46 42 26 26 27 30 42 25 22 10 25 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 67 63 47 47 46 43 31 47 48 49 56 17 56 17 53 6 12 36 66 59 32 51 63 48 50 59 31 47 48 51 62 43 30 43 31 46 44 34 58 27 32 52 68 66 59 29 39 15 45 39 15 48 52 67 63 48 52 68 67 64 51 63 45 39 14 43 32 49 53 7 16 49 56 19 63 47 47 48 52 67 63 45 40 19 63 48 50 59 29 39 15 47 46 41 21 6 11 29 38 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 6 11 31 45 37 6 10 26 26 26 26 26 25 21 5 5 5 5 5 5 5 7 13 39 13 37 7 13 37 7 13 38 11 29 40 17 53 5 6 12 35 61 37 5 5 5 8 19 61 37 7 13 38 9 21 6 10 25 23 13 40 17 54 12 34 60 36 65 53 8 18 60 33 54 10 26 26 26 25 24 17 53 6 12 33 54 9 21 8 17 56 17 53 6 10 26 27 29 38 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 28 36 66 59 31 47 46 44 35 62 42 26 27 31 47 45 40 18 59 30 41 22 10 25 22 12 35 62 44 35 62 43 31 47 46 44 33 54 9 22 10 28 33 53 7 15 46 43 29 38 9 21 8 19 62 42 28 34 59 31 47 45 39 14 42 25 24 20 66 60 33 54 12 34 60 34 60 34 58 27 29 38 12 34 57 24 17 53 5 6 10 27 30 44 35 63 45 40 20 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 29 39 13 37 7 15 46 43 30 42 27 30 44 34 60 35 63 47 47 45 40 20 66 58 27 30 42 28 36 68 65 56 18 60 36 65 53 6 12 33 53 7 14 44 36 65 54 11 29 39 13 38 11 29 38 12 36 65 53 5 6 11 29 38 10 27 30 44 36 68 68 66 59 31 46 41 21 5 5 7 14 44 34 60 33 54 9 24 17 56 20 68 68 65 54 11 32 49 54 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3163\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 63 47 45 39 15 47 46 41 23 14 41 21 8 20 67 63 47 46 44 36 68 65 56 17 56 19 62 42 25 24 18 58 27 30 42 28 35 63 46 43 31 45 40 20 68 65 56 19 61 38 11 32 50 57 22 11 30 44 36 65 53 8 20 68 68 68 67 61 37 8 20 65 55 13 38 11 30 44 36 65 55 13 40 17 54 10 25 22 12 33 55 13 37 8 19 61 40 20 68 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1306\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1584\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3442\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2698\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2791\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1028\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1399\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2048\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2327\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2884\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 35 61 37 8 20 67 63 48 51 63 48 51 64 50 60 33 55 16 50 59 29 40 20 65 55 16 51 63 48 51 64 52 66 59 29 40 20 65 55 16 50 59 32 49 53 8 19 63 45 37 5 8 17 53 8 17 55 15 45 40 17 56 19 63 45 40 19 63 48 52 68 66 60 36 67 61 39 13 40 19 64 52 66 60 34 59 32 50 60 36 67 63 48 50 60 34 58 28 35 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1862\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 45 37 5 7 13 39 13 38 10 26 28 36 66 58 26 26 25 23 13 38 10 26 26 28 33 55 13 40 17 54 9 23 14 42 27 31 45 37 5 7 14 42 27 31 47 46 43 31 45 37 6 10 26 26 26 28 34 58 27 30 43 29 40 18 57 23 13 40 17 56 18 57 22 12 34 60 33 56 18 60 35 62 44 34 58 25 22 11 30 43 31 47 47 45 38 10 27 30 43 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2513\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 46 43 29 38 12 34 59 29 40 19 62 44 35 62 43 29 40 19 62 44 36 65 56 17 56 19 61 39 13 38 12 34 60 35 62 42 28 33 53 8 18 58 26 27 30 42 26 27 29 39 15 46 42 27 29 39 14 43 30 42 28 35 62 42 26 25 21 6 9 21 5 5 8 19 61 38 12 34 57 22 9 21 8 18 59 31 45 40 20 65 56 19 62 42 25 22 10 26 26 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 6 10 26 28 35 61 39 13 40 20 65 55 13 39 16 51 61 38 10 26 28 34 57 23 14 42 25 21 6 12 36 65 56 18 58 28 33 53 7 15 46 43 29 37 5 7 15 47 45 40 20 66 59 30 41 23 13 40 20 65 53 7 15 45 40 17 54 10 27 31 45 39 16 50 58 27 30 42 25 21 7 15 45 38 10 27 30 42 27 29 40 18 60 33 55 13 37 7 13 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1677\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 64 50 60 33 53 7 14 43 29 40 20 65 56 20 67 61 40 17 56 19 62 42 28 35 61 40 18 60 33 56 19 62 44 33 56 17 54 10 28 35 64 51 63 45 38 10 28 35 61 39 14 43 31 45 40 19 63 46 44 36 68 67 61 37 6 9 24 17 56 19 64 49 56 17 55 14 43 30 44 34 59 30 43 29 37 5 5 5 5 5 5 6 9 21 5 6 9 21 5 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 62 42 27 31 46 44 36 65 56 17 53 8 20 65 56 18 59 32 52 67 62 41 22 11 32 50 60 33 56 19 63 48 52 67 63 47 47 46 43 29 37 7 15 47 46 42 26 27 29 39 13 40 19 63 45 37 8 20 67 63 47 46 43 30 43 32 52 68 67 61 39 15 45 40 19 63 47 45 40 20 66 60 33 55 14 43 29 40 20 66 58 28 36 68 67 62 42 27 30 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 46 43 29 40 19 63 47 47 47 47 47 46 42 27 29 40 18 58 27 30 42 25 21 5 8 19 64 51 62 44 35 61 37 6 10 27 32 51 62 44 35 62 44 35 61 40 19 63 45 38 9 22 10 26 27 30 42 25 23 14 43 30 43 30 43 32 52 68 68 67 62 44 36 65 53 8 19 62 42 27 31 46 44 33 55 14 44 33 53 8 17 54 11 30 43 30 43 30 44 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2977\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3628\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 45 39 15 47 47 46 44 34 60 35 62 44 35 63 46 42 27 32 52 68 68 68 67 61 40 19 61 40 19 62 44 33 53 7 14 44 36 68 68 66 59 31 45 39 13 37 7 15 45 40 20 67 61 39 16 50 59 32 52 68 68 67 63 45 39 15 46 44 34 59 30 44 36 67 61 40 20 68 68 67 61 40 20 68 66 59 29 37 6 12 36 68 66 59 30 42 26 27 32 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1213\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1120\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-749\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 42 26 28 33 55 13 37 8 18 57 24 17 56 17 53 7 14 42 27 30 44 36 68 66 57 23 13 38 10 26 26 25 21 7 14 41 21 8 17 56 18 58 26 26 26 28 35 63 45 39 13 40 18 58 28 33 54 9 23 16 50 58 25 24 19 62 43 30 42 27 31 46 44 36 65 53 6 10 26 26 26 25 22 12 34 58 26 26 25 21 5 5 6 10 27 30 43 29 40 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1491\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 12 34 57 21 7 13 38 11 32 52 65 55 16 49 56 20 67 61 39 16 49 56 18 60 33 56 20 68 68 68 68 65 56 20 67 64 52 66 60 36 67 64 52 67 64 52 67 63 45 38 10 26 26 28 33 56 19 63 48 51 62 44 35 63 48 51 63 45 38 10 28 36 65 56 18 60 36 68 67 63 47 47 47 47 47 47 47 46 42 26 27 31 47 47 47 46 42 27 32 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 28 35 62 43 29 40 18 60 36 67 63 47 45 40 20 67 62 44 36 65 56 18 60 35 61 40 18 60 36 67 64 51 64 49 54 11 30 43 32 52 67 62 43 29 39 14 44 35 61 37 8 19 62 43 31 48 51 63 46 43 31 47 45 40 20 66 58 27 29 38 12 35 63 45 38 10 27 30 43 31 46 44 35 63 46 43 29 40 19 63 46 43 31 47 48 49 56 18 57 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 41 21 5 6 9 21 6 9 22 10 27 31 45 38 10 28 34 60 34 57 21 8 18 58 26 25 22 12 35 63 45 37 6 10 26 26 27 30 42 26 25 22 10 27 29 38 10 27 29 38 11 30 42 26 26 28 33 54 9 24 17 55 13 38 10 28 33 53 6 10 26 28 34 58 26 26 26 27 30 42 26 26 27 30 41 22 11 30 42 28 36 67 62 41 22 10 28 34 60 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-842\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3256\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 67 62 42 26 28 36 68 67 62 44 36 67 62 44 35 61 40 18 59 30 44 34 59 30 44 33 56 20 68 67 64 52 67 63 48 49 53 8 18 60 36 67 62 44 36 67 62 43 29 38 10 26 25 21 8 17 54 12 33 56 20 67 62 42 27 30 44 35 62 44 35 62 42 27 30 43 31 46 41 24 18 60 36 67 64 52 67 64 50 59 30 44 35 64 49 54 12 36 66 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3349\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3070\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3535\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 60 36 68 66 60 36 67 63 46 44 33 56 18 60 35 63 46 42 26 25 22 12 33 54 9 22 9 23 13 38 12 35 62 44 36 67 62 42 27 30 42 26 27 29 37 5 8 19 61 38 9 22 9 21 5 7 15 45 37 8 17 56 17 56 17 55 13 40 17 56 20 68 65 56 19 63 45 40 18 59 29 40 17 56 17 56 19 61 39 15 47 45 37 8 20 68 68 68 65 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 52 65 55 16 49 55 13 37 7 14 42 27 32 49 56 17 56 18 59 32 51 61 38 11 30 43 31 48 49 55 13 39 16 52 68 67 62 43 30 44 35 64 52 68 67 63 46 43 29 38 12 35 62 43 32 50 59 32 49 56 18 58 27 30 42 27 32 51 63 47 47 47 46 44 36 66 60 33 56 18 59 31 45 38 10 27 30 44 35 63 48 49 56 20 67 64 52 68 68 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 36 65 54 11 29 37 5 8 18 59 29 39 15 45 38 12 34 59 31 47 47 45 37 8 17 56 17 53 8 18 59 30 41 21 5 8 17 55 14 43 29 39 14 41 24 18 58 27 29 37 5 8 18 58 28 35 63 47 46 44 36 65 54 11 31 47 46 43 30 43 30 43 30 43 30 44 34 60 35 61 40 20 65 56 18 57 21 5 5 6 11 30 41 24 19 61 39 13 39 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 35 62 42 25 21 6 12 35 61 37 8 19 61 38 11 30 44 34 59 30 42 25 22 10 26 27 31 46 44 35 63 47 46 44 34 57 24 17 54 11 32 51 63 45 39 13 38 11 29 39 13 40 17 53 6 11 32 50 57 22 11 30 41 22 11 30 43 29 39 13 40 19 62 44 33 55 13 37 7 15 46 42 28 34 60 35 62 44 33 53 7 15 46 41 23 13 40 20 65 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 17 54 9 21 6 12 33 56 20 68 66 60 33 55 14 44 33 56 20 68 65 53 5 5 5 5 6 12 36 66 59 30 43 31 46 44 33 53 6 10 25 23 13 38 10 25 22 11 29 37 5 5 7 13 38 10 25 24 17 56 17 56 18 58 27 29 37 5 5 8 20 65 53 5 5 6 12 33 56 17 53 7 13 39 14 42 25 22 10 25 22 10 26 26 26 28 34 59 30 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 34 58 28 34 58 26 25 22 11 30 41 22 10 26 28 35 61 38 12 35 61 38 10 26 27 31 46 42 27 30 41 24 17 55 13 37 8 20 65 55 15 47 46 44 36 68 68 67 61 40 18 58 27 30 44 35 62 42 28 33 55 14 41 24 18 60 33 56 17 53 5 6 9 22 10 26 25 23 14 41 21 5 6 9 22 10 25 24 20 65 54 12 33 55 14 44 36 68 68 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 36 67 62 41 24 20 65 56 20 67 64 52 68 68 67 64 50 60 36 68 68 67 64 52 66 60 36 65 53 7 14 43 31 46 44 36 68 67 62 44 35 64 52 67 61 38 10 27 29 39 16 50 60 33 54 11 30 44 35 61 39 16 52 68 67 64 51 61 40 17 54 12 34 57 24 20 67 61 39 15 48 52 66 59 31 48 49 56 18 60 35 63 46 44 35 63 47 46 43 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 29 37 5 5 5 7 13 38 12 34 59 30 42 27 30 42 26 26 26 26 28 34 60 33 53 6 12 33 53 5 8 17 56 18 60 36 67 62 44 36 67 61 38 12 34 57 24 19 61 40 20 66 57 23 14 44 33 53 5 6 10 25 22 10 26 28 34 57 24 17 56 18 60 33 53 6 12 33 54 9 21 5 6 11 30 43 29 40 18 58 27 29 40 20 68 67 61 39 14 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2420\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 52 68 68 68 67 63 48 51 64 52 68 67 64 52 68 68 68 68 67 64 52 68 67 63 48 52 68 68 68 67 64 52 68 68 68 67 64 50 60 36 68 68 67 64 52 67 63 47 45 38 10 26 27 31 46 43 31 46 43 31 46 41 24 19 63 48 52 65 55 14 44 36 65 56 20 68 65 56 17 55 13 37 5 8 19 61 40 19 64 51 63 47 48 50 59 30 44 35 62 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 56 19 63 45 37 6 12 33 53 5 5 8 18 58 26 27 29 40 19 62 43 29 40 20 65 53 5 5 5 8 18 57 22 10 26 25 23 14 42 26 28 33 53 8 17 54 10 27 29 40 17 56 20 65 53 5 5 5 6 9 22 11 30 43 30 43 29 37 5 5 6 10 28 34 58 26 26 27 30 43 30 44 36 68 65 53 5 5 7 13 40 17 53 5 5 6 10 26 26 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 26 26 26 28 33 53 6 9 23 14 43 31 45 37 7 15 46 41 21 5 5 7 13 38 12 33 53 6 11 29 37 8 17 53 8 20 65 53 5 5 5 5 7 14 43 29 39 15 45 38 10 25 24 19 62 42 25 24 18 57 23 13 40 17 54 12 33 54 9 22 12 34 57 21 5 7 13 37 7 14 42 26 26 25 22 12 36 67 63 46 42 25 21 5 5 5 7 13 40 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 68 65 54 11 29 39 14 42 27 31 48 51 63 46 43 30 44 36 68 68 66 59 29 40 20 67 62 43 31 48 51 63 47 45 40 19 62 42 28 35 63 47 48 52 67 61 38 11 29 39 14 43 32 51 64 52 67 61 38 10 28 36 65 56 18 59 29 37 8 17 54 12 36 65 56 20 65 56 18 57 23 16 51 64 51 64 49 56 17 56 19 63 46 42 28 36 66 60 33 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2606\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1955\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3164\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2234\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-935\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1770\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2141\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 25 21 5 5 5 6 9 24 17 55 14 42 26 25 21 6 12 36 67 63 45 38 12 34 57 22 12 36 66 60 36 67 62 43 29 39 13 39 15 46 41 22 9 21 6 11 31 47 45 40 19 61 39 16 50 58 28 36 68 65 56 20 67 62 44 33 56 20 67 61 40 20 65 56 20 65 54 11 29 39 14 42 28 33 56 20 67 63 45 40 20 65 56 18 58 27 29 37 8 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 40 20 66 58 28 35 62 42 26 28 33 54 9 21 8 17 53 8 18 60 33 56 19 62 43 31 47 46 44 34 59 29 40 18 60 36 65 56 20 67 61 38 10 27 29 37 8 19 61 40 17 53 8 19 61 40 19 62 44 36 66 58 28 36 67 62 43 31 46 42 28 36 68 68 68 65 56 17 55 14 44 34 60 36 66 57 24 19 61 40 17 56 20 68 68 67 62 42 27 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 29 39 14 44 34 60 34 58 26 26 27 31 45 40 20 67 62 41 24 17 56 18 57 23 13 40 18 60 36 66 57 23 13 40 18 59 29 38 9 24 19 62 43 29 39 14 44 35 61 40 19 63 46 43 29 39 15 47 46 43 30 43 32 52 68 67 62 43 29 37 8 20 65 53 6 11 31 46 43 31 47 48 50 59 30 42 25 24 19 61 38 11 30 43 29 37 8 18 57 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 63 48 51 64 51 62 42 27 31 45 39 16 51 64 51 61 40 20 67 63 45 37 5 8 20 67 61 37 8 17 53 7 16 50 60 35 64 52 67 62 44 36 65 55 16 52 67 63 45 39 14 43 32 51 64 51 63 48 52 68 67 63 45 39 13 39 16 52 67 64 51 61 37 7 13 40 19 62 42 27 30 44 35 61 37 6 12 36 67 64 50 59 29 39 16 50 58 27 29 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 47 45 39 13 37 6 10 26 27 29 39 15 46 44 36 68 65 53 8 17 55 15 48 49 56 18 59 31 46 43 32 49 56 20 65 54 10 28 35 64 49 55 16 50 60 35 64 51 61 40 19 62 44 34 59 32 52 65 56 19 64 50 60 36 65 54 11 31 46 44 36 68 65 55 16 51 62 42 26 26 28 35 62 42 25 22 10 26 26 26 26 28 34 57 23 13 39 15 46 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 14 42 26 27 31 48 49 53 8 20 66 57 22 10 27 31 47 45 37 6 10 26 26 26 26 26 25 21 6 10 26 25 21 5 6 9 24 17 54 9 21 6 11 32 49 53 6 11 29 38 12 33 53 8 18 58 28 34 57 22 10 26 25 22 12 34 58 26 25 23 14 42 28 35 61 40 18 58 27 30 42 26 25 22 10 26 26 27 31 48 51 61 40 18 60 36 68 65 56 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 19 62 44 36 67 62 42 26 26 26 26 25 21 5 5 8 17 54 11 29 37 7 13 37 5 5 6 9 21 5 5 5 8 17 55 14 44 35 62 41 24 19 61 37 8 17 55 15 45 37 6 9 21 5 8 17 53 8 17 53 5 5 8 17 56 17 56 17 53 8 17 53 6 11 29 37 5 6 9 24 17 54 12 35 61 37 6 9 21 5 5 5 5 6 12 33 54 9 21 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1214\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2792\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3350\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1307\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3071\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1400\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2049\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1585\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3443\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 35 63 46 42 25 24 19 63 47 47 47 47 48 49 56 18 57 24 19 62 44 36 68 65 55 14 41 23 13 40 20 67 64 50 60 34 60 35 62 41 23 15 45 39 13 39 15 47 45 40 19 62 41 21 6 10 26 26 26 26 28 34 57 22 10 26 26 26 25 24 18 57 24 17 56 17 55 13 40 20 68 66 58 26 27 29 39 15 45 38 12 34 58 25 24 19 63 45 40 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1678\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 68 66 60 35 62 44 35 64 52 67 64 51 64 49 53 6 9 21 8 17 56 19 63 48 52 65 55 15 48 51 64 51 62 42 28 35 64 51 61 38 10 28 33 56 18 59 31 47 45 39 14 43 31 46 42 27 32 49 55 15 46 43 30 44 35 63 48 51 61 40 19 63 47 48 50 60 35 63 48 51 63 48 51 63 48 51 63 46 43 31 46 44 36 68 65 53 8 20 66 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 19 64 51 61 38 10 26 26 25 24 19 64 51 62 44 35 64 52 66 59 30 41 24 17 55 16 51 62 43 32 51 64 52 66 59 31 47 48 51 63 47 45 40 18 58 28 35 63 45 40 17 56 20 65 56 19 63 48 52 68 65 55 13 39 15 48 49 56 20 68 65 53 8 17 56 20 67 63 48 52 65 56 20 68 65 56 17 54 11 32 51 64 52 65 56 20 68 68 65 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1863\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2328\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1492\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 48 51 62 41 23 16 50 60 35 64 51 64 51 63 47 46 43 30 44 35 63 47 47 47 45 38 11 30 43 31 45 39 14 43 30 43 31 48 51 63 47 48 52 67 63 48 51 63 45 38 10 28 35 63 46 43 32 50 60 35 63 47 48 51 64 51 63 48 52 66 59 32 52 66 58 28 36 66 60 36 67 64 51 63 46 42 26 28 34 59 31 46 41 23 16 52 67 64 52 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 5 5 8 17 56 18 59 31 46 42 26 28 33 53 6 9 22 11 30 43 31 48 50 57 24 20 65 56 18 58 27 29 39 15 47 47 47 46 44 36 65 55 14 43 31 46 42 27 29 39 14 43 29 40 20 68 68 66 58 26 28 36 68 68 65 56 19 61 40 19 63 47 45 39 13 40 20 68 68 65 55 15 46 44 35 63 47 45 38 9 24 20 68 67 63 45 39 15 46 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2514\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 13 38 10 26 26 26 26 27 29 38 9 24 20 66 57 21 5 6 10 25 21 5 6 11 29 40 17 56 17 54 11 31 46 43 29 40 20 65 56 20 65 56 17 54 9 23 15 47 45 38 12 36 66 59 31 46 43 30 44 36 65 53 5 5 7 13 39 14 44 33 54 12 36 68 68 68 65 56 20 68 66 58 27 30 44 33 56 18 57 22 10 26 27 31 45 37 6 11 31 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3629\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2885\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 10 27 29 38 11 31 45 39 16 50 59 29 40 17 53 5 8 19 63 47 47 45 40 17 54 12 35 63 45 39 15 45 40 19 62 43 30 42 27 30 42 26 28 35 62 42 26 27 29 37 6 12 34 58 26 25 21 6 12 36 65 53 7 13 39 14 41 21 8 20 66 59 30 44 35 64 49 53 6 10 27 30 41 23 15 46 43 29 39 13 37 5 6 12 34 59 30 42 27 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 12 36 68 66 60 36 67 64 52 66 59 29 39 16 50 60 35 63 47 45 40 17 53 7 16 50 59 31 48 52 67 64 50 58 27 32 51 63 47 47 48 51 63 47 46 43 31 47 45 40 18 58 26 27 31 48 51 64 51 64 51 63 46 43 30 42 26 28 36 67 61 40 19 62 44 36 66 59 29 39 13 38 12 36 66 60 33 56 20 68 66 60 36 68 68 68 66 60 33 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3257\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 15 46 44 36 67 62 43 31 46 43 30 42 25 22 10 26 26 26 26 26 26 25 24 19 61 37 5 6 10 28 35 63 46 42 27 29 37 8 17 53 7 15 46 44 36 66 60 35 61 40 17 56 17 53 5 7 15 48 52 67 61 38 12 35 64 52 65 56 19 61 40 19 61 40 20 67 63 45 40 19 63 48 52 67 63 45 39 15 48 51 63 45 39 15 47 45 37 7 14 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 52 68 68 66 60 36 68 65 55 13 40 17 56 18 60 36 68 67 64 52 67 63 45 38 12 34 58 26 28 34 58 25 24 20 68 68 67 61 40 17 53 8 19 63 46 43 31 47 45 38 10 25 23 16 52 65 56 19 61 39 16 49 56 17 56 17 56 18 59 31 45 38 12 33 53 8 20 66 59 31 47 47 48 51 64 49 55 14 43 31 47 48 52 65 55 14 44 36 65 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1121\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2978\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 63 47 46 43 31 46 41 23 14 42 27 31 47 47 48 50 59 30 44 35 64 50 59 31 48 51 64 50 58 27 32 51 64 52 67 62 43 31 47 48 50 58 26 28 35 61 38 11 29 38 11 30 43 31 45 40 19 63 48 52 67 64 52 67 62 44 35 62 43 31 45 40 20 68 65 56 20 67 62 44 36 68 67 64 51 64 49 54 11 31 46 43 30 43 31 48 51 63 47 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2699\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 38 12 33 56 18 57 22 11 29 37 7 13 37 8 20 68 68 67 62 42 26 27 31 46 44 36 68 65 53 8 17 53 8 19 61 40 18 60 36 66 60 34 59 31 46 43 30 43 29 40 19 62 44 33 56 18 60 35 62 41 24 18 57 22 11 31 47 47 45 40 20 67 62 43 29 37 6 12 33 53 5 7 13 40 20 68 67 62 41 24 17 54 12 36 68 67 62 44 34 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 67 64 50 60 36 67 63 46 44 36 68 67 64 51 61 40 19 61 38 11 30 41 22 9 21 5 8 20 67 64 52 68 67 64 52 67 64 52 67 61 40 17 56 20 67 64 51 63 45 38 10 26 26 28 35 64 49 53 7 16 52 67 64 49 56 19 61 40 19 64 52 67 64 52 67 64 52 67 64 51 64 52 65 56 17 56 17 55 16 51 61 40 19 64 52 65 56 20 66 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 24 17 56 19 62 44 33 54 12 35 62 43 30 44 33 55 14 43 31 45 40 20 67 62 41 24 17 56 20 68 68 67 63 47 48 52 68 65 53 8 19 62 44 36 68 68 66 59 29 39 14 42 27 31 46 44 35 62 43 31 48 50 59 31 47 45 40 18 59 29 39 15 47 46 44 33 53 7 14 44 36 66 59 31 47 47 48 52 68 66 59 31 46 43 29 40 19 63 46 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 33 56 17 56 20 65 54 9 22 9 21 8 17 54 9 22 11 31 45 40 19 61 40 17 55 14 44 36 67 62 44 33 55 16 50 60 35 61 40 18 57 24 20 66 60 35 63 47 45 37 6 9 21 5 6 9 22 10 25 24 18 58 27 31 46 43 31 47 46 42 26 26 27 31 46 42 27 30 43 30 43 30 42 25 22 9 21 8 17 53 5 5 6 11 29 37 6 10 26 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 42 26 26 26 26 26 28 36 65 56 18 58 28 33 55 13 38 9 21 5 6 10 25 22 9 24 17 55 13 40 17 54 12 34 59 30 41 22 10 25 21 6 9 23 14 41 22 11 29 37 6 9 21 7 14 42 28 35 64 50 58 27 29 38 10 25 21 8 17 55 14 42 26 26 28 35 62 41 22 11 29 38 9 21 5 5 7 14 41 21 7 13 38 10 27 30 41 24 18 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 63 48 52 67 64 52 68 68 67 64 52 68 68 68 68 66 60 35 62 42 26 26 25 21 8 17 53 7 15 48 52 67 64 52 67 62 44 36 67 61 40 18 60 36 68 67 62 43 29 40 18 59 32 52 68 68 68 66 60 35 64 52 68 68 67 62 44 34 60 33 55 15 46 41 24 17 56 20 67 62 42 27 29 40 18 60 34 59 32 49 54 11 31 47 47 48 49 56 20 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 5 5 5 5 5 5 5 7 16 51 61 38 9 23 14 43 31 46 41 24 17 53 7 14 41 21 7 13 37 5 6 9 23 13 38 10 26 25 24 19 61 37 8 17 54 10 28 35 61 37 8 18 57 22 12 33 53 8 18 59 29 40 19 61 39 14 43 29 37 5 5 8 18 58 25 21 6 10 28 35 61 38 10 26 27 30 41 22 12 34 57 23 14 41 24 20 65 53 6 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 31 48 51 63 47 45 39 16 51 63 46 43 31 48 50 59 30 43 31 46 43 31 48 51 64 51 62 43 31 48 51 63 45 39 13 40 19 63 48 52 67 63 48 49 55 13 39 15 45 39 13 39 15 45 40 19 63 48 52 68 68 65 56 19 63 48 51 63 48 51 63 48 51 63 48 51 63 48 51 63 45 39 15 46 43 30 44 33 56 19 61 40 19 63 48 52 67 62 44 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 33 53 8 19 62 44 36 65 53 5 7 15 45 38 11 29 38 10 27 30 43 29 40 19 61 37 5 7 14 41 22 11 29 39 13 37 8 20 65 55 13 40 17 53 5 5 7 15 45 37 5 7 13 39 15 48 51 61 38 12 34 58 27 30 43 29 39 14 43 29 38 9 24 20 66 60 36 68 65 53 6 10 28 33 53 7 13 37 6 12 33 56 17 53 7 13 39 14 42 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3536\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2793\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2607\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2235\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2421\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1956\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 17 55 15 47 48 51 63 47 47 45 40 17 56 20 67 62 43 29 37 8 17 53 5 5 7 15 47 48 51 64 52 68 65 56 19 63 46 43 32 51 63 47 48 52 65 55 15 47 45 40 20 65 53 7 14 43 32 50 60 35 62 43 32 52 68 68 67 63 45 37 7 15 48 52 67 62 44 36 68 67 63 48 51 64 49 54 11 32 51 64 50 58 26 27 32 50 59 31 48 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3165\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1771\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2142\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 56 17 56 18 57 22 10 28 35 62 43 30 44 35 63 45 39 13 38 9 23 13 40 17 54 10 28 34 57 22 12 33 54 11 30 42 28 33 56 18 57 21 5 5 5 8 18 59 29 37 7 14 44 34 58 26 28 36 68 66 59 30 42 25 23 13 40 18 59 30 42 27 30 43 31 46 44 36 67 62 42 26 28 35 62 42 25 24 18 58 26 26 27 30 44 34 57 22 10 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 30 44 33 54 9 22 10 25 22 10 25 22 9 21 5 6 10 26 27 29 38 10 26 28 35 62 43 29 38 10 28 35 61 38 9 24 19 61 38 9 24 19 62 42 26 26 26 27 29 37 6 10 25 22 10 26 26 25 23 14 42 27 30 41 21 7 14 42 26 26 26 26 26 28 35 61 38 10 25 22 10 25 22 9 22 10 26 25 21 5 8 18 57 21 8 18 58 26 26 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 6 10 27 31 46 42 28 36 66 59 29 38 9 22 12 33 54 9 21 6 9 23 14 41 23 13 39 15 47 45 37 7 14 42 26 26 26 28 33 56 20 65 53 7 14 41 23 15 45 37 5 7 14 44 34 58 26 26 27 31 45 37 5 8 18 60 33 55 13 40 19 61 39 15 45 38 11 30 42 25 22 12 34 58 27 29 39 13 39 15 45 40 19 61 37 6 12 33 53 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 20 66 60 36 68 68 67 61 40 20 68 67 62 44 36 67 63 46 42 27 30 44 34 60 35 62 41 24 18 59 29 39 14 42 27 31 46 43 30 42 27 31 48 52 66 58 26 27 29 38 9 22 12 33 53 7 14 43 30 43 31 47 48 51 63 45 39 15 47 48 52 68 65 53 7 13 40 18 60 36 67 62 44 35 63 45 39 15 48 50 58 26 28 34 60 34 58 26 26 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 43 30 44 36 67 61 40 18 58 25 24 19 63 47 48 51 63 47 48 51 62 43 32 52 67 64 51 61 40 20 68 67 64 50 60 36 67 62 42 27 30 43 32 50 57 24 19 63 45 38 10 25 24 20 65 53 5 7 13 40 19 61 37 7 15 47 46 42 26 27 29 39 15 46 43 29 40 18 58 26 26 27 30 42 27 29 39 14 43 31 48 52 67 61 38 10 26 28 35 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 23 15 45 39 14 44 33 55 13 38 12 33 55 14 42 26 27 31 45 37 5 5 5 8 19 61 39 13 38 9 21 6 10 26 28 33 53 6 11 29 37 7 13 39 13 37 7 15 45 39 15 47 45 39 13 40 19 63 46 41 21 6 10 25 22 10 25 24 19 61 38 11 29 38 11 31 47 47 47 46 41 23 14 41 22 10 26 26 26 26 25 21 7 15 45 37 5 6 11 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 54 10 28 36 67 62 44 35 63 48 52 68 68 66 59 31 47 48 51 61 40 18 59 31 48 51 63 46 43 29 40 19 63 47 48 51 63 48 51 64 51 63 48 51 63 47 46 43 29 40 18 57 23 13 40 19 62 43 31 48 52 67 63 48 51 63 48 51 64 51 63 48 51 63 46 44 36 67 62 42 26 27 32 50 57 22 10 27 32 50 58 28 34 58 27 30 43 32 52 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 12 33 54 12 36 67 62 44 34 59 30 43 31 45 37 8 18 58 26 28 36 68 68 68 65 53 5 5 6 9 21 6 12 36 67 61 37 5 5 7 13 37 5 5 5 5 6 11 29 39 13 38 10 26 28 33 56 18 57 24 18 58 27 30 42 25 24 17 56 19 61 37 6 10 27 31 46 41 24 17 56 17 54 12 34 57 22 10 26 27 30 42 25 23 15 46 44 36 67 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2422\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2050\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3444\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2515\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3630\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1864\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1679\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 39 13 37 5 6 10 26 25 23 13 39 13 39 15 45 37 5 5 8 20 67 63 47 46 44 34 57 24 20 65 56 19 61 38 12 36 68 65 56 18 57 23 15 45 37 5 8 19 61 40 17 54 9 24 17 53 8 17 56 17 53 6 9 24 19 61 40 20 65 53 5 8 17 53 5 6 9 24 18 57 24 18 58 25 22 11 29 37 8 20 66 59 31 45 39 15 45 40 20 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1586\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 24 19 62 41 21 6 10 26 26 26 25 21 5 5 7 14 42 26 26 26 26 26 25 22 11 30 42 26 26 28 34 57 24 17 56 17 56 17 53 8 20 66 59 30 43 29 39 15 45 38 9 22 10 28 35 63 46 44 36 68 67 62 44 36 66 59 30 43 29 37 5 7 16 51 63 46 41 22 11 31 46 43 30 41 24 18 60 33 54 11 30 44 34 59 30 43 29 37 7 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2794\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2329\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 47 47 48 52 67 64 51 63 47 47 46 43 31 47 45 40 19 63 48 50 58 26 28 33 53 8 19 64 52 67 62 43 32 52 68 67 62 44 35 64 52 67 62 44 36 67 62 43 29 40 17 56 18 60 36 67 64 51 64 52 68 68 68 68 67 64 50 60 36 68 68 67 64 52 66 60 35 62 44 33 56 20 65 56 19 62 44 33 53 8 19 63 48 50 60 36 67 63 45 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 44 33 56 19 63 48 51 61 39 15 48 51 63 47 46 43 31 47 45 40 17 53 8 18 59 30 42 26 28 36 68 65 56 20 67 64 52 67 63 47 47 45 39 16 51 63 46 43 29 40 20 67 64 49 55 14 44 36 66 58 28 34 58 25 23 15 48 49 56 20 65 53 8 17 54 12 36 67 64 52 67 64 51 63 45 40 17 55 15 47 48 49 56 20 67 64 51 62 41 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 13 40 20 65 56 19 62 42 27 29 38 10 26 26 26 25 22 10 26 27 30 43 29 40 17 56 20 66 57 23 14 42 28 33 55 14 41 21 6 10 27 31 46 44 35 63 46 43 29 40 20 65 55 15 46 42 26 28 35 61 39 14 42 28 35 62 43 30 42 28 35 62 44 35 62 42 27 30 44 35 63 46 41 24 17 53 6 12 34 58 28 35 62 42 27 30 42 28 35 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 41 23 14 43 30 44 36 66 60 35 62 44 35 64 51 62 44 36 66 58 27 31 45 40 18 59 32 51 61 38 11 30 44 35 61 40 19 64 51 62 43 31 46 42 28 36 67 63 45 39 14 44 34 60 35 64 50 58 25 23 13 40 19 63 46 44 36 65 56 20 68 68 67 64 51 62 44 34 59 32 52 68 66 59 29 39 15 46 42 28 33 55 15 47 47 45 40 20 66 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 65 53 5 6 12 33 55 14 44 33 56 19 63 47 48 50 59 30 44 36 68 66 59 31 47 47 48 51 63 47 48 51 62 43 32 51 64 51 62 43 31 47 47 46 44 36 67 63 45 39 13 39 15 47 46 43 31 48 51 63 46 44 36 65 55 16 51 61 40 19 61 40 19 63 45 39 15 48 51 63 48 51 64 50 59 31 47 46 43 30 43 30 43 31 45 39 16 49 56 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2979\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2236\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3351\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3072\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 14 43 30 43 30 42 27 32 51 62 44 33 56 17 53 7 16 52 67 63 48 51 62 41 24 19 64 52 68 68 65 55 14 44 33 53 8 20 67 63 48 52 68 65 56 19 63 47 45 39 14 43 31 47 48 49 55 15 47 48 52 68 68 67 62 41 24 19 64 50 60 35 64 50 59 31 47 46 41 24 17 56 18 59 32 49 56 19 64 52 68 68 67 61 37 8 20 68 65 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3258\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1493\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 27 31 48 52 67 62 44 35 62 44 35 63 48 52 67 64 49 53 5 5 8 17 56 19 63 48 49 56 20 68 67 63 48 52 67 64 52 66 60 36 66 60 36 67 64 52 67 63 45 38 12 34 58 28 36 68 65 56 19 61 40 19 61 40 20 66 59 31 48 52 67 64 52 67 64 52 67 62 44 35 63 46 44 34 60 34 60 35 63 48 52 68 67 64 51 64 52 65 56 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 26 25 23 15 45 37 8 20 67 62 42 26 27 29 40 19 61 38 9 23 13 38 9 23 13 38 10 25 22 10 25 21 5 6 10 25 21 6 12 33 53 6 12 33 53 6 12 35 61 38 12 35 62 41 21 7 14 43 29 39 16 50 60 35 62 43 30 42 27 31 45 37 5 6 9 24 19 63 47 46 44 34 60 34 60 34 57 24 20 65 53 5 8 20 68 68 67 63 48 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2886\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2700\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3537\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 25 24 17 53 5 8 20 65 56 20 65 54 12 35 63 46 43 30 41 21 6 12 34 59 29 37 5 6 10 27 30 42 25 22 11 31 45 40 18 59 31 47 47 45 37 8 18 59 29 39 15 46 42 27 31 47 47 46 44 36 67 61 37 8 17 53 6 10 25 24 20 67 63 47 46 42 25 22 9 21 7 14 44 36 65 53 8 18 58 25 23 13 37 7 14 41 23 15 46 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 40 19 62 43 29 38 11 31 46 41 23 13 40 19 62 41 22 10 26 27 29 39 14 42 26 27 30 44 35 61 38 10 28 35 62 42 26 26 28 33 53 5 6 10 28 33 55 15 45 39 16 50 60 35 62 43 30 44 33 53 6 12 34 59 29 38 12 34 60 35 61 40 18 59 29 38 9 23 13 37 5 8 19 62 42 25 21 6 12 36 67 63 47 45 38 11 30 42 28 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 25 21 5 5 6 10 27 29 37 6 9 22 10 28 35 61 38 10 25 23 13 37 5 6 9 22 10 28 35 61 38 10 27 32 51 62 42 26 25 22 9 24 17 55 14 43 31 47 45 39 16 50 60 35 63 45 38 11 31 45 38 9 21 5 5 5 6 11 29 38 10 26 25 21 5 5 5 8 20 66 60 33 53 6 9 22 12 34 60 34 57 21 6 12 34 58 28 35 63 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 63 47 46 42 25 22 9 21 6 11 30 44 35 64 49 55 16 50 60 36 67 63 48 52 67 62 42 27 30 42 27 30 44 35 63 47 48 52 65 56 17 56 20 65 55 16 50 59 29 38 10 26 27 31 48 51 63 48 49 56 18 59 31 47 46 44 33 55 15 46 44 35 62 44 35 62 41 24 20 65 54 11 32 51 64 49 55 16 52 68 65 53 7 14 44 36 65 56 19 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 26 26 26 26 26 26 26 26 26 28 34 57 24 17 56 17 55 16 52 68 68 66 58 26 27 29 39 14 44 34 60 34 58 28 35 63 47 45 40 20 67 62 44 36 66 59 30 43 29 37 5 7 14 43 31 46 44 36 68 67 62 44 33 53 8 19 62 41 22 11 30 44 35 63 47 45 39 15 46 42 28 36 67 61 38 11 31 47 45 37 5 8 18 60 34 58 28 33 56 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 66 58 28 33 53 6 12 33 53 6 11 29 39 13 40 17 53 8 17 53 8 20 66 57 24 19 62 44 36 67 61 37 5 6 12 33 53 8 20 65 56 18 58 25 24 20 66 59 29 39 13 40 18 57 23 14 42 28 35 63 47 46 42 26 26 28 33 56 19 61 38 10 27 30 43 29 37 5 8 18 60 35 61 39 15 47 45 40 19 61 40 20 65 54 9 24 17 56 20 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 45 37 7 13 38 12 36 66 60 33 53 5 7 15 47 46 44 34 59 30 43 30 41 23 14 41 21 5 5 5 6 9 23 13 37 5 5 5 6 10 25 24 19 63 45 40 20 67 61 38 12 36 66 60 36 67 61 40 20 66 60 35 63 46 44 34 57 21 6 11 31 47 45 40 19 62 41 23 14 42 28 36 68 65 56 20 67 62 44 33 56 20 67 61 40 20 65 56 17 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 39 13 38 12 33 53 7 13 38 9 24 17 53 6 11 29 40 17 53 6 11 29 37 5 5 6 9 23 14 42 25 21 5 6 10 28 34 60 33 56 17 53 5 5 8 20 67 63 45 40 20 67 64 51 64 52 66 60 36 67 62 43 29 39 16 51 63 46 44 34 57 21 6 11 31 47 45 40 19 64 51 62 42 26 27 32 52 65 56 20 67 63 48 49 56 20 67 64 52 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 63 45 38 11 30 44 36 67 62 44 34 60 36 65 53 5 6 9 23 13 40 19 61 40 20 67 61 40 19 61 37 5 8 19 61 37 5 8 18 59 31 47 45 39 15 47 46 43 29 38 12 36 65 56 19 62 44 36 68 68 67 62 43 31 45 40 20 68 65 56 20 65 56 19 61 40 17 55 16 49 56 20 68 65 56 17 53 5 8 17 55 15 47 45 40 20 67 61 40 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2608\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2051\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1772\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3445\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1865\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2516\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3631\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2795\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 68 66 57 21 6 11 30 43 29 40 20 68 66 60 33 56 17 55 16 49 56 20 67 62 41 24 19 64 51 64 49 56 20 66 57 23 16 52 67 62 41 24 17 56 19 64 50 59 29 38 10 26 27 31 48 52 67 62 43 32 49 53 6 12 35 63 47 48 52 67 61 40 19 63 48 50 60 36 67 64 52 67 62 41 24 17 56 19 64 50 58 27 31 46 43 31 47 47 45 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2330\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 48 50 57 21 8 17 56 17 53 8 17 56 20 65 54 10 26 28 34 58 27 29 39 14 44 34 60 35 63 46 41 22 11 30 43 30 44 34 59 29 38 9 22 10 27 29 39 15 45 37 8 18 59 30 44 36 68 67 62 42 28 33 56 20 68 66 60 33 53 6 12 35 61 40 17 54 10 27 29 39 13 40 18 60 33 53 8 17 53 5 5 5 8 20 68 68 68 66 60 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 21 7 14 44 35 63 46 42 26 27 30 43 29 40 20 67 61 38 9 24 17 53 8 18 57 21 5 5 5 8 17 53 8 18 57 21 5 5 8 20 66 57 21 7 16 52 67 63 45 40 18 58 28 35 63 47 46 43 32 52 67 63 46 43 30 43 31 45 38 10 28 36 66 60 36 65 54 10 26 28 35 61 40 17 56 18 60 35 62 44 33 53 5 6 10 27 29 39 15 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1957\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 17 55 13 40 17 53 5 8 20 65 56 20 68 65 55 15 45 40 20 67 61 38 12 36 66 60 36 67 62 43 32 50 60 35 63 46 41 22 9 21 5 6 11 31 47 45 40 19 61 39 14 42 26 28 36 68 65 56 20 67 63 45 37 8 20 67 64 52 68 67 61 40 17 54 11 29 39 15 46 44 33 56 20 66 59 29 40 20 68 68 66 58 27 29 37 8 17 55 15 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 6 10 26 25 21 6 10 26 26 25 21 6 12 35 63 45 40 20 65 53 8 18 59 31 47 45 37 7 15 48 50 59 31 47 46 43 31 46 43 29 37 8 18 59 31 48 51 63 45 39 16 51 63 45 37 7 15 47 46 41 22 9 23 13 40 18 58 27 31 45 37 5 7 15 48 51 63 45 39 15 47 47 47 45 39 15 47 47 45 37 6 12 35 62 42 25 24 17 56 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3166\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 47 46 42 27 31 48 51 63 46 44 35 63 48 51 63 46 42 27 31 45 39 14 43 31 48 51 63 47 47 46 44 36 67 64 52 65 56 20 65 53 8 18 60 33 54 12 34 59 29 39 13 40 20 67 63 47 47 45 38 12 34 60 33 56 19 64 52 65 54 10 28 35 61 37 7 13 39 13 38 12 35 61 40 19 62 44 35 63 46 44 36 65 56 17 56 17 56 20 68 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2143\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 33 53 6 12 33 54 9 21 7 14 44 35 62 42 26 28 35 62 44 36 68 68 65 53 8 20 68 65 56 19 63 45 38 12 33 53 7 14 43 31 45 37 8 17 53 5 6 11 29 40 19 61 40 18 60 35 62 44 35 62 43 31 45 40 18 57 24 18 57 21 7 14 42 26 26 27 31 47 47 47 45 40 17 54 12 33 54 12 36 67 62 44 35 63 46 42 28 35 63 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 16 51 61 37 8 20 68 68 68 67 64 50 60 34 59 32 49 56 19 63 46 44 33 56 20 68 65 56 20 68 68 66 59 32 52 67 63 46 44 35 64 52 68 66 57 24 19 63 45 40 17 56 20 67 62 44 36 68 68 67 64 52 67 62 44 34 60 36 65 56 19 63 46 44 34 60 36 66 60 36 68 65 56 19 64 52 67 61 38 10 28 34 60 36 68 68 67 61 40 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2423\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 5 8 18 58 25 24 17 53 5 6 9 22 9 22 12 34 57 22 10 26 25 22 10 26 25 22 10 26 25 22 11 30 44 35 62 44 33 53 8 20 65 53 6 12 33 55 15 47 45 37 6 11 30 44 33 55 14 44 34 58 26 25 21 6 10 25 23 14 41 23 13 37 5 8 20 66 59 29 38 11 31 45 40 17 54 12 36 67 63 45 37 7 14 41 21 5 5 6 11 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 47 47 48 51 63 46 43 31 48 51 63 47 47 47 48 52 67 62 44 36 67 64 50 60 33 56 19 62 44 36 68 66 60 34 58 26 27 31 46 44 35 63 46 43 30 43 30 43 29 40 18 59 31 48 52 68 66 58 26 28 36 65 56 17 55 14 43 31 46 44 35 64 50 59 31 46 43 31 48 49 55 14 42 26 26 27 29 38 12 36 66 57 24 20 68 65 56 20 68 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 62 42 28 33 53 7 15 47 48 49 56 17 56 20 66 60 36 65 56 20 66 58 28 35 61 40 18 60 33 56 19 63 48 52 68 65 54 11 32 50 60 35 63 45 39 14 44 35 61 39 14 43 31 45 40 19 63 46 44 36 68 67 62 44 35 61 40 17 56 18 57 21 5 5 7 13 37 7 16 50 59 30 43 29 37 5 5 5 5 5 5 6 9 21 6 9 21 6 9 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 44 36 67 62 43 31 46 44 36 66 57 21 5 5 6 12 33 54 12 36 67 62 44 33 56 20 67 63 46 43 32 52 68 66 59 31 48 50 60 33 56 19 63 46 43 32 51 63 45 38 11 32 52 68 65 56 20 68 68 65 55 15 48 51 64 49 56 18 60 35 64 52 68 67 61 40 17 56 19 64 52 67 63 47 48 52 68 68 66 59 32 52 68 66 60 36 68 68 68 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 56 20 68 67 64 52 68 68 67 63 46 44 35 62 42 27 30 43 31 46 43 29 40 19 62 42 27 29 40 20 67 64 52 67 62 44 35 64 49 55 16 49 56 19 63 47 46 43 29 40 20 67 64 49 53 7 15 46 43 30 43 32 52 67 62 42 26 27 31 47 48 51 64 51 64 52 67 64 51 63 48 51 63 46 43 30 42 28 35 62 44 35 64 51 63 46 43 31 48 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3446\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3632\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 7 15 46 42 28 36 65 53 8 19 64 52 65 56 20 66 58 28 35 61 40 18 60 33 56 19 62 44 33 53 5 7 15 45 38 12 35 63 45 38 10 28 35 61 39 14 43 31 45 40 19 63 46 44 36 68 67 61 40 19 61 37 8 17 56 19 64 49 53 5 7 14 43 31 45 38 11 29 39 13 39 13 39 13 39 13 39 13 39 13 39 13 39 13 39 13 39 13 39 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 58 27 29 38 11 29 38 9 21 6 11 29 40 18 57 22 12 36 65 53 5 6 12 36 67 61 38 10 26 26 28 33 55 14 41 24 20 65 53 5 5 6 12 34 59 31 46 43 29 38 10 27 30 42 25 21 8 20 68 65 54 12 35 61 40 20 67 62 44 33 53 8 18 59 30 42 27 29 40 19 63 45 38 10 26 27 29 38 10 28 34 57 22 9 24 20 67 62 43 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2980\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2237\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3352\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3259\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3538\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2887\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 45 39 14 41 23 13 39 16 51 62 44 35 62 41 21 6 10 26 26 26 28 34 57 22 10 26 26 26 25 24 18 57 24 17 56 17 54 12 36 68 68 66 58 26 27 29 39 15 45 38 12 34 58 28 36 67 63 45 40 20 67 61 40 20 66 59 30 44 36 65 53 7 14 43 31 46 44 33 55 15 46 43 29 37 5 6 12 33 54 11 31 45 39 15 46 44 35 63 46 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 39 14 42 27 29 38 12 35 61 40 17 54 9 22 12 36 65 54 9 23 14 41 21 6 12 36 66 58 25 22 12 33 53 5 7 13 40 17 54 9 21 7 15 46 42 25 23 15 45 39 14 44 33 55 15 47 45 37 5 5 7 14 42 28 35 63 47 45 38 9 22 12 33 55 14 44 35 61 37 8 17 53 5 6 12 33 53 5 5 6 12 33 54 10 26 26 28 33 54 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2701\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3073\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 39 16 52 68 65 55 16 52 68 67 61 40 20 65 56 19 62 44 36 65 56 19 62 43 31 48 50 60 35 63 48 51 63 46 44 34 57 23 14 43 31 47 48 51 63 46 42 27 29 38 10 26 27 31 47 45 38 11 32 50 60 35 62 44 33 56 20 67 64 52 68 66 60 36 67 61 38 12 36 67 64 52 65 56 17 53 8 20 65 54 12 33 55 13 39 15 48 52 65 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 10 28 36 66 59 29 40 17 56 19 64 51 63 47 48 52 67 64 51 62 42 27 30 44 36 66 58 28 36 67 63 48 52 67 63 47 46 44 35 62 41 22 11 29 38 11 31 47 45 40 17 56 18 60 35 61 38 10 28 35 64 52 68 68 67 63 48 51 62 42 27 31 46 42 26 27 31 48 51 62 43 32 52 67 62 44 34 58 26 26 27 31 46 44 35 64 51 61 40 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 9 23 14 41 21 8 18 59 30 44 34 58 28 36 67 62 42 26 26 27 29 38 9 22 10 27 29 37 8 19 62 44 33 54 10 26 27 31 46 42 25 22 12 36 66 58 28 35 61 37 8 17 54 12 36 66 58 28 34 59 29 37 6 12 36 68 68 67 62 43 31 45 40 20 67 61 38 11 29 39 13 39 15 47 47 47 45 38 10 27 29 37 8 20 67 61 40 20 66 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 26 28 36 67 61 40 19 62 44 34 58 28 35 61 39 15 47 47 45 39 14 44 36 67 61 37 5 6 11 31 46 41 22 10 27 29 38 9 23 14 42 27 30 42 26 27 31 47 45 38 10 27 29 39 15 45 38 10 28 35 63 45 38 10 28 36 65 56 17 53 6 10 28 33 54 10 26 27 30 43 30 43 30 43 31 46 43 30 41 23 14 43 30 41 21 7 15 46 42 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 40 19 63 46 44 34 59 32 52 67 63 45 39 15 48 50 59 31 48 51 64 49 56 19 63 48 49 56 19 63 48 52 65 55 16 50 60 33 55 16 50 59 29 39 16 51 62 43 29 39 14 42 27 31 48 51 63 48 51 63 48 50 59 32 52 68 68 68 67 62 44 35 64 51 64 52 68 68 68 67 62 44 34 60 36 67 62 43 31 48 52 68 65 55 14 44 35 64 52 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 43 30 44 33 55 14 42 26 27 31 45 39 14 42 28 35 63 46 44 35 61 40 17 53 5 5 5 7 14 44 33 54 10 28 36 67 62 42 26 28 35 62 42 26 26 27 30 43 29 37 8 19 61 40 20 68 67 61 40 20 68 67 61 40 18 57 22 9 21 7 14 44 36 66 57 22 9 24 17 56 17 55 13 40 17 55 14 42 27 31 46 41 21 8 20 65 54 9 23 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2609\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 47 46 43 31 47 48 52 67 62 42 27 30 44 35 62 43 31 45 39 15 46 44 33 55 16 51 63 46 44 35 64 51 61 40 18 57 21 8 18 57 21 8 19 63 46 44 35 63 45 40 17 55 13 39 13 39 14 44 34 60 33 55 16 52 67 62 44 35 63 46 44 33 53 8 19 62 41 24 18 60 33 56 18 59 32 51 64 52 67 64 51 63 48 51 64 51 61 39 14 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-1958\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2144\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3167\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 27 31 46 42 28 35 63 47 46 43 30 41 24 19 63 47 47 46 43 29 39 15 46 43 31 47 48 51 61 40 17 55 15 47 47 46 42 27 31 46 44 33 54 12 36 67 63 47 45 38 10 26 27 31 48 52 67 64 52 67 64 52 67 63 48 51 64 52 67 62 44 35 64 51 63 46 43 30 44 33 56 20 68 65 53 8 18 60 36 67 64 52 68 66 59 30 44 36 68 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 68 68 65 54 10 26 26 26 27 30 41 22 10 27 29 40 18 60 34 58 26 28 36 65 54 10 26 25 24 19 63 45 38 10 26 26 26 26 27 30 41 21 8 18 58 26 26 27 29 38 9 21 5 6 10 27 29 40 18 58 26 25 22 9 23 13 38 10 28 34 57 23 13 37 5 6 12 34 60 34 60 34 60 34 57 22 9 22 12 34 60 34 58 26 28 34 59 30 43 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 26 26 26 26 26 26 26 25 21 5 7 15 47 46 42 26 27 32 51 62 42 27 31 47 48 51 63 47 48 49 53 6 9 21 6 9 21 6 9 21 5 5 5 8 19 63 47 47 45 38 10 28 36 65 56 18 60 33 56 20 67 64 52 68 68 68 66 60 36 67 64 52 67 64 52 67 61 37 7 15 48 51 64 52 67 64 52 68 68 68 66 57 22 11 31 48 52 68 68 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3353\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2981\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3260\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3539\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2888\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 29 37 6 9 21 6 9 21 8 20 66 58 28 33 53 5 8 20 67 63 46 43 29 38 12 36 68 65 53 5 5 8 18 59 31 46 42 28 34 59 30 43 29 38 9 22 12 35 61 37 7 13 39 14 41 24 17 54 12 33 54 9 22 12 36 65 53 5 5 5 7 15 45 39 15 46 42 25 22 10 26 26 27 31 45 39 15 46 41 21 6 10 26 25 24 19 64 49 56 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-3074\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 22 10 26 25 22 10 26 25 22 10 27 32 49 56 17 55 13 40 20 68 66 59 30 43 29 39 14 43 30 44 34 59 29 39 15 47 45 40 20 67 62 44 36 65 56 18 60 35 61 40 18 60 36 66 57 23 16 49 55 15 46 43 29 40 19 62 43 29 39 14 44 35 61 37 7 15 46 42 26 25 23 15 46 43 31 47 48 52 65 54 10 27 29 37 8 19 61 37 6 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   guid: train-2702\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 42 27 30 43 30 43 32 51 64 50 59 32 52 67 63 47 48 52 67 63 47 47 46 43 31 46 44 35 63 46 43 32 52 67 62 43 32 50 59 31 47 46 43 31 46 42 27 31 45 40 17 53 7 16 50 59 31 47 48 52 68 67 62 43 31 46 44 35 63 48 49 56 18 59 29 40 17 53 8 17 53 5 6 12 36 68 65 55 14 43 31 47 46 43 31 48 51 64 49 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 19 61 40 17 56 20 66 58 28 35 61 40 18 60 33 56 19 63 48 49 56 17 54 11 29 39 16 51 63 45 38 10 28 35 61 39 15 47 47 45 40 19 63 46 44 36 68 67 61 37 7 13 37 8 17 56 20 68 65 53 5 7 14 43 31 48 50 59 30 43 29 37 5 7 13 37 7 13 37 5 5 5 5 5 8 17 53 6 11 31 46 44 36 66 58 26 25 24 17 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 68 65 54 12 33 56 20 68 68 67 64 52 68 68 66 60 35 61 37 7 15 48 52 65 53 8 18 59 32 51 61 40 18 57 22 10 27 30 43 32 52 67 62 43 32 52 67 63 45 38 10 25 22 10 27 30 44 34 60 35 63 46 43 32 52 67 62 44 35 63 48 52 65 53 8 20 68 67 62 43 32 50 58 27 31 46 44 34 60 34 59 29 38 11 30 43 31 46 41 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 45 38 12 34 58 27 31 46 42 27 31 45 40 18 60 36 68 67 63 46 44 36 68 67 62 42 26 27 30 43 31 46 42 26 28 35 63 46 42 26 28 34 57 23 15 45 39 15 45 39 15 46 43 31 46 44 33 53 5 8 17 56 19 64 49 54 11 29 39 14 42 28 34 59 29 39 15 47 46 42 27 31 46 44 35 63 47 46 44 35 61 38 12 35 63 45 40 19 62 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   input_ids: 2 40 20 68 66 60 34 59 31 46 44 36 66 60 36 68 67 63 46 42 28 34 58 27 30 44 35 62 42 26 28 34 60 33 54 12 33 55 13 40 19 61 40 19 63 47 46 43 31 45 37 8 17 55 16 52 66 59 29 37 8 19 62 42 28 33 55 15 45 38 9 22 9 21 5 6 10 25 23 15 48 50 57 22 9 21 5 6 9 21 7 15 45 38 12 33 54 12 36 68 3\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:48 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:49 - INFO - __main__ -   Saving features into cached file ./dataset/bert/classic/m6Am/cached_train_m6Am_101_dnaprom\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "12/20/2023 22:23:50 - INFO - __main__ -   ***** Running training *****\n",
      "12/20/2023 22:23:50 - INFO - __main__ -     Num examples = 3747\n",
      "12/20/2023 22:23:50 - INFO - __main__ -     Num Epochs = 25\n",
      "12/20/2023 22:23:50 - INFO - __main__ -     Instantaneous batch size per GPU = 32\n",
      "12/20/2023 22:23:50 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "12/20/2023 22:23:50 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "12/20/2023 22:23:50 - INFO - __main__ -     Total optimization steps = 2950\n",
      "12/20/2023 22:23:50 - INFO - __main__ -     Continuing training from checkpoint, will skip to saved global_step\n",
      "12/20/2023 22:23:50 - INFO - __main__ -     Continuing training from epoch 0\n",
      "12/20/2023 22:23:50 - INFO - __main__ -     Continuing training from global step 0\n",
      "12/20/2023 22:23:50 - INFO - __main__ -     Will skip the first 0 steps in the first epoch\n",
      "Epoch:   0%|                                             | 0/25 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                        | 0/118 [00:00<?, ?it/s]\u001b[A/root/.conda/envs/pytorch/lib/python3.7/site-packages/apex/amp/wrap.py:101: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541035/work/torch/csrc/utils/python_arg_parser.cpp:1420.)\n",
      "  return orig_fn(arg0, *args, **kwargs)\n",
      "\n",
      "Iteration:   1%|▎                               | 1/118 [00:00<01:13,  1.59it/s]\u001b[A\n",
      "Iteration:   3%|▊                               | 3/118 [00:00<00:23,  4.92it/s]\u001b[A\n",
      "Iteration:   4%|█▎                              | 5/118 [00:00<00:14,  7.99it/s]\u001b[A\n",
      "Iteration:   6%|█▉                              | 7/118 [00:00<00:10, 10.64it/s]\u001b[A\n",
      "Iteration:   8%|██▍                             | 9/118 [00:01<00:08, 12.84it/s]\u001b[A\n",
      "Iteration:   9%|██▉                            | 11/118 [00:01<00:07, 14.58it/s]\u001b[A\n",
      "Iteration:  11%|███▍                           | 13/118 [00:01<00:06, 15.92it/s]\u001b[A\n",
      "Iteration:  13%|███▉                           | 15/118 [00:01<00:06, 16.91it/s]\u001b[A\n",
      "Iteration:  14%|████▍                          | 17/118 [00:01<00:05, 17.51it/s]\u001b[A\n",
      "Iteration:  16%|████▉                          | 19/118 [00:01<00:05, 18.08it/s]\u001b[A\n",
      "Iteration:  18%|█████▌                         | 21/118 [00:01<00:05, 18.48it/s]\u001b[A\n",
      "Iteration:  19%|██████                         | 23/118 [00:01<00:05, 18.78it/s]\u001b[A\n",
      "Iteration:  21%|██████▌                        | 25/118 [00:01<00:04, 18.98it/s]\u001b[A\n",
      "Iteration:  23%|███████                        | 27/118 [00:01<00:04, 19.08it/s]\u001b[A\n",
      "Iteration:  25%|███████▌                       | 29/118 [00:02<00:04, 19.19it/s]\u001b[A\n",
      "Iteration:  26%|████████▏                      | 31/118 [00:02<00:04, 19.16it/s]\u001b[A\n",
      "Iteration:  28%|████████▋                      | 33/118 [00:02<00:04, 19.24it/s]\u001b[A\n",
      "Iteration:  30%|█████████▏                     | 35/118 [00:02<00:04, 19.30it/s]\u001b[A\n",
      "Iteration:  31%|█████████▋                     | 37/118 [00:02<00:04, 19.31it/s]\u001b[A\n",
      "Iteration:  33%|██████████▏                    | 39/118 [00:02<00:04, 19.35it/s]\u001b[A\n",
      "Iteration:  35%|██████████▊                    | 41/118 [00:02<00:03, 19.39it/s]\u001b[A\n",
      "Iteration:  36%|███████████▎                   | 43/118 [00:02<00:03, 19.40it/s]\u001b[A\n",
      "Iteration:  38%|███████████▊                   | 45/118 [00:02<00:03, 19.36it/s]\u001b[A\n",
      "Iteration:  40%|████████████▎                  | 47/118 [00:03<00:03, 19.40it/s]\u001b[A\n",
      "Iteration:  42%|████████████▊                  | 49/118 [00:03<00:03, 19.41it/s]\u001b[A\n",
      "Iteration:  43%|█████████████▍                 | 51/118 [00:03<00:03, 19.43it/s]\u001b[A\n",
      "Iteration:  45%|█████████████▉                 | 53/118 [00:03<00:03, 19.44it/s]\u001b[A\n",
      "Iteration:  47%|██████████████▍                | 55/118 [00:03<00:03, 19.46it/s]\u001b[A\n",
      "Iteration:  48%|██████████████▉                | 57/118 [00:03<00:03, 19.35it/s]\u001b[A\n",
      "Iteration:  50%|███████████████▌               | 59/118 [00:03<00:03, 19.38it/s]\u001b[A\n",
      "Iteration:  52%|████████████████               | 61/118 [00:03<00:02, 19.41it/s]\u001b[A\n",
      "Iteration:  53%|████████████████▌              | 63/118 [00:03<00:02, 19.42it/s]\u001b[A\n",
      "Iteration:  55%|█████████████████              | 65/118 [00:03<00:02, 19.43it/s]\u001b[A\n",
      "Iteration:  57%|█████████████████▌             | 67/118 [00:04<00:02, 19.39it/s]\u001b[A\n",
      "Iteration:  58%|██████████████████▏            | 69/118 [00:04<00:02, 19.41it/s]\u001b[A\n",
      "Iteration:  60%|██████████████████▋            | 71/118 [00:04<00:02, 19.28it/s]\u001b[A\n",
      "Iteration:  62%|███████████████████▏           | 73/118 [00:04<00:02, 19.23it/s]\u001b[A\n",
      "Iteration:  64%|███████████████████▋           | 75/118 [00:04<00:02, 19.24it/s]\u001b[A\n",
      "Iteration:  65%|████████████████████▏          | 77/118 [00:04<00:02, 19.27it/s]\u001b[A\n",
      "Iteration:  67%|████████████████████▊          | 79/118 [00:04<00:02, 19.32it/s]\u001b[A\n",
      "Iteration:  69%|█████████████████████▎         | 81/118 [00:04<00:01, 19.36it/s]\u001b[A\n",
      "Iteration:  70%|█████████████████████▊         | 83/118 [00:04<00:01, 19.39it/s]\u001b[A\n",
      "Iteration:  72%|██████████████████████▎        | 85/118 [00:04<00:01, 19.39it/s]\u001b[A\n",
      "Iteration:  74%|██████████████████████▊        | 87/118 [00:05<00:01, 19.42it/s]\u001b[A\n",
      "Iteration:  75%|███████████████████████▍       | 89/118 [00:05<00:01, 19.43it/s]\u001b[A\n",
      "Iteration:  77%|███████████████████████▉       | 91/118 [00:05<00:01, 19.45it/s]\u001b[AGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "\n",
      "Iteration:  80%|████████████████████████▋      | 94/118 [00:05<00:01, 20.01it/s]\u001b[A\n",
      "Iteration:  81%|█████████████████████████▏     | 96/118 [00:05<00:01, 19.87it/s]\u001b[A\n",
      "Iteration:  83%|█████████████████████████▋     | 98/118 [00:05<00:01, 19.75it/s]\u001b[A12/20/2023 22:23:55 - INFO - __main__ -   Creating features from dataset file at ./dataset/bert/classic/m6Am\n",
      "finish loading examples\n",
      "number of processes for converting feature: 10\n",
      "1 processor started !\n",
      "2 processor started !\n",
      "3 processor started !\n",
      "4 processor started !\n",
      "5 processor started !\n",
      "6 processor started !\n",
      "7 processor started !\n",
      "8 processor started !\n",
      "9 processor started !\n",
      "10 processor started !\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 22 9 22 11 31 46 41 22 9 22 10 28 36 67 63 45 39 14 44 34 60 34 58 28 36 67 63 46 42 26 26 27 29 40 18 59 29 40 18 58 25 22 12 34 58 26 26 27 29 40 18 59 29 37 6 10 25 22 10 25 21 6 11 30 42 26 26 28 35 61 39 14 41 22 11 30 42 28 34 59 29 37 5 6 9 22 9 21 5 5 6 10 27 30 42 26 25 21 6 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-94\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 22 10 27 31 46 44 34 59 30 44 35 61 40 17 55 14 44 33 56 19 63 48 51 62 44 34 60 34 59 31 47 46 43 29 40 18 60 36 66 57 24 17 56 17 56 18 60 35 61 39 13 39 13 39 13 37 5 8 20 65 53 8 17 56 18 57 24 17 53 5 8 17 54 11 32 52 65 53 8 18 59 29 40 17 56 18 59 30 44 36 68 65 54 11 30 44 33 55 13 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-187\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 25 22 10 25 23 13 38 10 26 27 31 46 41 22 10 26 25 21 6 12 33 56 18 60 35 62 42 26 27 29 38 12 34 60 35 63 45 40 19 61 39 14 42 26 28 35 62 43 29 40 18 57 23 14 42 28 36 66 60 36 68 65 54 10 26 26 25 22 11 29 38 9 24 20 67 62 43 30 42 27 29 38 10 28 35 61 40 20 67 63 47 45 38 10 26 26 25 23 13 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-280\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 63 47 47 47 48 52 67 64 51 63 48 51 64 51 64 52 65 53 7 14 43 32 51 64 52 68 66 58 27 32 52 65 56 19 63 48 51 63 47 48 51 62 44 33 56 20 66 59 29 40 17 53 8 20 65 56 20 67 64 50 59 30 44 35 64 51 62 44 33 54 11 32 52 68 66 59 31 48 51 63 48 51 64 51 64 51 63 45 40 17 56 19 63 45 40 17 56 18 59 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   Writing example 0/93\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   Writing example 0/100\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-373\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 68 67 63 47 46 43 31 47 47 46 44 36 68 67 62 43 29 40 20 67 63 46 44 36 67 62 43 31 45 37 6 11 31 45 37 6 10 25 24 18 58 26 26 27 30 42 27 31 45 40 17 54 10 25 24 17 56 17 53 8 19 63 45 37 5 6 10 27 29 37 5 6 11 30 43 30 44 34 59 30 43 30 44 34 57 24 19 61 39 15 45 40 19 62 41 22 9 21 5 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 33 56 17 55 13 37 8 20 66 59 30 42 28 35 62 43 30 44 34 59 29 39 15 47 45 40 20 67 62 44 36 65 56 18 57 23 13 40 18 60 36 67 61 39 16 49 54 11 29 38 12 35 63 46 42 28 33 55 15 46 43 31 46 44 36 68 67 63 47 45 37 8 19 61 37 6 11 31 46 43 31 47 45 39 15 46 43 29 37 7 15 46 42 27 31 48 49 56 18 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-95\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 5 5 8 17 56 20 67 61 37 5 5 6 11 30 42 26 26 27 30 42 25 24 19 61 40 17 54 10 25 24 19 62 42 26 28 34 59 30 41 22 10 26 26 27 30 41 23 15 45 39 13 37 5 6 12 35 62 43 30 42 27 31 46 42 27 31 47 45 38 11 31 46 43 31 48 51 61 37 7 14 44 34 59 29 37 6 10 25 21 6 12 33 53 5 8 19 62 41 23 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-466\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 35 64 51 64 52 67 61 40 19 62 43 31 47 48 51 62 44 35 62 44 33 53 6 9 24 19 62 44 33 56 17 56 20 67 64 51 61 39 15 47 48 52 68 67 64 52 67 63 45 40 19 62 44 36 68 67 62 43 32 52 65 56 18 60 35 61 40 19 64 52 68 68 66 58 25 24 20 65 54 12 36 65 55 16 49 56 20 65 55 16 50 60 34 58 28 35 61 39 15 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-188\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 8 18 57 21 6 11 30 42 27 29 37 8 20 67 63 45 39 13 40 18 57 22 12 34 58 27 30 43 30 43 31 45 37 5 7 13 39 13 38 12 34 58 26 26 27 29 39 15 45 39 16 49 56 20 68 65 54 12 34 57 24 17 54 11 30 44 34 60 34 60 35 63 47 46 42 25 22 10 28 36 67 62 42 26 25 23 14 42 26 26 26 27 29 39 14 42 25 23 14 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-559\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 51 62 43 32 51 63 45 38 12 36 65 54 12 35 63 48 52 66 60 34 60 33 55 14 44 33 53 5 8 18 60 36 65 55 14 41 21 5 6 12 34 60 33 55 14 43 30 43 29 39 15 45 38 10 27 32 52 67 62 42 27 30 42 25 22 12 35 61 39 16 52 65 53 5 8 20 65 53 8 18 57 24 20 65 53 8 19 61 38 11 29 38 10 28 36 68 65 53 8 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-281\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 60 36 67 63 46 44 35 64 52 68 67 64 52 66 58 25 22 9 21 5 8 17 56 20 67 61 40 18 60 36 66 60 35 64 51 64 51 64 51 64 52 67 63 48 52 67 62 43 29 40 18 60 35 62 44 35 63 48 52 68 67 61 39 15 48 52 68 68 67 64 52 67 64 52 68 66 58 28 36 66 59 30 41 23 16 51 62 44 34 60 35 64 51 64 52 67 64 52 65 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-652\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 34 58 27 29 39 14 41 23 14 41 24 17 56 20 67 62 44 34 59 29 40 20 65 56 20 68 67 62 42 26 28 33 56 20 66 58 26 27 30 41 24 20 68 65 56 18 59 31 45 37 6 12 33 54 11 30 44 33 54 10 28 36 65 54 11 30 44 34 60 34 58 26 26 27 29 40 17 53 8 20 66 60 36 67 62 43 30 44 35 63 45 40 18 59 32 52 67 62 42 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 40 18 60 36 67 64 51 61 37 6 11 30 43 32 52 67 62 43 29 39 14 44 35 61 37 8 19 62 43 31 48 51 62 42 27 31 47 48 52 68 66 58 27 29 39 16 51 63 45 38 10 27 30 43 31 46 44 35 63 46 43 29 40 19 63 46 43 31 47 48 49 56 18 57 24 19 62 44 36 68 65 55 14 41 23 13 40 20 67 64 51 63 47 48 51 63 45 39 15 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-374\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 13 38 9 22 10 28 34 60 34 58 25 24 17 54 12 33 54 9 21 8 17 54 12 33 56 17 56 17 53 8 20 65 53 8 17 53 5 7 13 38 9 21 5 6 10 26 26 27 29 37 8 18 57 24 18 58 26 27 31 45 37 5 5 8 17 53 8 19 62 44 36 68 66 60 34 57 22 9 22 11 29 38 10 28 35 63 47 45 39 15 45 40 17 53 6 12 34 58 27 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-96\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-745\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-838\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 67 61 40 20 65 56 18 58 27 31 46 44 36 65 53 6 9 24 18 57 21 5 8 18 59 29 38 10 26 26 25 21 8 18 58 28 33 53 8 17 56 18 57 21 5 5 7 15 45 38 12 34 57 21 5 6 9 22 9 24 17 54 9 24 18 57 22 12 34 60 33 54 10 28 34 58 25 21 6 12 36 66 58 26 26 28 36 66 58 27 30 43 29 40 19 63 46 43 31 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 24 20 66 58 25 22 12 33 54 12 34 60 33 54 10 28 35 62 43 29 37 8 19 63 47 45 37 6 11 30 44 36 68 65 55 14 44 36 68 65 53 5 5 7 16 52 67 63 45 39 13 38 10 26 28 35 63 45 38 9 22 11 32 49 54 10 27 30 44 35 61 40 19 61 40 17 54 10 28 33 53 6 10 25 24 17 54 11 30 41 21 5 5 8 19 63 45 39 15 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 52 66 57 21 7 16 49 55 13 40 20 67 63 48 52 68 68 67 63 47 47 48 51 63 47 47 47 47 48 49 53 8 18 57 24 20 68 67 64 50 60 34 60 36 67 64 50 59 29 39 14 42 27 31 48 52 67 62 42 27 31 46 42 27 29 40 18 59 31 48 51 62 44 36 66 59 31 47 48 49 56 19 61 39 16 49 56 19 62 44 34 60 33 56 20 68 68 65 54 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-467\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-189\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 17 53 8 17 56 17 53 8 19 63 46 43 32 50 57 21 5 5 7 15 47 48 52 68 68 67 61 37 5 5 5 6 9 22 11 29 40 20 68 66 60 33 53 8 17 55 14 43 29 39 14 42 25 23 15 47 46 43 30 42 28 36 65 53 5 8 19 62 44 36 66 57 23 13 40 20 65 53 8 20 65 53 8 18 59 30 44 34 60 36 67 62 44 36 66 59 30 43 31 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 45 38 10 27 30 43 31 46 44 35 63 46 43 29 40 19 63 46 42 27 31 48 49 56 18 57 24 19 62 44 36 68 65 55 14 41 23 13 40 20 67 64 50 60 35 61 39 15 45 39 15 45 39 13 39 15 47 48 52 67 62 41 21 6 10 27 31 45 38 10 26 26 26 26 25 21 6 10 28 33 56 20 66 58 26 26 26 28 34 58 26 26 28 36 68 66 58 25 22 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-560\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-282\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 68 65 54 12 36 65 56 18 59 30 43 32 51 62 41 22 12 34 59 32 51 62 43 32 52 68 65 54 12 36 65 56 18 59 30 43 32 51 62 41 22 12 34 59 32 51 62 43 29 40 20 67 62 44 36 65 56 18 60 35 61 40 18 57 24 19 61 39 15 48 50 57 23 14 44 36 67 62 43 29 39 14 44 35 61 37 7 15 46 43 31 45 39 15 46 43 31 46 44 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 34 60 33 56 19 61 40 19 64 52 66 60 36 65 53 7 13 38 10 26 25 23 13 40 17 55 13 39 16 49 56 19 64 50 59 29 39 13 40 18 60 36 67 62 44 36 68 67 61 37 8 20 67 63 47 48 50 60 36 67 62 44 33 56 18 60 35 61 39 16 52 66 60 36 67 63 47 46 44 36 68 67 64 52 68 68 67 62 43 31 46 43 30 44 34 58 28 36 68 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-653\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 44 36 66 60 35 64 51 62 44 35 61 39 15 47 45 39 14 41 21 7 14 43 32 50 59 29 38 11 30 41 24 19 61 38 10 25 24 20 66 57 22 9 22 11 30 43 31 47 45 37 6 12 35 62 41 22 11 31 47 46 43 31 47 47 47 47 46 43 31 47 47 47 47 45 39 15 47 47 45 39 16 49 55 13 40 18 59 31 46 43 29 40 17 56 18 60 34 60 33 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 59 32 51 64 51 62 42 27 31 48 52 68 67 64 52 65 55 16 49 56 19 61 40 19 64 51 64 51 62 43 31 45 40 18 60 33 55 16 50 59 29 39 16 52 67 64 51 63 45 39 14 42 26 27 31 48 52 67 63 48 52 66 60 33 55 13 40 17 56 18 59 31 45 40 19 64 52 65 56 18 58 28 34 60 36 68 68 68 67 63 48 52 68 68 68 67 64 51 63 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-375\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 40 18 60 35 64 51 63 47 48 51 64 51 64 50 57 21 7 16 51 63 48 49 56 19 64 49 55 16 51 64 51 63 46 44 33 56 17 56 18 60 33 54 12 33 55 13 38 11 29 40 17 56 18 60 35 64 51 63 48 50 59 32 50 59 32 51 63 48 51 63 48 51 63 45 39 15 45 39 14 44 34 59 32 51 63 45 38 12 33 53 7 13 40 18 60 34 60 36 68 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-97\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 12 36 67 63 47 45 40 20 68 68 66 59 29 38 11 30 44 33 54 10 27 29 40 19 64 51 63 47 46 44 35 63 46 43 30 43 31 45 38 9 24 17 53 7 13 37 7 15 45 40 19 63 46 41 24 17 53 8 19 63 46 41 24 20 68 67 62 43 31 46 44 35 62 44 36 67 63 46 44 33 55 15 47 46 42 27 32 52 68 65 54 10 27 31 47 45 39 14 41 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-746\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-839\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 15 47 47 46 41 21 8 19 63 47 45 40 19 63 47 46 43 32 52 67 62 43 31 47 45 39 15 47 46 42 28 34 60 34 57 23 15 46 44 36 68 67 63 48 49 53 7 15 45 38 10 27 29 39 15 48 52 65 56 19 64 51 64 51 61 40 19 64 52 68 66 60 36 65 56 18 60 34 60 36 67 62 43 32 52 65 56 20 65 55 15 48 51 64 52 67 64 52 68 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 57 24 19 64 52 66 60 36 67 64 52 66 59 32 51 64 52 67 62 44 34 60 36 67 63 48 52 68 68 68 65 53 8 18 60 33 53 6 12 36 66 58 26 26 25 23 15 47 45 40 17 56 20 68 67 63 47 46 44 35 64 51 63 48 51 63 46 42 26 27 30 43 31 48 51 62 44 36 67 61 37 7 16 52 67 64 51 63 48 51 62 43 31 47 47 48 51 62 43 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-190\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-468\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 50 59 29 39 16 51 63 48 52 67 63 47 48 49 55 16 51 62 43 32 52 67 63 45 40 20 68 66 60 33 56 19 63 47 47 48 51 64 50 59 31 47 48 52 67 64 51 63 45 39 14 43 32 52 67 63 47 45 40 20 68 67 61 40 20 68 65 55 15 47 47 48 51 63 45 39 16 52 67 63 48 52 68 65 55 15 48 51 63 47 48 52 67 63 47 48 52 67 63 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 33 53 8 17 53 5 5 6 11 30 44 36 68 68 68 65 54 10 28 36 68 65 54 12 36 68 68 68 66 57 22 12 34 60 34 60 36 65 54 10 26 26 26 27 31 46 43 31 45 39 14 41 21 5 6 11 30 44 35 61 40 20 65 53 5 7 13 38 12 34 58 28 33 53 7 13 38 9 21 5 6 10 27 29 37 5 5 5 6 10 26 26 25 22 11 31 47 45 38 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-283\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-561\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 65 53 5 7 15 47 45 37 8 18 58 26 26 27 29 37 8 17 54 9 21 5 5 8 18 59 31 45 37 8 18 58 26 28 36 68 67 61 37 8 18 58 26 25 22 11 30 43 29 39 13 37 8 19 61 39 13 39 14 42 26 25 21 8 20 68 68 66 58 28 36 66 60 36 67 61 38 12 35 62 42 25 24 19 63 46 42 28 36 67 64 50 60 33 54 11 30 43 32 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 65 54 10 27 29 39 15 45 39 14 42 28 36 67 62 43 32 49 54 9 22 10 25 23 13 37 8 17 55 15 45 37 8 19 64 49 56 18 59 31 46 43 31 47 46 43 31 47 45 37 6 10 28 36 65 53 5 6 9 22 12 33 53 6 11 29 38 11 30 41 23 13 40 19 63 46 43 30 44 35 63 47 46 44 36 66 57 24 19 61 38 9 21 5 6 10 26 28 34 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-654\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 47 48 50 60 33 56 19 63 46 43 30 44 35 62 43 30 41 23 16 51 63 47 47 46 44 36 65 54 12 36 68 68 67 64 50 60 35 61 38 10 27 31 47 46 41 23 15 47 45 40 19 61 40 19 63 47 46 43 32 51 64 51 64 52 66 59 31 48 52 67 61 39 13 40 19 64 52 65 55 13 39 15 45 40 20 65 55 14 43 31 45 37 5 5 6 12 36 67 64 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 20 65 53 6 10 26 26 28 33 55 14 42 26 27 29 37 8 19 63 46 42 25 22 10 25 22 10 26 25 22 10 25 21 6 9 21 8 17 53 7 14 41 22 9 22 10 26 27 29 38 12 33 56 20 67 62 41 21 5 6 12 35 62 44 34 57 24 17 54 9 24 18 60 33 54 10 27 31 46 42 26 28 33 54 12 36 65 54 11 30 44 36 65 55 13 37 5 8 18 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-376\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 12 33 54 11 29 40 17 53 6 9 22 10 28 36 68 65 54 12 33 54 9 24 18 58 28 34 60 35 62 44 35 62 44 33 54 12 35 62 44 35 62 41 24 19 62 43 30 43 29 39 13 38 12 36 68 68 67 61 40 18 60 36 67 63 46 44 33 56 17 53 5 5 7 15 45 37 5 8 20 68 67 64 49 54 9 21 8 19 61 38 12 33 55 14 41 21 5 5 6 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-98\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 27 31 46 43 32 52 68 67 62 44 35 61 37 6 10 27 29 37 7 15 45 37 6 11 31 45 37 8 18 57 23 14 43 32 52 66 60 34 58 28 33 53 8 20 68 67 62 43 29 40 18 60 33 55 13 37 6 10 25 21 5 8 17 54 12 36 67 62 44 35 64 51 63 47 45 38 12 34 57 21 7 13 38 11 29 39 14 41 24 19 64 49 55 15 48 52 66 60 33 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-747\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-840\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 34 60 35 63 45 39 13 38 12 33 54 11 31 47 48 51 61 40 19 62 44 34 59 31 47 45 39 14 42 28 36 68 67 63 45 40 20 68 67 61 39 13 37 8 20 65 55 15 45 37 7 16 50 60 34 57 22 12 33 56 20 68 67 63 48 51 63 46 44 35 64 49 56 18 60 36 65 56 20 68 67 61 40 19 64 52 68 68 65 55 15 47 46 44 33 56 20 65 56 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 57 23 13 37 8 19 64 49 54 10 27 30 43 31 46 44 35 63 46 43 29 40 19 63 46 43 31 47 48 49 53 6 10 28 35 62 44 33 56 17 54 9 21 7 16 52 68 67 61 39 15 46 44 35 63 45 39 14 44 35 64 51 63 47 48 52 67 62 41 21 6 10 26 26 26 28 34 57 22 10 26 26 26 25 24 18 57 24 17 56 17 54 12 36 68 68 66 58 26 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-191\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-469\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 42 26 28 36 65 53 8 20 67 63 48 49 56 20 67 64 52 68 67 61 40 17 54 11 29 40 18 58 28 33 56 20 67 61 39 16 52 65 56 18 58 27 32 49 56 17 55 15 45 40 19 63 46 44 36 67 63 45 37 6 9 22 12 36 66 60 33 53 5 7 15 47 47 45 38 11 30 42 28 33 55 14 41 21 5 5 5 6 9 23 13 37 5 5 5 6 10 25 24 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 36 68 68 68 65 54 11 29 38 10 27 30 41 23 15 47 47 45 37 7 13 40 19 62 42 28 36 67 62 42 28 33 53 7 13 40 18 58 25 21 5 5 8 17 54 12 35 63 45 39 14 41 24 18 60 35 62 41 23 15 45 38 10 26 28 34 59 29 40 20 65 56 17 56 17 56 19 63 47 46 43 30 43 29 40 20 65 53 6 11 31 45 40 20 67 63 45 37 5 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-562\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-284\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 60 34 58 25 22 10 28 36 67 61 40 17 56 20 68 65 53 7 13 40 19 62 42 28 34 57 23 16 49 53 8 17 55 15 47 46 44 33 54 12 35 63 46 41 21 5 8 19 61 37 7 13 37 8 18 60 33 54 12 36 68 66 60 34 59 30 44 36 68 68 67 62 43 29 38 12 35 62 43 29 39 13 39 13 37 8 17 56 19 62 42 28 33 53 7 14 42 28 33 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 39 13 37 6 11 30 43 32 52 67 62 43 29 39 14 44 35 61 37 7 15 46 43 31 45 38 11 30 41 23 14 44 36 68 68 66 59 29 37 8 18 60 34 60 33 54 10 27 29 39 15 46 44 35 63 46 43 29 40 19 63 46 43 31 47 48 49 56 18 57 23 15 47 48 52 65 55 14 41 23 13 40 20 67 64 50 60 35 61 39 15 45 39 15 45 39 16 50 59 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-655\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 18 60 33 54 9 24 19 61 38 9 22 12 33 53 5 6 9 21 5 6 10 26 26 28 36 66 59 29 40 18 57 24 17 54 10 27 30 44 34 59 29 40 18 60 34 60 34 59 29 38 9 22 9 22 9 21 6 9 21 6 9 21 5 6 12 34 58 25 23 14 42 25 22 10 25 22 10 26 25 22 10 26 28 33 54 10 26 26 26 26 27 31 47 46 44 33 53 8 20 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-377\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 31 47 47 45 37 8 19 61 39 13 39 14 41 23 13 40 20 67 62 44 33 53 8 20 68 68 66 60 34 59 30 41 23 13 38 12 35 63 47 47 47 47 46 44 35 62 42 27 29 40 19 63 46 42 28 33 53 8 20 67 62 41 24 20 68 65 56 20 65 53 8 20 65 53 8 18 57 22 10 28 36 65 54 12 36 67 61 38 11 30 43 31 47 45 40 19 63 46 44 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-748\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-841\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 17 55 16 50 58 27 31 47 47 45 40 19 61 37 8 17 56 17 55 15 48 52 67 64 51 62 44 36 67 63 45 37 8 19 64 51 62 41 23 16 49 56 19 64 49 55 15 47 45 39 15 47 46 44 36 66 60 36 65 56 19 62 44 35 63 48 51 61 39 16 50 60 35 64 52 68 67 64 51 63 47 47 47 48 51 63 48 51 63 47 47 47 48 51 63 48 51 63 47 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 62 42 27 30 43 30 42 26 27 29 40 19 63 47 46 42 26 26 25 24 20 66 58 25 24 20 65 53 5 5 5 7 13 37 5 8 17 54 11 31 46 41 23 14 42 28 36 67 61 38 9 21 5 7 13 37 5 5 6 12 34 59 29 37 7 13 40 17 53 8 17 54 12 35 64 51 61 37 6 9 21 6 12 34 59 30 41 22 9 21 5 5 5 6 11 30 44 33 56 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-470\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 16 51 61 39 13 40 17 55 13 39 15 47 47 47 47 48 51 63 47 45 39 13 40 19 63 47 45 40 17 55 16 51 63 47 47 47 46 43 31 47 46 43 32 51 62 44 35 63 45 39 15 47 47 48 49 55 15 47 48 50 59 30 43 32 52 65 56 19 62 44 36 65 55 14 44 36 68 66 60 35 63 48 49 55 14 42 27 30 42 27 32 51 61 40 17 53 8 20 68 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-563\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 58 28 33 55 13 38 10 26 26 26 25 22 10 26 28 33 54 9 21 8 19 61 39 15 46 41 21 5 6 12 33 53 6 12 34 60 35 61 39 14 44 34 60 33 56 17 56 19 61 38 9 21 6 12 36 65 53 8 17 55 13 37 5 8 19 61 38 12 34 58 25 22 9 22 10 26 25 23 13 37 8 20 66 60 34 59 29 38 10 27 29 40 18 58 28 34 57 21 5 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-656\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 51 63 46 43 30 44 35 63 48 51 63 48 51 64 49 56 18 57 24 20 65 53 8 19 64 51 64 49 56 19 64 51 63 48 52 68 67 61 39 15 48 52 68 67 62 44 34 59 29 40 18 60 33 56 19 63 46 44 36 68 68 67 63 45 40 19 64 52 68 67 64 49 56 19 63 45 40 19 64 52 67 62 43 31 48 52 67 62 43 31 47 48 51 62 43 31 47 48 51 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-749\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   guid: dev-842\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 47 48 51 64 51 63 47 48 49 56 20 65 53 8 19 63 47 47 48 51 63 47 47 48 51 64 51 61 40 19 63 47 47 48 50 59 31 47 48 51 63 47 47 46 42 27 31 47 45 38 11 32 50 60 34 57 23 16 52 66 59 31 47 48 51 64 50 60 36 67 62 44 35 64 51 64 51 64 52 67 64 51 62 43 30 44 36 68 65 56 18 57 23 16 49 55 13 38 12 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   input_ids: 2 67 64 49 55 15 46 43 32 52 65 56 19 61 40 17 53 7 15 47 45 37 7 15 46 43 31 48 49 56 19 61 40 18 57 23 13 38 12 35 62 41 21 8 17 55 14 42 27 29 39 15 45 40 18 59 29 37 5 8 19 64 49 53 7 14 41 23 14 41 22 9 23 14 43 29 37 6 10 28 33 54 11 31 45 37 6 9 21 7 14 42 28 33 55 15 45 37 7 16 3\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/20/2023 22:23:56 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "12/20/2023 22:23:56 - INFO - __main__ -   Saving features into cached file ./dataset/bert/classic/m6Am/cached_dev_m6Am_101_dnaprom\n",
      "12/20/2023 22:23:56 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/20/2023 22:23:56 - INFO - __main__ -     Num examples = 937\n",
      "12/20/2023 22:23:56 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                        | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  30%|█████████▌                      | 9/30 [00:00<00:00, 87.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  63%|███████████████████▋           | 19/30 [00:00<00:00, 91.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████| 30/30 [00:00<00:00, 92.54it/s]\u001b[A\u001b[A\n",
      "12/20/2023 22:23:56 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/20/2023 22:23:56 - INFO - __main__ -     acc = 0.847385272145144\n",
      "12/20/2023 22:23:56 - INFO - __main__ -     auc = 0.9287860110487148\n",
      "12/20/2023 22:23:56 - INFO - __main__ -     f1 = 0.8473602369492781\n",
      "12/20/2023 22:23:56 - INFO - __main__ -     mcc = 0.6965780552784521\n",
      "12/20/2023 22:23:56 - INFO - __main__ -     precision = 0.8479936372503442\n",
      "12/20/2023 22:23:56 - INFO - __main__ -     recall = 0.8485846687668356\n",
      "/root/.conda/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "{\"eval_acc\": 0.847385272145144, \"eval_f1\": 0.8473602369492781, \"eval_mcc\": 0.6965780552784521, \"eval_auc\": 0.9287860110487148, \"eval_precision\": 0.8479936372503442, \"eval_recall\": 0.8485846687668356, \"learning_rate\": 2.88135593220339e-05, \"loss\": 0.5580141504108905, \"step\": 100}\n",
      "12/20/2023 22:23:56 - INFO - transformers.configuration_utils -   Configuration saved in ./results/your_models/bert/classic/m6Am/checkpoint-100/config.json\n",
      "12/20/2023 22:23:56 - INFO - transformers.modeling_utils -   Model weights saved in ./results/your_models/bert/classic/m6Am/checkpoint-100/pytorch_model.bin\n",
      "12/20/2023 22:23:56 - INFO - __main__ -   Saving model checkpoint to ./results/your_models/bert/classic/m6Am/checkpoint-100\n",
      "12/20/2023 22:23:57 - INFO - __main__ -   Saving optimizer and scheduler states to ./results/your_models/bert/classic/m6Am/checkpoint-100\n",
      "\n",
      "Iteration:  85%|█████████████████████████▍    | 100/118 [00:07<00:05,  3.50it/s]\u001b[A\n",
      "Iteration:  86%|█████████████████████████▉    | 102/118 [00:07<00:03,  4.58it/s]\u001b[A\n",
      "Iteration:  88%|██████████████████████████▍   | 104/118 [00:07<00:02,  5.90it/s]\u001b[A\n",
      "Iteration:  90%|██████████████████████████▉   | 106/118 [00:07<00:01,  7.43it/s]\u001b[A\n",
      "Iteration:  92%|███████████████████████████▍  | 108/118 [00:07<00:01,  9.10it/s]\u001b[A\n",
      "Iteration:  93%|███████████████████████████▉  | 110/118 [00:07<00:00, 10.81it/s]\u001b[A\n",
      "Iteration:  95%|████████████████████████████▍ | 112/118 [00:07<00:00, 12.45it/s]\u001b[A\n",
      "Iteration:  97%|████████████████████████████▉ | 114/118 [00:08<00:00, 13.94it/s]\u001b[A\n",
      "Iteration:  98%|█████████████████████████████▍| 116/118 [00:08<00:00, 15.23it/s]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████| 118/118 [00:08<00:00, 14.21it/s]\u001b[A\n",
      "Epoch:   4%|█▍                                   | 1/25 [00:08<03:19,  8.30s/it]\n",
      "Iteration:   0%|                                        | 0/118 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▌                               | 2/118 [00:00<00:05, 19.40it/s]\u001b[A\n",
      "Iteration:   3%|█                               | 4/118 [00:00<00:05, 19.40it/s]\u001b[A\n",
      "Iteration:   5%|█▋                              | 6/118 [00:00<00:05, 19.41it/s]\u001b[A\n",
      "Iteration:   7%|██▏                             | 8/118 [00:00<00:05, 19.43it/s]\u001b[A\n",
      "Iteration:   8%|██▋                            | 10/118 [00:00<00:05, 19.42it/s]\u001b[A\n",
      "Iteration:  10%|███▏                           | 12/118 [00:00<00:05, 19.43it/s]\u001b[A\n",
      "Iteration:  12%|███▋                           | 14/118 [00:00<00:05, 19.43it/s]\u001b[A\n",
      "Iteration:  14%|████▏                          | 16/118 [00:00<00:05, 19.44it/s]\u001b[A\n",
      "Iteration:  15%|████▋                          | 18/118 [00:00<00:05, 19.44it/s]\u001b[A\n",
      "Iteration:  17%|█████▎                         | 20/118 [00:01<00:05, 19.43it/s]\u001b[A\n",
      "Iteration:  19%|█████▊                         | 22/118 [00:01<00:04, 19.44it/s]\u001b[A\n",
      "Iteration:  20%|██████▎                        | 24/118 [00:01<00:04, 19.45it/s]\u001b[A\n",
      "Iteration:  22%|██████▊                        | 26/118 [00:01<00:04, 19.45it/s]\u001b[A\n",
      "Iteration:  24%|███████▎                       | 28/118 [00:01<00:04, 19.45it/s]\u001b[A\n",
      "Iteration:  25%|███████▉                       | 30/118 [00:01<00:04, 19.46it/s]\u001b[A\n",
      "Iteration:  27%|████████▍                      | 32/118 [00:01<00:04, 19.43it/s]\u001b[A\n",
      "Iteration:  29%|████████▉                      | 34/118 [00:01<00:04, 19.38it/s]\u001b[A\n",
      "Iteration:  31%|█████████▍                     | 36/118 [00:01<00:04, 19.41it/s]\u001b[A\n",
      "Iteration:  32%|█████████▉                     | 38/118 [00:01<00:04, 19.42it/s]\u001b[A\n",
      "Iteration:  34%|██████████▌                    | 40/118 [00:02<00:04, 19.42it/s]\u001b[A\n",
      "Iteration:  36%|███████████                    | 42/118 [00:02<00:03, 19.43it/s]\u001b[A\n",
      "Iteration:  37%|███████████▌                   | 44/118 [00:02<00:03, 19.41it/s]\u001b[A\n",
      "Iteration:  39%|████████████                   | 46/118 [00:02<00:03, 19.42it/s]\u001b[A\n",
      "Iteration:  41%|████████████▌                  | 48/118 [00:02<00:03, 19.43it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▏                 | 50/118 [00:02<00:03, 19.45it/s]\u001b[A\n",
      "Iteration:  44%|█████████████▋                 | 52/118 [00:02<00:03, 19.45it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▏                | 54/118 [00:02<00:03, 19.45it/s]\u001b[A\n",
      "Iteration:  47%|██████████████▋                | 56/118 [00:02<00:03, 19.46it/s]\u001b[A\n",
      "Iteration:  49%|███████████████▏               | 58/118 [00:02<00:03, 19.45it/s]\u001b[A\n",
      "Iteration:  51%|███████████████▊               | 60/118 [00:03<00:02, 19.46it/s]\u001b[A\n",
      "Iteration:  53%|████████████████▎              | 62/118 [00:03<00:02, 19.46it/s]\u001b[A\n",
      "Iteration:  54%|████████████████▊              | 64/118 [00:03<00:02, 19.45it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▎             | 66/118 [00:03<00:02, 19.47it/s]\u001b[A\n",
      "Iteration:  58%|█████████████████▊             | 68/118 [00:03<00:02, 19.45it/s]\u001b[A\n",
      "Iteration:  59%|██████████████████▍            | 70/118 [00:03<00:02, 19.45it/s]\u001b[A\n",
      "Iteration:  61%|██████████████████▉            | 72/118 [00:03<00:02, 19.42it/s]\u001b[A\n",
      "Iteration:  63%|███████████████████▍           | 74/118 [00:03<00:02, 19.43it/s]\u001b[A\n",
      "Iteration:  64%|███████████████████▉           | 76/118 [00:03<00:02, 19.43it/s]\u001b[A\n",
      "Iteration:  66%|████████████████████▍          | 78/118 [00:04<00:02, 19.43it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████          | 80/118 [00:04<00:01, 19.43it/s]\u001b[A12/20/2023 22:24:02 - INFO - __main__ -   Loading features from cached file ./dataset/bert/classic/m6Am/cached_dev_m6Am_101_dnaprom\n",
      "12/20/2023 22:24:02 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/20/2023 22:24:02 - INFO - __main__ -     Num examples = 937\n",
      "12/20/2023 22:24:02 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                        | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|██████████▎                    | 10/30 [00:00<00:00, 93.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|████████████████████▋          | 20/30 [00:00<00:00, 94.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████| 30/30 [00:00<00:00, 95.10it/s]\u001b[A\u001b[A\n",
      "12/20/2023 22:24:02 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/20/2023 22:24:02 - INFO - __main__ -     acc = 0.8569903948772679\n",
      "12/20/2023 22:24:02 - INFO - __main__ -     auc = 0.9325092453088617\n",
      "12/20/2023 22:24:02 - INFO - __main__ -     f1 = 0.8569889288805869\n",
      "12/20/2023 22:24:02 - INFO - __main__ -     mcc = 0.7181187873970405\n",
      "12/20/2023 22:24:02 - INFO - __main__ -     precision = 0.8591725599839191\n",
      "12/20/2023 22:24:02 - INFO - __main__ -     recall = 0.8589462630689859\n",
      "/root/.conda/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "{\"eval_acc\": 0.8569903948772679, \"eval_f1\": 0.8569889288805869, \"eval_mcc\": 0.7181187873970405, \"eval_auc\": 0.9325092453088617, \"eval_precision\": 0.8591725599839191, \"eval_recall\": 0.8589462630689859, \"learning_rate\": 2.7683615819209038e-05, \"loss\": 0.308576442450285, \"step\": 200}\n",
      "12/20/2023 22:24:02 - INFO - transformers.configuration_utils -   Configuration saved in ./results/your_models/bert/classic/m6Am/checkpoint-200/config.json\n",
      "12/20/2023 22:24:03 - INFO - transformers.modeling_utils -   Model weights saved in ./results/your_models/bert/classic/m6Am/checkpoint-200/pytorch_model.bin\n",
      "12/20/2023 22:24:03 - INFO - __main__ -   Saving model checkpoint to ./results/your_models/bert/classic/m6Am/checkpoint-200\n",
      "12/20/2023 22:24:03 - INFO - __main__ -   Saving optimizer and scheduler states to ./results/your_models/bert/classic/m6Am/checkpoint-200\n",
      "\n",
      "Iteration:  69%|█████████████████████▌         | 82/118 [00:05<00:08,  4.20it/s]\u001b[A\n",
      "Iteration:  71%|██████████████████████         | 84/118 [00:05<00:06,  5.47it/s]\u001b[A\n",
      "Iteration:  73%|██████████████████████▌        | 86/118 [00:05<00:04,  6.97it/s]\u001b[A\n",
      "Iteration:  75%|███████████████████████        | 88/118 [00:05<00:03,  8.63it/s]\u001b[A\n",
      "Iteration:  76%|███████████████████████▋       | 90/118 [00:05<00:02, 10.36it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▏      | 92/118 [00:05<00:02, 12.04it/s]\u001b[A\n",
      "Iteration:  80%|████████████████████████▋      | 94/118 [00:06<00:01, 13.58it/s]\u001b[A\n",
      "Iteration:  81%|█████████████████████████▏     | 96/118 [00:06<00:01, 14.92it/s]\u001b[A\n",
      "Iteration:  83%|█████████████████████████▋     | 98/118 [00:06<00:01, 16.04it/s]\u001b[A\n",
      "Iteration:  85%|█████████████████████████▍    | 100/118 [00:06<00:01, 16.91it/s]\u001b[A\n",
      "Iteration:  86%|█████████████████████████▉    | 102/118 [00:06<00:00, 17.60it/s]\u001b[A\n",
      "Iteration:  88%|██████████████████████████▍   | 104/118 [00:06<00:00, 18.12it/s]\u001b[A\n",
      "Iteration:  90%|██████████████████████████▉   | 106/118 [00:06<00:00, 18.50it/s]\u001b[A\n",
      "Iteration:  92%|███████████████████████████▍  | 108/118 [00:06<00:00, 18.78it/s]\u001b[A\n",
      "Iteration:  93%|███████████████████████████▉  | 110/118 [00:06<00:00, 18.97it/s]\u001b[A\n",
      "Iteration:  95%|████████████████████████████▍ | 112/118 [00:07<00:00, 19.11it/s]\u001b[A\n",
      "Iteration:  97%|████████████████████████████▉ | 114/118 [00:07<00:00, 19.20it/s]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████| 118/118 [00:07<00:00, 16.13it/s]\u001b[A\n",
      "Epoch:   8%|██▉                                  | 2/25 [00:15<02:57,  7.72s/it]\n",
      "Iteration:   0%|                                        | 0/118 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▌                               | 2/118 [00:00<00:05, 19.41it/s]\u001b[A\n",
      "Iteration:   3%|█                               | 4/118 [00:00<00:05, 19.43it/s]\u001b[A\n",
      "Iteration:   5%|█▋                              | 6/118 [00:00<00:05, 19.43it/s]\u001b[A\n",
      "Iteration:   7%|██▏                             | 8/118 [00:00<00:05, 19.37it/s]\u001b[A\n",
      "Iteration:   8%|██▋                            | 10/118 [00:00<00:05, 19.40it/s]\u001b[A\n",
      "Iteration:  10%|███▏                           | 12/118 [00:00<00:05, 19.42it/s]\u001b[A\n",
      "Iteration:  12%|███▋                           | 14/118 [00:00<00:05, 19.43it/s]\u001b[A\n",
      "Iteration:  14%|████▏                          | 16/118 [00:00<00:05, 19.43it/s]\u001b[A\n",
      "Iteration:  15%|████▋                          | 18/118 [00:00<00:05, 19.43it/s]\u001b[A\n",
      "Iteration:  17%|█████▎                         | 20/118 [00:01<00:05, 19.43it/s]\u001b[A\n",
      "Iteration:  19%|█████▊                         | 22/118 [00:01<00:04, 19.40it/s]\u001b[A\n",
      "Iteration:  20%|██████▎                        | 24/118 [00:01<00:04, 19.41it/s]\u001b[A\n",
      "Iteration:  22%|██████▊                        | 26/118 [00:01<00:04, 19.42it/s]\u001b[A\n",
      "Iteration:  24%|███████▎                       | 28/118 [00:01<00:04, 19.42it/s]\u001b[A\n",
      "Iteration:  25%|███████▉                       | 30/118 [00:01<00:04, 19.42it/s]\u001b[A\n",
      "Iteration:  27%|████████▍                      | 32/118 [00:01<00:04, 19.43it/s]\u001b[A\n",
      "Iteration:  29%|████████▉                      | 34/118 [00:01<00:04, 19.43it/s]\u001b[A\n",
      "Iteration:  31%|█████████▍                     | 36/118 [00:01<00:04, 19.44it/s]\u001b[A\n",
      "Iteration:  32%|█████████▉                     | 38/118 [00:01<00:04, 19.44it/s]\u001b[A\n",
      "Iteration:  34%|██████████▌                    | 40/118 [00:02<00:04, 19.45it/s]\u001b[A\n",
      "Iteration:  36%|███████████                    | 42/118 [00:02<00:03, 19.46it/s]\u001b[A\n",
      "Iteration:  37%|███████████▌                   | 44/118 [00:02<00:03, 19.46it/s]\u001b[A\n",
      "Iteration:  39%|████████████                   | 46/118 [00:02<00:03, 19.40it/s]\u001b[A\n",
      "Iteration:  41%|████████████▌                  | 48/118 [00:02<00:03, 19.40it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▏                 | 50/118 [00:02<00:03, 19.41it/s]\u001b[A\n",
      "Iteration:  44%|█████████████▋                 | 52/118 [00:02<00:03, 19.40it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▏                | 54/118 [00:02<00:03, 19.40it/s]\u001b[A\n",
      "Iteration:  47%|██████████████▋                | 56/118 [00:02<00:03, 19.40it/s]\u001b[A\n",
      "Iteration:  49%|███████████████▏               | 58/118 [00:02<00:03, 19.41it/s]\u001b[A\n",
      "Iteration:  51%|███████████████▊               | 60/118 [00:03<00:02, 19.41it/s]\u001b[A\n",
      "Iteration:  53%|████████████████▎              | 62/118 [00:03<00:02, 19.39it/s]\u001b[A12/20/2023 22:24:09 - INFO - __main__ -   Loading features from cached file ./dataset/bert/classic/m6Am/cached_dev_m6Am_101_dnaprom\n",
      "12/20/2023 22:24:09 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/20/2023 22:24:09 - INFO - __main__ -     Num examples = 937\n",
      "12/20/2023 22:24:09 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                        | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|██████████▎                    | 10/30 [00:00<00:00, 93.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|████████████████████▋          | 20/30 [00:00<00:00, 94.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████| 30/30 [00:00<00:00, 95.03it/s]\u001b[A\u001b[A\n",
      "12/20/2023 22:24:09 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/20/2023 22:24:09 - INFO - __main__ -     acc = 0.8537886872998933\n",
      "12/20/2023 22:24:09 - INFO - __main__ -     auc = 0.9307971510751953\n",
      "12/20/2023 22:24:09 - INFO - __main__ -     f1 = 0.8537880211625748\n",
      "12/20/2023 22:24:09 - INFO - __main__ -     mcc = 0.7115236353622568\n",
      "12/20/2023 22:24:09 - INFO - __main__ -     precision = 0.8558349319448251\n",
      "12/20/2023 22:24:09 - INFO - __main__ -     recall = 0.8556887184403963\n",
      "/root/.conda/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "{\"eval_acc\": 0.8537886872998933, \"eval_f1\": 0.8537880211625748, \"eval_mcc\": 0.7115236353622568, \"eval_auc\": 0.9307971510751953, \"eval_precision\": 0.8558349319448251, \"eval_recall\": 0.8556887184403963, \"learning_rate\": 2.6553672316384183e-05, \"loss\": 0.271200038716197, \"step\": 300}\n",
      "12/20/2023 22:24:09 - INFO - transformers.configuration_utils -   Configuration saved in ./results/your_models/bert/classic/m6Am/checkpoint-300/config.json\n",
      "12/20/2023 22:24:09 - INFO - transformers.modeling_utils -   Model weights saved in ./results/your_models/bert/classic/m6Am/checkpoint-300/pytorch_model.bin\n",
      "12/20/2023 22:24:09 - INFO - __main__ -   Saving model checkpoint to ./results/your_models/bert/classic/m6Am/checkpoint-300\n",
      "12/20/2023 22:24:10 - INFO - __main__ -   Saving optimizer and scheduler states to ./results/your_models/bert/classic/m6Am/checkpoint-300\n",
      "\n",
      "Iteration:  54%|████████████████▊              | 64/118 [00:04<00:12,  4.25it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▎             | 66/118 [00:04<00:09,  5.53it/s]\u001b[A\n",
      "Iteration:  58%|█████████████████▊             | 68/118 [00:04<00:07,  7.04it/s]\u001b[A\n",
      "Iteration:  59%|██████████████████▍            | 70/118 [00:04<00:05,  8.71it/s]\u001b[A\n",
      "Iteration:  61%|██████████████████▉            | 72/118 [00:04<00:04, 10.44it/s]\u001b[A\n",
      "Iteration:  63%|███████████████████▍           | 74/118 [00:05<00:03, 12.10it/s]\u001b[A\n",
      "Iteration:  64%|███████████████████▉           | 76/118 [00:05<00:03, 13.64it/s]\u001b[A\n",
      "Iteration:  66%|████████████████████▍          | 78/118 [00:05<00:02, 14.97it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████          | 80/118 [00:05<00:02, 16.07it/s]\u001b[A\n",
      "Iteration:  69%|█████████████████████▌         | 82/118 [00:05<00:02, 16.95it/s]\u001b[A\n",
      "Iteration:  71%|██████████████████████         | 84/118 [00:05<00:01, 17.62it/s]\u001b[A\n",
      "Iteration:  73%|██████████████████████▌        | 86/118 [00:05<00:01, 18.13it/s]\u001b[A\n",
      "Iteration:  75%|███████████████████████        | 88/118 [00:05<00:01, 18.51it/s]\u001b[A\n",
      "Iteration:  76%|███████████████████████▋       | 90/118 [00:05<00:01, 18.77it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▏      | 92/118 [00:05<00:01, 18.86it/s]\u001b[A\n",
      "Iteration:  80%|████████████████████████▋      | 94/118 [00:06<00:01, 19.02it/s]\u001b[A\n",
      "Iteration:  81%|█████████████████████████▏     | 96/118 [00:06<00:01, 19.15it/s]\u001b[A\n",
      "Iteration:  83%|█████████████████████████▋     | 98/118 [00:06<00:01, 19.24it/s]\u001b[A\n",
      "Iteration:  85%|█████████████████████████▍    | 100/118 [00:06<00:00, 19.26it/s]\u001b[A\n",
      "Iteration:  86%|█████████████████████████▉    | 102/118 [00:06<00:00, 19.31it/s]\u001b[A\n",
      "Iteration:  88%|██████████████████████████▍   | 104/118 [00:06<00:00, 19.35it/s]\u001b[A\n",
      "Iteration:  90%|██████████████████████████▉   | 106/118 [00:06<00:00, 19.38it/s]\u001b[A\n",
      "Iteration:  92%|███████████████████████████▍  | 108/118 [00:06<00:00, 19.39it/s]\u001b[A\n",
      "Iteration:  93%|███████████████████████████▉  | 110/118 [00:06<00:00, 19.40it/s]\u001b[A\n",
      "Iteration:  95%|████████████████████████████▍ | 112/118 [00:06<00:00, 19.42it/s]\u001b[A\n",
      "Iteration:  97%|████████████████████████████▉ | 114/118 [00:07<00:00, 19.36it/s]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████| 118/118 [00:07<00:00, 16.16it/s]\u001b[A\n",
      "Epoch:  12%|████▍                                | 3/25 [00:22<02:45,  7.53s/it]\n",
      "Iteration:   0%|                                        | 0/118 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▌                               | 2/118 [00:00<00:05, 19.40it/s]\u001b[A\n",
      "Iteration:   3%|█                               | 4/118 [00:00<00:05, 19.43it/s]\u001b[A\n",
      "Iteration:   5%|█▋                              | 6/118 [00:00<00:05, 19.45it/s]\u001b[A\n",
      "Iteration:   7%|██▏                             | 8/118 [00:00<00:05, 19.46it/s]\u001b[A\n",
      "Iteration:   8%|██▋                            | 10/118 [00:00<00:05, 19.46it/s]\u001b[A\n",
      "Iteration:  10%|███▏                           | 12/118 [00:00<00:05, 19.46it/s]\u001b[A\n",
      "Iteration:  12%|███▋                           | 14/118 [00:00<00:05, 19.45it/s]\u001b[A\n",
      "Iteration:  14%|████▏                          | 16/118 [00:00<00:05, 19.45it/s]\u001b[A\n",
      "Iteration:  15%|████▋                          | 18/118 [00:00<00:05, 19.44it/s]\u001b[A\n",
      "Iteration:  17%|█████▎                         | 20/118 [00:01<00:05, 19.42it/s]\u001b[A\n",
      "Iteration:  19%|█████▊                         | 22/118 [00:01<00:04, 19.41it/s]\u001b[A\n",
      "Iteration:  20%|██████▎                        | 24/118 [00:01<00:04, 19.42it/s]\u001b[A\n",
      "Iteration:  22%|██████▊                        | 26/118 [00:01<00:04, 19.43it/s]\u001b[A\n",
      "Iteration:  24%|███████▎                       | 28/118 [00:01<00:04, 19.42it/s]\u001b[A\n",
      "Iteration:  25%|███████▉                       | 30/118 [00:01<00:04, 19.43it/s]\u001b[A\n",
      "Iteration:  27%|████████▍                      | 32/118 [00:01<00:04, 19.44it/s]\u001b[A\n",
      "Iteration:  29%|████████▉                      | 34/118 [00:01<00:04, 19.44it/s]\u001b[A\n",
      "Iteration:  31%|█████████▍                     | 36/118 [00:01<00:04, 19.45it/s]\u001b[A\n",
      "Iteration:  32%|█████████▉                     | 38/118 [00:01<00:04, 19.43it/s]\u001b[A\n",
      "Iteration:  34%|██████████▌                    | 40/118 [00:02<00:04, 19.41it/s]\u001b[A\n",
      "Iteration:  36%|███████████                    | 42/118 [00:02<00:03, 19.43it/s]\u001b[A\n",
      "Iteration:  37%|███████████▌                   | 44/118 [00:02<00:03, 19.45it/s]\u001b[A12/20/2023 22:24:15 - INFO - __main__ -   Loading features from cached file ./dataset/bert/classic/m6Am/cached_dev_m6Am_101_dnaprom\n",
      "12/20/2023 22:24:15 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/20/2023 22:24:15 - INFO - __main__ -     Num examples = 937\n",
      "12/20/2023 22:24:15 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                        | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|██████████▎                    | 10/30 [00:00<00:00, 93.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|████████████████████▋          | 20/30 [00:00<00:00, 94.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████| 30/30 [00:00<00:00, 94.42it/s]\u001b[A\u001b[A\n",
      "12/20/2023 22:24:15 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/20/2023 22:24:15 - INFO - __main__ -     acc = 0.848452508004269\n",
      "12/20/2023 22:24:15 - INFO - __main__ -     auc = 0.924985161849975\n",
      "12/20/2023 22:24:15 - INFO - __main__ -     f1 = 0.8480702475564081\n",
      "12/20/2023 22:24:15 - INFO - __main__ -     mcc = 0.7120085826307276\n",
      "12/20/2023 22:24:15 - INFO - __main__ -     precision = 0.8594925512104283\n",
      "12/20/2023 22:24:15 - INFO - __main__ -     recall = 0.8525498790120075\n",
      "/root/.conda/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "{\"eval_acc\": 0.848452508004269, \"eval_f1\": 0.8480702475564081, \"eval_mcc\": 0.7120085826307276, \"eval_auc\": 0.924985161849975, \"eval_precision\": 0.8594925512104283, \"eval_recall\": 0.8525498790120075, \"learning_rate\": 2.5423728813559322e-05, \"loss\": 0.21701212480664253, \"step\": 400}\n",
      "12/20/2023 22:24:15 - INFO - transformers.configuration_utils -   Configuration saved in ./results/your_models/bert/classic/m6Am/checkpoint-400/config.json\n",
      "12/20/2023 22:24:16 - INFO - transformers.modeling_utils -   Model weights saved in ./results/your_models/bert/classic/m6Am/checkpoint-400/pytorch_model.bin\n",
      "12/20/2023 22:24:16 - INFO - __main__ -   Saving model checkpoint to ./results/your_models/bert/classic/m6Am/checkpoint-400\n",
      "12/20/2023 22:24:16 - INFO - __main__ -   Saving optimizer and scheduler states to ./results/your_models/bert/classic/m6Am/checkpoint-400\n",
      "\n",
      "Iteration:  39%|████████████                   | 46/118 [00:03<00:16,  4.27it/s]\u001b[A\n",
      "Iteration:  41%|████████████▌                  | 48/118 [00:03<00:12,  5.56it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▏                 | 50/118 [00:03<00:09,  7.08it/s]\u001b[A\n",
      "Iteration:  44%|█████████████▋                 | 52/118 [00:03<00:07,  8.75it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▏                | 54/118 [00:03<00:06, 10.47it/s]\u001b[A\n",
      "Iteration:  47%|██████████████▋                | 56/118 [00:04<00:05, 12.15it/s]\u001b[A\n",
      "Iteration:  49%|███████████████▏               | 58/118 [00:04<00:04, 13.69it/s]\u001b[A\n",
      "Iteration:  51%|███████████████▊               | 60/118 [00:04<00:03, 15.02it/s]\u001b[A\n",
      "Iteration:  53%|████████████████▎              | 62/118 [00:04<00:03, 16.12it/s]\u001b[A\n",
      "Iteration:  54%|████████████████▊              | 64/118 [00:04<00:03, 17.00it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▎             | 66/118 [00:04<00:02, 17.66it/s]\u001b[A\n",
      "Iteration:  58%|█████████████████▊             | 68/118 [00:04<00:02, 18.16it/s]\u001b[A\n",
      "Iteration:  59%|██████████████████▍            | 70/118 [00:04<00:02, 18.52it/s]\u001b[A\n",
      "Iteration:  61%|██████████████████▉            | 72/118 [00:04<00:02, 18.78it/s]\u001b[A\n",
      "Iteration:  63%|███████████████████▍           | 74/118 [00:05<00:02, 18.94it/s]\u001b[A\n",
      "Iteration:  64%|███████████████████▉           | 76/118 [00:05<00:02, 19.06it/s]\u001b[A\n",
      "Iteration:  66%|████████████████████▍          | 78/118 [00:05<00:02, 19.16it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████          | 80/118 [00:05<00:01, 19.25it/s]\u001b[A\n",
      "Iteration:  69%|█████████████████████▌         | 82/118 [00:05<00:01, 19.30it/s]\u001b[A\n",
      "Iteration:  71%|██████████████████████         | 84/118 [00:05<00:01, 19.33it/s]\u001b[A\n",
      "Iteration:  73%|██████████████████████▌        | 86/118 [00:05<00:01, 19.37it/s]\u001b[A\n",
      "Iteration:  75%|███████████████████████        | 88/118 [00:05<00:01, 19.40it/s]\u001b[A\n",
      "Iteration:  76%|███████████████████████▋       | 90/118 [00:05<00:01, 19.41it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▏      | 92/118 [00:05<00:01, 19.37it/s]\u001b[A\n",
      "Iteration:  80%|████████████████████████▋      | 94/118 [00:06<00:01, 19.36it/s]\u001b[A\n",
      "Iteration:  81%|█████████████████████████▏     | 96/118 [00:06<00:01, 19.37it/s]\u001b[A\n",
      "Iteration:  83%|█████████████████████████▋     | 98/118 [00:06<00:01, 19.39it/s]\u001b[A\n",
      "Iteration:  85%|█████████████████████████▍    | 100/118 [00:06<00:00, 19.15it/s]\u001b[A\n",
      "Iteration:  86%|█████████████████████████▉    | 102/118 [00:06<00:00, 19.19it/s]\u001b[A\n",
      "Iteration:  88%|██████████████████████████▍   | 104/118 [00:06<00:00, 19.27it/s]\u001b[A\n",
      "Iteration:  90%|██████████████████████████▉   | 106/118 [00:06<00:00, 19.33it/s]\u001b[A\n",
      "Iteration:  92%|███████████████████████████▍  | 108/118 [00:06<00:00, 19.36it/s]\u001b[A\n",
      "Iteration:  93%|███████████████████████████▉  | 110/118 [00:06<00:00, 19.39it/s]\u001b[A\n",
      "Iteration:  95%|████████████████████████████▍ | 112/118 [00:06<00:00, 19.40it/s]\u001b[A\n",
      "Iteration:  97%|████████████████████████████▉ | 114/118 [00:07<00:00, 19.23it/s]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████| 118/118 [00:07<00:00, 16.15it/s]\u001b[A\n",
      "Epoch:  16%|█████▉                               | 4/25 [00:30<02:36,  7.44s/it]\n",
      "Iteration:   0%|                                        | 0/118 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▌                               | 2/118 [00:00<00:05, 19.41it/s]\u001b[A\n",
      "Iteration:   3%|█                               | 4/118 [00:00<00:05, 19.42it/s]\u001b[A\n",
      "Iteration:   5%|█▋                              | 6/118 [00:00<00:05, 19.45it/s]\u001b[A\n",
      "Iteration:   7%|██▏                             | 8/118 [00:00<00:05, 19.45it/s]\u001b[A\n",
      "Iteration:   8%|██▋                            | 10/118 [00:00<00:05, 18.36it/s]\u001b[A\n",
      "Iteration:  10%|███▏                           | 12/118 [00:00<00:05, 18.68it/s]\u001b[A\n",
      "Iteration:  12%|███▋                           | 14/118 [00:00<00:05, 18.92it/s]\u001b[A\n",
      "Iteration:  14%|████▏                          | 16/118 [00:00<00:05, 19.04it/s]\u001b[A\n",
      "Iteration:  15%|████▋                          | 18/118 [00:00<00:05, 19.16it/s]\u001b[A\n",
      "Iteration:  17%|█████▎                         | 20/118 [00:01<00:05, 19.25it/s]\u001b[A\n",
      "Iteration:  19%|█████▊                         | 22/118 [00:01<00:04, 19.24it/s]\u001b[A\n",
      "Iteration:  20%|██████▎                        | 24/118 [00:01<00:04, 19.30it/s]\u001b[A\n",
      "Iteration:  22%|██████▊                        | 26/118 [00:01<00:04, 19.35it/s]\u001b[A12/20/2023 22:24:21 - INFO - __main__ -   Loading features from cached file ./dataset/bert/classic/m6Am/cached_dev_m6Am_101_dnaprom\n",
      "12/20/2023 22:24:21 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/20/2023 22:24:21 - INFO - __main__ -     Num examples = 937\n",
      "12/20/2023 22:24:21 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                        | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|██████████▎                    | 10/30 [00:00<00:00, 92.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|████████████████████▋          | 20/30 [00:00<00:00, 93.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████| 30/30 [00:00<00:00, 93.84it/s]\u001b[A\u001b[A\n",
      "12/20/2023 22:24:22 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/20/2023 22:24:22 - INFO - __main__ -     acc = 0.8537886872998933\n",
      "12/20/2023 22:24:22 - INFO - __main__ -     auc = 0.9230036981235448\n",
      "12/20/2023 22:24:22 - INFO - __main__ -     f1 = 0.8536927009903201\n",
      "12/20/2023 22:24:22 - INFO - __main__ -     mcc = 0.7161706703259638\n",
      "12/20/2023 22:24:22 - INFO - __main__ -     precision = 0.8594070517242965\n",
      "12/20/2023 22:24:22 - INFO - __main__ -     recall = 0.856768479203762\n",
      "/root/.conda/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "{\"eval_acc\": 0.8537886872998933, \"eval_f1\": 0.8536927009903201, \"eval_mcc\": 0.7161706703259638, \"eval_auc\": 0.9230036981235448, \"eval_precision\": 0.8594070517242965, \"eval_recall\": 0.856768479203762, \"learning_rate\": 2.4293785310734464e-05, \"loss\": 0.17504883379675448, \"step\": 500}\n",
      "12/20/2023 22:24:22 - INFO - transformers.configuration_utils -   Configuration saved in ./results/your_models/bert/classic/m6Am/checkpoint-500/config.json\n",
      "12/20/2023 22:24:22 - INFO - transformers.modeling_utils -   Model weights saved in ./results/your_models/bert/classic/m6Am/checkpoint-500/pytorch_model.bin\n",
      "12/20/2023 22:24:22 - INFO - __main__ -   Saving model checkpoint to ./results/your_models/bert/classic/m6Am/checkpoint-500\n",
      "12/20/2023 22:24:23 - INFO - __main__ -   Saving optimizer and scheduler states to ./results/your_models/bert/classic/m6Am/checkpoint-500\n",
      "\n",
      "Iteration:  24%|███████▎                       | 28/118 [00:02<00:22,  4.08it/s]\u001b[A\n",
      "Iteration:  25%|███████▉                       | 30/118 [00:02<00:16,  5.34it/s]\u001b[A\n",
      "Iteration:  27%|████████▍                      | 32/118 [00:02<00:12,  6.83it/s]\u001b[A\n",
      "Iteration:  29%|████████▉                      | 34/118 [00:03<00:09,  8.48it/s]\u001b[A\n",
      "Iteration:  31%|█████████▍                     | 36/118 [00:03<00:08, 10.21it/s]\u001b[A\n",
      "Iteration:  32%|█████████▉                     | 38/118 [00:03<00:06, 11.90it/s]\u001b[A\n",
      "Iteration:  34%|██████████▌                    | 40/118 [00:03<00:05, 13.47it/s]\u001b[A\n",
      "Iteration:  36%|███████████                    | 42/118 [00:03<00:05, 14.83it/s]\u001b[A\n",
      "Iteration:  37%|███████████▌                   | 44/118 [00:03<00:04, 15.96it/s]\u001b[A\n",
      "Iteration:  39%|████████████                   | 46/118 [00:03<00:04, 16.87it/s]\u001b[A\n",
      "Iteration:  41%|████████████▌                  | 48/118 [00:03<00:03, 17.53it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▏                 | 50/118 [00:03<00:03, 18.05it/s]\u001b[A\n",
      "Iteration:  44%|█████████████▋                 | 52/118 [00:03<00:03, 18.45it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▏                | 54/118 [00:04<00:03, 18.73it/s]\u001b[A\n",
      "Iteration:  47%|██████████████▋                | 56/118 [00:04<00:03, 18.93it/s]\u001b[A\n",
      "Iteration:  49%|███████████████▏               | 58/118 [00:04<00:03, 19.08it/s]\u001b[A\n",
      "Iteration:  51%|███████████████▊               | 60/118 [00:04<00:03, 19.19it/s]\u001b[A\n",
      "Iteration:  53%|████████████████▎              | 62/118 [00:04<00:02, 19.25it/s]\u001b[A\n",
      "Iteration:  54%|████████████████▊              | 64/118 [00:04<00:02, 19.29it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▎             | 66/118 [00:04<00:02, 19.32it/s]\u001b[A\n",
      "Iteration:  58%|█████████████████▊             | 68/118 [00:04<00:02, 19.36it/s]\u001b[A\n",
      "Iteration:  59%|██████████████████▍            | 70/118 [00:04<00:02, 19.35it/s]\u001b[A\n",
      "Iteration:  61%|██████████████████▉            | 72/118 [00:05<00:02, 19.37it/s]\u001b[A\n",
      "Iteration:  63%|███████████████████▍           | 74/118 [00:05<00:02, 19.38it/s]\u001b[A\n",
      "Iteration:  64%|███████████████████▉           | 76/118 [00:05<00:02, 19.39it/s]\u001b[A\n",
      "Iteration:  66%|████████████████████▍          | 78/118 [00:05<00:02, 19.40it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████          | 80/118 [00:05<00:01, 19.42it/s]\u001b[A\n",
      "Iteration:  69%|█████████████████████▌         | 82/118 [00:05<00:01, 19.42it/s]\u001b[A\n",
      "Iteration:  71%|██████████████████████         | 84/118 [00:05<00:01, 19.43it/s]\u001b[A\n",
      "Iteration:  73%|██████████████████████▌        | 86/118 [00:05<00:01, 19.39it/s]\u001b[A\n",
      "Iteration:  75%|███████████████████████        | 88/118 [00:05<00:01, 19.39it/s]\u001b[A\n",
      "Iteration:  76%|███████████████████████▋       | 90/118 [00:05<00:01, 19.40it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▏      | 92/118 [00:06<00:01, 19.40it/s]\u001b[A\n",
      "Iteration:  80%|████████████████████████▋      | 94/118 [00:06<00:01, 19.42it/s]\u001b[A\n",
      "Iteration:  81%|█████████████████████████▏     | 96/118 [00:06<00:01, 19.42it/s]\u001b[A\n",
      "Iteration:  83%|█████████████████████████▋     | 98/118 [00:06<00:01, 19.42it/s]\u001b[A\n",
      "Iteration:  85%|█████████████████████████▍    | 100/118 [00:06<00:00, 19.43it/s]\u001b[A\n",
      "Iteration:  86%|█████████████████████████▉    | 102/118 [00:06<00:00, 19.44it/s]\u001b[A\n",
      "Iteration:  88%|██████████████████████████▍   | 104/118 [00:06<00:00, 19.45it/s]\u001b[A\n",
      "Iteration:  90%|██████████████████████████▉   | 106/118 [00:06<00:00, 19.46it/s]\u001b[A\n",
      "Iteration:  92%|███████████████████████████▍  | 108/118 [00:06<00:00, 19.45it/s]\u001b[A\n",
      "Iteration:  93%|███████████████████████████▉  | 110/118 [00:06<00:00, 19.43it/s]\u001b[A\n",
      "Iteration:  95%|████████████████████████████▍ | 112/118 [00:07<00:00, 19.41it/s]\u001b[A\n",
      "Iteration:  97%|████████████████████████████▉ | 114/118 [00:07<00:00, 19.42it/s]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████| 118/118 [00:07<00:00, 16.00it/s]\u001b[A\n",
      "Epoch:  20%|███████▍                             | 5/25 [00:37<02:28,  7.42s/it]\n",
      "Iteration:   0%|                                        | 0/118 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▌                               | 2/118 [00:00<00:05, 19.43it/s]\u001b[A\n",
      "Iteration:   3%|█                               | 4/118 [00:00<00:05, 19.44it/s]\u001b[A\n",
      "Iteration:   5%|█▋                              | 6/118 [00:00<00:05, 19.45it/s]\u001b[A\n",
      "Iteration:   7%|██▏                             | 8/118 [00:00<00:05, 19.39it/s]\u001b[A12/20/2023 22:24:28 - INFO - __main__ -   Loading features from cached file ./dataset/bert/classic/m6Am/cached_dev_m6Am_101_dnaprom\n",
      "12/20/2023 22:24:28 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/20/2023 22:24:28 - INFO - __main__ -     Num examples = 937\n",
      "12/20/2023 22:24:28 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                        | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|██████████▎                    | 10/30 [00:00<00:00, 93.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|████████████████████▋          | 20/30 [00:00<00:00, 94.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████| 30/30 [00:00<00:00, 94.43it/s]\u001b[A\u001b[A\n",
      "12/20/2023 22:24:28 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/20/2023 22:24:28 - INFO - __main__ -     acc = 0.8388473852721452\n",
      "12/20/2023 22:24:28 - INFO - __main__ -     auc = 0.9138770944619459\n",
      "12/20/2023 22:24:28 - INFO - __main__ -     f1 = 0.8381387101387101\n",
      "12/20/2023 22:24:28 - INFO - __main__ -     mcc = 0.6977691204754503\n",
      "12/20/2023 22:24:28 - INFO - __main__ -     precision = 0.8541878411443629\n",
      "12/20/2023 22:24:28 - INFO - __main__ -     recall = 0.8436606857508104\n",
      "/root/.conda/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "{\"eval_acc\": 0.8388473852721452, \"eval_f1\": 0.8381387101387101, \"eval_mcc\": 0.6977691204754503, \"eval_auc\": 0.9138770944619459, \"eval_precision\": 0.8541878411443629, \"eval_recall\": 0.8436606857508104, \"learning_rate\": 2.3163841807909602e-05, \"loss\": 0.11972432895563542, \"step\": 600}\n",
      "12/20/2023 22:24:28 - INFO - transformers.configuration_utils -   Configuration saved in ./results/your_models/bert/classic/m6Am/checkpoint-600/config.json\n",
      "12/20/2023 22:24:28 - INFO - transformers.modeling_utils -   Model weights saved in ./results/your_models/bert/classic/m6Am/checkpoint-600/pytorch_model.bin\n",
      "12/20/2023 22:24:28 - INFO - __main__ -   Saving model checkpoint to ./results/your_models/bert/classic/m6Am/checkpoint-600\n",
      "12/20/2023 22:24:29 - INFO - __main__ -   Saving optimizer and scheduler states to ./results/your_models/bert/classic/m6Am/checkpoint-600\n",
      "\n",
      "Iteration:   8%|██▋                            | 10/118 [00:01<00:29,  3.69it/s]\u001b[A\n",
      "Iteration:  10%|███▏                           | 12/118 [00:01<00:20,  5.08it/s]\u001b[A\n",
      "Iteration:  12%|███▋                           | 14/118 [00:01<00:15,  6.70it/s]\u001b[A\n",
      "Iteration:  14%|████▏                          | 16/118 [00:02<00:12,  8.43it/s]\u001b[A\n",
      "Iteration:  15%|████▋                          | 18/118 [00:02<00:09, 10.24it/s]\u001b[A\n",
      "Iteration:  17%|█████▎                         | 20/118 [00:02<00:08, 11.99it/s]\u001b[A\n",
      "Iteration:  19%|█████▊                         | 22/118 [00:02<00:07, 13.57it/s]\u001b[A\n",
      "Iteration:  20%|██████▎                        | 24/118 [00:02<00:06, 14.93it/s]\u001b[A\n",
      "Iteration:  22%|██████▊                        | 26/118 [00:02<00:05, 16.05it/s]\u001b[A\n",
      "Iteration:  24%|███████▎                       | 28/118 [00:02<00:05, 16.95it/s]\u001b[A\n",
      "Iteration:  25%|███████▉                       | 30/118 [00:02<00:04, 17.62it/s]\u001b[A\n",
      "Iteration:  27%|████████▍                      | 32/118 [00:02<00:04, 18.14it/s]\u001b[A\n",
      "Iteration:  29%|████████▉                      | 34/118 [00:02<00:04, 18.51it/s]\u001b[A\n",
      "Iteration:  31%|█████████▍                     | 36/118 [00:03<00:04, 18.77it/s]\u001b[A\n",
      "Iteration:  32%|█████████▉                     | 38/118 [00:03<00:04, 18.97it/s]\u001b[A\n",
      "Iteration:  34%|██████████▌                    | 40/118 [00:03<00:04, 19.12it/s]\u001b[A\n",
      "Iteration:  36%|███████████                    | 42/118 [00:03<00:03, 19.22it/s]\u001b[A\n",
      "Iteration:  37%|███████████▌                   | 44/118 [00:03<00:03, 19.28it/s]\u001b[A\n",
      "Iteration:  39%|████████████                   | 46/118 [00:03<00:03, 19.34it/s]\u001b[A\n",
      "Iteration:  41%|████████████▌                  | 48/118 [00:03<00:03, 19.26it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▏                 | 50/118 [00:03<00:03, 19.31it/s]\u001b[A\n",
      "Iteration:  44%|█████████████▋                 | 52/118 [00:03<00:03, 19.35it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▏                | 54/118 [00:04<00:03, 19.39it/s]\u001b[A\n",
      "Iteration:  47%|██████████████▋                | 56/118 [00:04<00:03, 19.41it/s]\u001b[A\n",
      "Iteration:  49%|███████████████▏               | 58/118 [00:04<00:03, 19.43it/s]\u001b[A\n",
      "Iteration:  51%|███████████████▊               | 60/118 [00:04<00:02, 19.43it/s]\u001b[A\n",
      "Iteration:  53%|████████████████▎              | 62/118 [00:04<00:02, 19.38it/s]\u001b[A\n",
      "Iteration:  54%|████████████████▊              | 64/118 [00:04<00:02, 19.40it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▎             | 66/118 [00:04<00:02, 19.42it/s]\u001b[A\n",
      "Iteration:  58%|█████████████████▊             | 68/118 [00:04<00:02, 19.43it/s]\u001b[A\n",
      "Iteration:  59%|██████████████████▍            | 70/118 [00:04<00:02, 19.44it/s]\u001b[A\n",
      "Iteration:  61%|██████████████████▉            | 72/118 [00:04<00:02, 19.44it/s]\u001b[A\n",
      "Iteration:  63%|███████████████████▍           | 74/118 [00:05<00:02, 19.45it/s]\u001b[A\n",
      "Iteration:  64%|███████████████████▉           | 76/118 [00:05<00:02, 19.45it/s]\u001b[A\n",
      "Iteration:  66%|████████████████████▍          | 78/118 [00:05<00:02, 19.46it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████          | 80/118 [00:05<00:01, 19.44it/s]\u001b[A\n",
      "Iteration:  69%|█████████████████████▌         | 82/118 [00:05<00:01, 19.45it/s]\u001b[A\n",
      "Iteration:  71%|██████████████████████         | 84/118 [00:05<00:01, 19.45it/s]\u001b[A\n",
      "Iteration:  73%|██████████████████████▌        | 86/118 [00:05<00:01, 19.43it/s]\u001b[A\n",
      "Iteration:  75%|███████████████████████        | 88/118 [00:05<00:01, 19.41it/s]\u001b[A\n",
      "Iteration:  76%|███████████████████████▋       | 90/118 [00:05<00:01, 19.42it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▏      | 92/118 [00:05<00:01, 19.41it/s]\u001b[A\n",
      "Iteration:  80%|████████████████████████▋      | 94/118 [00:06<00:01, 19.42it/s]\u001b[A\n",
      "Iteration:  81%|█████████████████████████▏     | 96/118 [00:06<00:01, 19.43it/s]\u001b[A\n",
      "Iteration:  83%|█████████████████████████▋     | 98/118 [00:06<00:01, 19.43it/s]\u001b[A\n",
      "Iteration:  85%|█████████████████████████▍    | 100/118 [00:06<00:00, 19.38it/s]\u001b[A\n",
      "Iteration:  86%|█████████████████████████▉    | 102/118 [00:06<00:00, 19.39it/s]\u001b[A\n",
      "Iteration:  88%|██████████████████████████▍   | 104/118 [00:06<00:00, 19.41it/s]\u001b[A\n",
      "Iteration:  90%|██████████████████████████▉   | 106/118 [00:06<00:00, 19.41it/s]\u001b[A\n",
      "Iteration:  92%|███████████████████████████▍  | 108/118 [00:06<00:00, 19.41it/s]\u001b[A12/20/2023 22:24:34 - INFO - __main__ -   Loading features from cached file ./dataset/bert/classic/m6Am/cached_dev_m6Am_101_dnaprom\n",
      "12/20/2023 22:24:34 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/20/2023 22:24:34 - INFO - __main__ -     Num examples = 937\n",
      "12/20/2023 22:24:34 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                        | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|██████████▎                    | 10/30 [00:00<00:00, 93.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|████████████████████▋          | 20/30 [00:00<00:00, 93.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████| 30/30 [00:00<00:00, 94.05it/s]\u001b[A\u001b[A\n",
      "12/20/2023 22:24:34 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/20/2023 22:24:34 - INFO - __main__ -     acc = 0.8345784418356457\n",
      "12/20/2023 22:24:34 - INFO - __main__ -     auc = 0.912648952198329\n",
      "12/20/2023 22:24:34 - INFO - __main__ -     f1 = 0.8334829561805701\n",
      "12/20/2023 22:24:34 - INFO - __main__ -     mcc = 0.6946319596422876\n",
      "12/20/2023 22:24:34 - INFO - __main__ -     precision = 0.8547165049098511\n",
      "12/20/2023 22:24:34 - INFO - __main__ -     recall = 0.8400698534447335\n",
      "/root/.conda/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "{\"eval_acc\": 0.8345784418356457, \"eval_f1\": 0.8334829561805701, \"eval_mcc\": 0.6946319596422876, \"eval_auc\": 0.912648952198329, \"eval_precision\": 0.8547165049098511, \"eval_recall\": 0.8400698534447335, \"learning_rate\": 2.2033898305084748e-05, \"loss\": 0.0867164287623018, \"step\": 700}\n",
      "12/20/2023 22:24:34 - INFO - transformers.configuration_utils -   Configuration saved in ./results/your_models/bert/classic/m6Am/checkpoint-700/config.json\n",
      "12/20/2023 22:24:35 - INFO - transformers.modeling_utils -   Model weights saved in ./results/your_models/bert/classic/m6Am/checkpoint-700/pytorch_model.bin\n",
      "12/20/2023 22:24:35 - INFO - __main__ -   Saving model checkpoint to ./results/your_models/bert/classic/m6Am/checkpoint-700\n",
      "12/20/2023 22:24:35 - INFO - __main__ -   Saving optimizer and scheduler states to ./results/your_models/bert/classic/m6Am/checkpoint-700\n",
      "\n",
      "Iteration:  95%|████████████████████████████▍ | 112/118 [00:08<00:01,  5.56it/s]\u001b[A\n",
      "Iteration:  97%|████████████████████████████▉ | 114/118 [00:08<00:00,  7.07it/s]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████| 118/118 [00:08<00:00, 13.86it/s]\u001b[A\n",
      "Epoch:  24%|████████▉                            | 6/25 [00:46<02:28,  7.79s/it]\n",
      "Iteration:   0%|                                        | 0/118 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▌                               | 2/118 [00:00<00:05, 19.40it/s]\u001b[A\n",
      "Iteration:   3%|█                               | 4/118 [00:00<00:05, 19.44it/s]\u001b[A\n",
      "Iteration:   5%|█▋                              | 6/118 [00:00<00:05, 19.42it/s]\u001b[A\n",
      "Iteration:   7%|██▏                             | 8/118 [00:00<00:05, 19.42it/s]\u001b[A\n",
      "Iteration:   8%|██▋                            | 10/118 [00:00<00:05, 19.43it/s]\u001b[A\n",
      "Iteration:  10%|███▏                           | 12/118 [00:00<00:05, 19.44it/s]\u001b[A\n",
      "Iteration:  12%|███▋                           | 14/118 [00:00<00:05, 19.44it/s]\u001b[A\n",
      "Iteration:  14%|████▏                          | 16/118 [00:00<00:05, 19.41it/s]\u001b[A\n",
      "Iteration:  15%|████▋                          | 18/118 [00:00<00:05, 19.42it/s]\u001b[A\n",
      "Iteration:  17%|█████▎                         | 20/118 [00:01<00:05, 19.44it/s]\u001b[A\n",
      "Iteration:  19%|█████▊                         | 22/118 [00:01<00:04, 19.43it/s]\u001b[A\n",
      "Iteration:  20%|██████▎                        | 24/118 [00:01<00:04, 19.42it/s]\u001b[A\n",
      "Iteration:  22%|██████▊                        | 26/118 [00:01<00:04, 19.40it/s]\u001b[A\n",
      "Iteration:  24%|███████▎                       | 28/118 [00:01<00:04, 19.42it/s]\u001b[A\n",
      "Iteration:  25%|███████▉                       | 30/118 [00:01<00:04, 19.43it/s]\u001b[A\n",
      "Iteration:  27%|████████▍                      | 32/118 [00:01<00:04, 19.43it/s]\u001b[A\n",
      "Iteration:  29%|████████▉                      | 34/118 [00:01<00:04, 19.43it/s]\u001b[A\n",
      "Iteration:  31%|█████████▍                     | 36/118 [00:01<00:04, 19.39it/s]\u001b[A\n",
      "Iteration:  32%|█████████▉                     | 38/118 [00:01<00:04, 19.38it/s]\u001b[A\n",
      "Iteration:  34%|██████████▌                    | 40/118 [00:02<00:04, 19.41it/s]\u001b[A\n",
      "Iteration:  36%|███████████                    | 42/118 [00:02<00:03, 19.41it/s]\u001b[A\n",
      "Iteration:  37%|███████████▌                   | 44/118 [00:02<00:03, 19.42it/s]\u001b[A\n",
      "Iteration:  39%|████████████                   | 46/118 [00:02<00:03, 19.42it/s]\u001b[A\n",
      "Iteration:  41%|████████████▌                  | 48/118 [00:02<00:03, 19.43it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▏                 | 50/118 [00:02<00:03, 19.43it/s]\u001b[A\n",
      "Iteration:  44%|█████████████▋                 | 52/118 [00:02<00:03, 19.43it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▏                | 54/118 [00:02<00:03, 19.43it/s]\u001b[A\n",
      "Iteration:  47%|██████████████▋                | 56/118 [00:02<00:03, 19.43it/s]\u001b[A\n",
      "Iteration:  49%|███████████████▏               | 58/118 [00:02<00:03, 19.45it/s]\u001b[A\n",
      "Iteration:  51%|███████████████▊               | 60/118 [00:03<00:02, 19.45it/s]\u001b[A\n",
      "Iteration:  53%|████████████████▎              | 62/118 [00:03<00:02, 19.40it/s]\u001b[A\n",
      "Iteration:  54%|████████████████▊              | 64/118 [00:03<00:02, 19.39it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▎             | 66/118 [00:03<00:02, 19.41it/s]\u001b[A\n",
      "Iteration:  58%|█████████████████▊             | 68/118 [00:03<00:02, 19.42it/s]\u001b[A\n",
      "Iteration:  59%|██████████████████▍            | 70/118 [00:03<00:02, 19.43it/s]\u001b[A\n",
      "Iteration:  61%|██████████████████▉            | 72/118 [00:03<00:02, 19.43it/s]\u001b[A\n",
      "Iteration:  63%|███████████████████▍           | 74/118 [00:03<00:02, 19.42it/s]\u001b[A\n",
      "Iteration:  64%|███████████████████▉           | 76/118 [00:03<00:02, 19.39it/s]\u001b[A\n",
      "Iteration:  66%|████████████████████▍          | 78/118 [00:04<00:02, 19.41it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████          | 80/118 [00:04<00:01, 19.42it/s]\u001b[A\n",
      "Iteration:  69%|█████████████████████▌         | 82/118 [00:04<00:01, 19.42it/s]\u001b[A\n",
      "Iteration:  71%|██████████████████████         | 84/118 [00:04<00:01, 19.39it/s]\u001b[A\n",
      "Iteration:  73%|██████████████████████▌        | 86/118 [00:04<00:01, 19.39it/s]\u001b[A\n",
      "Iteration:  75%|███████████████████████        | 88/118 [00:04<00:01, 19.40it/s]\u001b[A\n",
      "Iteration:  76%|███████████████████████▋       | 90/118 [00:04<00:01, 19.42it/s]\u001b[A12/20/2023 22:24:40 - INFO - __main__ -   Loading features from cached file ./dataset/bert/classic/m6Am/cached_dev_m6Am_101_dnaprom\n",
      "12/20/2023 22:24:41 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/20/2023 22:24:41 - INFO - __main__ -     Num examples = 937\n",
      "12/20/2023 22:24:41 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                        | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|██████████▎                    | 10/30 [00:00<00:00, 93.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|████████████████████▋          | 20/30 [00:00<00:00, 94.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████| 30/30 [00:00<00:00, 94.67it/s]\u001b[A\u001b[A\n",
      "12/20/2023 22:24:41 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/20/2023 22:24:41 - INFO - __main__ -     acc = 0.8452508004268944\n",
      "12/20/2023 22:24:41 - INFO - __main__ -     auc = 0.9119001963201387\n",
      "12/20/2023 22:24:41 - INFO - __main__ -     f1 = 0.8447351189013694\n",
      "12/20/2023 22:24:41 - INFO - __main__ -     mcc = 0.7078855448680043\n",
      "12/20/2023 22:24:41 - INFO - __main__ -     precision = 0.8582524135608443\n",
      "12/20/2023 22:24:41 - INFO - __main__ -     recall = 0.8496849746610053\n",
      "/root/.conda/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "{\"eval_acc\": 0.8452508004268944, \"eval_f1\": 0.8447351189013694, \"eval_mcc\": 0.7078855448680043, \"eval_auc\": 0.9119001963201387, \"eval_precision\": 0.8582524135608443, \"eval_recall\": 0.8496849746610053, \"learning_rate\": 2.090395480225989e-05, \"loss\": 0.061669548253994434, \"step\": 800}\n",
      "12/20/2023 22:24:41 - INFO - transformers.configuration_utils -   Configuration saved in ./results/your_models/bert/classic/m6Am/checkpoint-800/config.json\n",
      "12/20/2023 22:24:41 - INFO - transformers.modeling_utils -   Model weights saved in ./results/your_models/bert/classic/m6Am/checkpoint-800/pytorch_model.bin\n",
      "12/20/2023 22:24:41 - INFO - __main__ -   Saving model checkpoint to ./results/your_models/bert/classic/m6Am/checkpoint-800\n",
      "12/20/2023 22:24:42 - INFO - __main__ -   Saving optimizer and scheduler states to ./results/your_models/bert/classic/m6Am/checkpoint-800\n",
      "\n",
      "Iteration:  78%|████████████████████████▏      | 92/118 [00:05<00:06,  4.27it/s]\u001b[A\n",
      "Iteration:  80%|████████████████████████▋      | 94/118 [00:06<00:04,  5.56it/s]\u001b[A\n",
      "Iteration:  81%|█████████████████████████▏     | 96/118 [00:06<00:03,  7.07it/s]\u001b[A\n",
      "Iteration:  83%|█████████████████████████▋     | 98/118 [00:06<00:02,  8.74it/s]\u001b[A\n",
      "Iteration:  85%|█████████████████████████▍    | 100/118 [00:06<00:01, 10.47it/s]\u001b[A\n",
      "Iteration:  86%|█████████████████████████▉    | 102/118 [00:06<00:01, 12.15it/s]\u001b[A\n",
      "Iteration:  88%|██████████████████████████▍   | 104/118 [00:06<00:01, 13.69it/s]\u001b[A\n",
      "Iteration:  90%|██████████████████████████▉   | 106/118 [00:06<00:00, 15.02it/s]\u001b[A\n",
      "Iteration:  92%|███████████████████████████▍  | 108/118 [00:06<00:00, 16.12it/s]\u001b[A\n",
      "Iteration:  93%|███████████████████████████▉  | 110/118 [00:06<00:00, 17.00it/s]\u001b[A\n",
      "Iteration:  95%|████████████████████████████▍ | 112/118 [00:06<00:00, 17.66it/s]\u001b[A\n",
      "Iteration:  97%|████████████████████████████▉ | 114/118 [00:07<00:00, 18.16it/s]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████| 118/118 [00:07<00:00, 16.19it/s]\u001b[A\n",
      "Epoch:  28%|██████████▎                          | 7/25 [00:53<02:17,  7.63s/it]\n",
      "Iteration:   0%|                                        | 0/118 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▌                               | 2/118 [00:00<00:06, 19.21it/s]\u001b[A\n",
      "Iteration:   3%|█                               | 4/118 [00:00<00:05, 19.34it/s]\u001b[A\n",
      "Iteration:   5%|█▋                              | 6/118 [00:00<00:05, 19.39it/s]\u001b[A\n",
      "Iteration:   7%|██▏                             | 8/118 [00:00<00:05, 19.34it/s]\u001b[A\n",
      "Iteration:   8%|██▋                            | 10/118 [00:00<00:05, 19.14it/s]\u001b[A\n",
      "Iteration:  10%|███▏                           | 12/118 [00:00<00:05, 19.20it/s]\u001b[A\n",
      "Iteration:  12%|███▋                           | 14/118 [00:00<00:05, 19.28it/s]\u001b[A\n",
      "Iteration:  14%|████▏                          | 16/118 [00:00<00:05, 19.34it/s]\u001b[A\n",
      "Iteration:  15%|████▋                          | 18/118 [00:00<00:05, 19.37it/s]\u001b[A\n",
      "Iteration:  17%|█████▎                         | 20/118 [00:01<00:05, 19.39it/s]\u001b[A\n",
      "Iteration:  19%|█████▊                         | 22/118 [00:01<00:04, 19.41it/s]\u001b[A\n",
      "Iteration:  20%|██████▎                        | 24/118 [00:01<00:04, 19.43it/s]\u001b[A\n",
      "Iteration:  22%|██████▊                        | 26/118 [00:01<00:04, 19.43it/s]\u001b[A\n",
      "Iteration:  24%|███████▎                       | 28/118 [00:01<00:04, 19.44it/s]\u001b[A\n",
      "Iteration:  25%|███████▉                       | 30/118 [00:01<00:04, 19.42it/s]\u001b[A\n",
      "Iteration:  27%|████████▍                      | 32/118 [00:01<00:04, 19.43it/s]\u001b[A\n",
      "Iteration:  29%|████████▉                      | 34/118 [00:01<00:04, 19.44it/s]\u001b[A\n",
      "Iteration:  31%|█████████▍                     | 36/118 [00:01<00:04, 19.44it/s]\u001b[A\n",
      "Iteration:  32%|█████████▉                     | 38/118 [00:01<00:04, 19.43it/s]\u001b[A\n",
      "Iteration:  34%|██████████▌                    | 40/118 [00:02<00:04, 19.44it/s]\u001b[A\n",
      "Iteration:  36%|███████████                    | 42/118 [00:02<00:03, 19.41it/s]\u001b[A\n",
      "Iteration:  37%|███████████▌                   | 44/118 [00:02<00:03, 19.41it/s]\u001b[A\n",
      "Iteration:  39%|████████████                   | 46/118 [00:02<00:03, 19.42it/s]\u001b[A\n",
      "Iteration:  41%|████████████▌                  | 48/118 [00:02<00:03, 19.43it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▏                 | 50/118 [00:02<00:03, 19.38it/s]\u001b[A\n",
      "Iteration:  44%|█████████████▋                 | 52/118 [00:02<00:03, 19.41it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▏                | 54/118 [00:02<00:03, 19.42it/s]\u001b[A\n",
      "Iteration:  47%|██████████████▋                | 56/118 [00:02<00:03, 19.42it/s]\u001b[A\n",
      "Iteration:  49%|███████████████▏               | 58/118 [00:02<00:03, 19.44it/s]\u001b[A\n",
      "Iteration:  51%|███████████████▊               | 60/118 [00:03<00:02, 19.44it/s]\u001b[A\n",
      "Iteration:  53%|████████████████▎              | 62/118 [00:03<00:02, 19.46it/s]\u001b[A\n",
      "Iteration:  54%|████████████████▊              | 64/118 [00:03<00:02, 19.46it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▎             | 66/118 [00:03<00:02, 19.46it/s]\u001b[A\n",
      "Iteration:  58%|█████████████████▊             | 68/118 [00:03<00:02, 19.45it/s]\u001b[A\n",
      "Iteration:  59%|██████████████████▍            | 70/118 [00:03<00:02, 19.46it/s]\u001b[A\n",
      "Iteration:  61%|██████████████████▉            | 72/118 [00:03<00:02, 19.45it/s]\u001b[A12/20/2023 22:24:47 - INFO - __main__ -   Loading features from cached file ./dataset/bert/classic/m6Am/cached_dev_m6Am_101_dnaprom\n",
      "12/20/2023 22:24:47 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/20/2023 22:24:47 - INFO - __main__ -     Num examples = 937\n",
      "12/20/2023 22:24:47 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                        | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|██████████▎                    | 10/30 [00:00<00:00, 93.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|████████████████████▋          | 20/30 [00:00<00:00, 94.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████| 30/30 [00:00<00:00, 95.02it/s]\u001b[A\u001b[A\n",
      "12/20/2023 22:24:47 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/20/2023 22:24:47 - INFO - __main__ -     acc = 0.8153681963713981\n",
      "12/20/2023 22:24:47 - INFO - __main__ -     auc = 0.9002762178696984\n",
      "12/20/2023 22:24:47 - INFO - __main__ -     f1 = 0.8128822989593734\n",
      "12/20/2023 22:24:47 - INFO - __main__ -     mcc = 0.6702123339528314\n",
      "12/20/2023 22:24:47 - INFO - __main__ -     precision = 0.848324306192717\n",
      "12/20/2023 22:24:47 - INFO - __main__ -     recall = 0.8223896269917363\n",
      "/root/.conda/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "{\"eval_acc\": 0.8153681963713981, \"eval_f1\": 0.8128822989593734, \"eval_mcc\": 0.6702123339528314, \"eval_auc\": 0.9002762178696984, \"eval_precision\": 0.848324306192717, \"eval_recall\": 0.8223896269917363, \"learning_rate\": 1.9774011299435028e-05, \"loss\": 0.040771933633368465, \"step\": 900}\n",
      "12/20/2023 22:24:47 - INFO - transformers.configuration_utils -   Configuration saved in ./results/your_models/bert/classic/m6Am/checkpoint-900/config.json\n",
      "12/20/2023 22:24:47 - INFO - transformers.modeling_utils -   Model weights saved in ./results/your_models/bert/classic/m6Am/checkpoint-900/pytorch_model.bin\n",
      "12/20/2023 22:24:47 - INFO - __main__ -   Saving model checkpoint to ./results/your_models/bert/classic/m6Am/checkpoint-900\n",
      "12/20/2023 22:24:48 - INFO - __main__ -   Saving optimizer and scheduler states to ./results/your_models/bert/classic/m6Am/checkpoint-900\n",
      "\n",
      "Iteration:  63%|███████████████████▍           | 74/118 [00:05<00:10,  4.30it/s]\u001b[A\n",
      "Iteration:  64%|███████████████████▉           | 76/118 [00:05<00:07,  5.59it/s]\u001b[A\n",
      "Iteration:  66%|████████████████████▍          | 78/118 [00:05<00:05,  7.11it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████          | 80/118 [00:05<00:04,  8.78it/s]\u001b[A\n",
      "Iteration:  69%|█████████████████████▌         | 82/118 [00:05<00:03, 10.51it/s]\u001b[A\n",
      "Iteration:  71%|██████████████████████         | 84/118 [00:05<00:02, 12.18it/s]\u001b[A\n",
      "Iteration:  73%|██████████████████████▌        | 86/118 [00:05<00:02, 13.72it/s]\u001b[A\n",
      "Iteration:  75%|███████████████████████        | 88/118 [00:05<00:01, 15.04it/s]\u001b[A\n",
      "Iteration:  76%|███████████████████████▋       | 90/118 [00:05<00:01, 16.14it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▏      | 92/118 [00:05<00:01, 17.01it/s]\u001b[A\n",
      "Iteration:  80%|████████████████████████▋      | 94/118 [00:06<00:01, 17.67it/s]\u001b[A\n",
      "Iteration:  81%|█████████████████████████▏     | 96/118 [00:06<00:01, 18.17it/s]\u001b[A\n",
      "Iteration:  83%|█████████████████████████▋     | 98/118 [00:06<00:01, 18.51it/s]\u001b[A\n",
      "Iteration:  85%|█████████████████████████▍    | 100/118 [00:06<00:00, 18.77it/s]\u001b[AGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "\n",
      "Iteration:  87%|██████████████████████████▏   | 103/118 [00:06<00:00, 19.57it/s]\u001b[A\n",
      "Iteration:  89%|██████████████████████████▋   | 105/118 [00:06<00:00, 19.48it/s]\u001b[A\n",
      "Iteration:  91%|███████████████████████████▏  | 107/118 [00:06<00:00, 19.48it/s]\u001b[A\n",
      "Iteration:  92%|███████████████████████████▋  | 109/118 [00:06<00:00, 19.47it/s]\u001b[A\n",
      "Iteration:  94%|████████████████████████████▏ | 111/118 [00:06<00:00, 19.46it/s]\u001b[A\n",
      "Iteration:  96%|████████████████████████████▋ | 113/118 [00:07<00:00, 19.46it/s]\u001b[A\n",
      "Iteration:  97%|█████████████████████████████▏| 115/118 [00:07<00:00, 19.45it/s]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████| 118/118 [00:07<00:00, 16.23it/s]\u001b[A\n",
      "Epoch:  32%|███████████▊                         | 8/25 [01:00<02:07,  7.51s/it]\n",
      "Iteration:   0%|                                        | 0/118 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▌                               | 2/118 [00:00<00:05, 19.43it/s]\u001b[A\n",
      "Iteration:   3%|█                               | 4/118 [00:00<00:05, 19.45it/s]\u001b[A\n",
      "Iteration:   5%|█▋                              | 6/118 [00:00<00:05, 19.40it/s]\u001b[A\n",
      "Iteration:   7%|██▏                             | 8/118 [00:00<00:05, 19.41it/s]\u001b[A\n",
      "Iteration:   8%|██▋                            | 10/118 [00:00<00:05, 19.43it/s]\u001b[A\n",
      "Iteration:  10%|███▏                           | 12/118 [00:00<00:05, 19.42it/s]\u001b[A\n",
      "Iteration:  12%|███▋                           | 14/118 [00:00<00:05, 19.44it/s]\u001b[A\n",
      "Iteration:  14%|████▏                          | 16/118 [00:00<00:05, 19.45it/s]\u001b[A\n",
      "Iteration:  15%|████▋                          | 18/118 [00:00<00:05, 19.45it/s]\u001b[A\n",
      "Iteration:  17%|█████▎                         | 20/118 [00:01<00:05, 19.43it/s]\u001b[A\n",
      "Iteration:  19%|█████▊                         | 22/118 [00:01<00:04, 19.43it/s]\u001b[A\n",
      "Iteration:  20%|██████▎                        | 24/118 [00:01<00:04, 19.43it/s]\u001b[A\n",
      "Iteration:  22%|██████▊                        | 26/118 [00:01<00:04, 19.38it/s]\u001b[A\n",
      "Iteration:  24%|███████▎                       | 28/118 [00:01<00:04, 19.40it/s]\u001b[A\n",
      "Iteration:  25%|███████▉                       | 30/118 [00:01<00:04, 19.41it/s]\u001b[A\n",
      "Iteration:  27%|████████▍                      | 32/118 [00:01<00:04, 19.41it/s]\u001b[A\n",
      "Iteration:  29%|████████▉                      | 34/118 [00:01<00:04, 19.43it/s]\u001b[A\n",
      "Iteration:  31%|█████████▍                     | 36/118 [00:01<00:04, 19.43it/s]\u001b[A\n",
      "Iteration:  32%|█████████▉                     | 38/118 [00:01<00:04, 19.43it/s]\u001b[A\n",
      "Iteration:  34%|██████████▌                    | 40/118 [00:02<00:04, 19.44it/s]\u001b[A\n",
      "Iteration:  36%|███████████                    | 42/118 [00:02<00:03, 19.45it/s]\u001b[A\n",
      "Iteration:  37%|███████████▌                   | 44/118 [00:02<00:03, 19.45it/s]\u001b[A\n",
      "Iteration:  39%|████████████                   | 46/118 [00:02<00:03, 19.45it/s]\u001b[A\n",
      "Iteration:  41%|████████████▌                  | 48/118 [00:02<00:03, 19.45it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▏                 | 50/118 [00:02<00:03, 19.45it/s]\u001b[A\n",
      "Iteration:  44%|█████████████▋                 | 52/118 [00:02<00:03, 19.46it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▏                | 54/118 [00:02<00:03, 19.46it/s]\u001b[A12/20/2023 22:24:53 - INFO - __main__ -   Loading features from cached file ./dataset/bert/classic/m6Am/cached_dev_m6Am_101_dnaprom\n",
      "12/20/2023 22:24:53 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/20/2023 22:24:53 - INFO - __main__ -     Num examples = 937\n",
      "12/20/2023 22:24:53 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                        | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|██████████▎                    | 10/30 [00:00<00:00, 94.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|████████████████████▋          | 20/30 [00:00<00:00, 95.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████| 30/30 [00:00<00:00, 95.17it/s]\u001b[A\u001b[A\n",
      "12/20/2023 22:24:54 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/20/2023 22:24:54 - INFO - __main__ -     acc = 0.8271077908217717\n",
      "12/20/2023 22:24:54 - INFO - __main__ -     auc = 0.9116125644888826\n",
      "12/20/2023 22:24:54 - INFO - __main__ -     f1 = 0.8262717683898259\n",
      "12/20/2023 22:24:54 - INFO - __main__ -     mcc = 0.6749920846459081\n",
      "12/20/2023 22:24:54 - INFO - __main__ -     precision = 0.8430380642422528\n",
      "12/20/2023 22:24:54 - INFO - __main__ -     recall = 0.8320435556773045\n",
      "/root/.conda/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "{\"eval_acc\": 0.8271077908217717, \"eval_f1\": 0.8262717683898259, \"eval_mcc\": 0.6749920846459081, \"eval_auc\": 0.9116125644888826, \"eval_precision\": 0.8430380642422528, \"eval_recall\": 0.8320435556773045, \"learning_rate\": 1.864406779661017e-05, \"loss\": 0.04307100430247374, \"step\": 1000}\n",
      "12/20/2023 22:24:54 - INFO - transformers.configuration_utils -   Configuration saved in ./results/your_models/bert/classic/m6Am/checkpoint-1000/config.json\n",
      "12/20/2023 22:24:54 - INFO - transformers.modeling_utils -   Model weights saved in ./results/your_models/bert/classic/m6Am/checkpoint-1000/pytorch_model.bin\n",
      "12/20/2023 22:24:54 - INFO - __main__ -   Saving model checkpoint to ./results/your_models/bert/classic/m6Am/checkpoint-1000\n",
      "12/20/2023 22:24:54 - INFO - __main__ -   Saving optimizer and scheduler states to ./results/your_models/bert/classic/m6Am/checkpoint-1000\n",
      "\n",
      "Iteration:  47%|██████████████▋                | 56/118 [00:04<00:14,  4.30it/s]\u001b[A\n",
      "Iteration:  49%|███████████████▏               | 58/118 [00:04<00:10,  5.60it/s]\u001b[A\n",
      "Iteration:  51%|███████████████▊               | 60/118 [00:04<00:08,  7.12it/s]\u001b[A\n",
      "Iteration:  53%|████████████████▎              | 62/118 [00:04<00:06,  8.80it/s]\u001b[A\n",
      "Iteration:  54%|████████████████▊              | 64/118 [00:04<00:05, 10.53it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▎             | 66/118 [00:04<00:04, 12.21it/s]\u001b[A\n",
      "Iteration:  58%|█████████████████▊             | 68/118 [00:04<00:03, 13.74it/s]\u001b[A\n",
      "Iteration:  59%|██████████████████▍            | 70/118 [00:04<00:03, 15.07it/s]\u001b[A\n",
      "Iteration:  61%|██████████████████▉            | 72/118 [00:04<00:02, 16.15it/s]\u001b[A\n",
      "Iteration:  63%|███████████████████▍           | 74/118 [00:05<00:02, 17.02it/s]\u001b[A\n",
      "Iteration:  64%|███████████████████▉           | 76/118 [00:05<00:02, 17.67it/s]\u001b[A\n",
      "Iteration:  66%|████████████████████▍          | 78/118 [00:05<00:02, 18.08it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████          | 80/118 [00:05<00:02, 18.38it/s]\u001b[A\n",
      "Iteration:  69%|█████████████████████▌         | 82/118 [00:05<00:01, 18.68it/s]\u001b[A\n",
      "Iteration:  71%|██████████████████████         | 84/118 [00:05<00:01, 18.90it/s]\u001b[A\n",
      "Iteration:  73%|██████████████████████▌        | 86/118 [00:05<00:01, 19.06it/s]\u001b[A\n",
      "Iteration:  75%|███████████████████████        | 88/118 [00:05<00:01, 19.17it/s]\u001b[A\n",
      "Iteration:  76%|███████████████████████▋       | 90/118 [00:05<00:01, 19.26it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▏      | 92/118 [00:05<00:01, 19.31it/s]\u001b[A\n",
      "Iteration:  80%|████████████████████████▋      | 94/118 [00:06<00:01, 19.36it/s]\u001b[A\n",
      "Iteration:  81%|█████████████████████████▏     | 96/118 [00:06<00:01, 19.38it/s]\u001b[A\n",
      "Iteration:  83%|█████████████████████████▋     | 98/118 [00:06<00:01, 19.41it/s]\u001b[A\n",
      "Iteration:  85%|█████████████████████████▍    | 100/118 [00:06<00:00, 19.40it/s]\u001b[A\n",
      "Iteration:  86%|█████████████████████████▉    | 102/118 [00:06<00:00, 19.40it/s]\u001b[A\n",
      "Iteration:  88%|██████████████████████████▍   | 104/118 [00:06<00:00, 19.42it/s]\u001b[A\n",
      "Iteration:  90%|██████████████████████████▉   | 106/118 [00:06<00:00, 19.42it/s]\u001b[A\n",
      "Iteration:  92%|███████████████████████████▍  | 108/118 [00:06<00:00, 19.43it/s]\u001b[A\n",
      "Iteration:  93%|███████████████████████████▉  | 110/118 [00:06<00:00, 19.45it/s]\u001b[A\n",
      "Iteration:  95%|████████████████████████████▍ | 112/118 [00:06<00:00, 19.45it/s]\u001b[A\n",
      "Iteration:  97%|████████████████████████████▉ | 114/118 [00:07<00:00, 19.46it/s]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████| 118/118 [00:07<00:00, 16.21it/s]\u001b[A\n",
      "Epoch:  36%|█████████████▎                       | 9/25 [01:07<01:59,  7.44s/it]\n",
      "Iteration:   0%|                                        | 0/118 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▌                               | 2/118 [00:00<00:05, 19.38it/s]\u001b[A\n",
      "Iteration:   3%|█                               | 4/118 [00:00<00:05, 19.43it/s]\u001b[A\n",
      "Iteration:   5%|█▋                              | 6/118 [00:00<00:05, 19.35it/s]\u001b[A\n",
      "Iteration:   7%|██▏                             | 8/118 [00:00<00:05, 19.37it/s]\u001b[A\n",
      "Iteration:   8%|██▋                            | 10/118 [00:00<00:05, 19.41it/s]\u001b[A\n",
      "Iteration:  10%|███▏                           | 12/118 [00:00<00:05, 19.42it/s]\u001b[A\n",
      "Iteration:  12%|███▋                           | 14/118 [00:00<00:05, 19.44it/s]\u001b[A\n",
      "Iteration:  14%|████▏                          | 16/118 [00:00<00:05, 19.45it/s]\u001b[A\n",
      "Iteration:  15%|████▋                          | 18/118 [00:00<00:05, 19.45it/s]\u001b[A\n",
      "Iteration:  17%|█████▎                         | 20/118 [00:01<00:05, 19.45it/s]\u001b[A\n",
      "Iteration:  19%|█████▊                         | 22/118 [00:01<00:04, 19.45it/s]\u001b[A\n",
      "Iteration:  20%|██████▎                        | 24/118 [00:01<00:04, 19.45it/s]\u001b[A\n",
      "Iteration:  22%|██████▊                        | 26/118 [00:01<00:04, 19.45it/s]\u001b[A\n",
      "Iteration:  24%|███████▎                       | 28/118 [00:01<00:04, 19.46it/s]\u001b[A\n",
      "Iteration:  25%|███████▉                       | 30/118 [00:01<00:04, 19.46it/s]\u001b[A\n",
      "Iteration:  27%|████████▍                      | 32/118 [00:01<00:04, 19.45it/s]\u001b[A\n",
      "Iteration:  29%|████████▉                      | 34/118 [00:01<00:04, 19.45it/s]\u001b[A\n",
      "Iteration:  31%|█████████▍                     | 36/118 [00:01<00:04, 19.45it/s]\u001b[A12/20/2023 22:25:00 - INFO - __main__ -   Loading features from cached file ./dataset/bert/classic/m6Am/cached_dev_m6Am_101_dnaprom\n",
      "12/20/2023 22:25:00 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/20/2023 22:25:00 - INFO - __main__ -     Num examples = 937\n",
      "12/20/2023 22:25:00 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                        | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|██████████▎                    | 10/30 [00:00<00:00, 92.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|████████████████████▋          | 20/30 [00:00<00:00, 94.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████| 30/30 [00:00<00:00, 94.79it/s]\u001b[A\u001b[A\n",
      "12/20/2023 22:25:00 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/20/2023 22:25:00 - INFO - __main__ -     acc = 0.8463180362860192\n",
      "12/20/2023 22:25:00 - INFO - __main__ -     auc = 0.9007510386705018\n",
      "12/20/2023 22:25:00 - INFO - __main__ -     f1 = 0.8458247634718223\n",
      "12/20/2023 22:25:00 - INFO - __main__ -     mcc = 0.7097022845995506\n",
      "12/20/2023 22:25:00 - INFO - __main__ -     precision = 0.859045909639061\n",
      "12/20/2023 22:25:00 - INFO - __main__ -     recall = 0.8507053828242706\n",
      "/root/.conda/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "{\"eval_acc\": 0.8463180362860192, \"eval_f1\": 0.8458247634718223, \"eval_mcc\": 0.7097022845995506, \"eval_auc\": 0.9007510386705018, \"eval_precision\": 0.859045909639061, \"eval_recall\": 0.8507053828242706, \"learning_rate\": 1.7514124293785312e-05, \"loss\": 0.02317792140151141, \"step\": 1100}\n",
      "12/20/2023 22:25:00 - INFO - transformers.configuration_utils -   Configuration saved in ./results/your_models/bert/classic/m6Am/checkpoint-1100/config.json\n",
      "12/20/2023 22:25:00 - INFO - transformers.modeling_utils -   Model weights saved in ./results/your_models/bert/classic/m6Am/checkpoint-1100/pytorch_model.bin\n",
      "12/20/2023 22:25:00 - INFO - __main__ -   Saving model checkpoint to ./results/your_models/bert/classic/m6Am/checkpoint-1100\n",
      "12/20/2023 22:25:01 - INFO - __main__ -   Saving optimizer and scheduler states to ./results/your_models/bert/classic/m6Am/checkpoint-1100\n",
      "\n",
      "Iteration:  32%|█████████▉                     | 38/118 [00:03<00:18,  4.29it/s]\u001b[A\n",
      "Iteration:  34%|██████████▌                    | 40/118 [00:03<00:13,  5.59it/s]\u001b[A\n",
      "Iteration:  36%|███████████                    | 42/118 [00:03<00:10,  7.11it/s]\u001b[A\n",
      "Iteration:  37%|███████████▌                   | 44/118 [00:03<00:08,  8.78it/s]\u001b[A\n",
      "Iteration:  39%|████████████                   | 46/118 [00:03<00:06, 10.51it/s]\u001b[A\n",
      "Iteration:  41%|████████████▌                  | 48/118 [00:03<00:05, 12.19it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▏                 | 50/118 [00:03<00:04, 13.73it/s]\u001b[A\n",
      "Iteration:  44%|█████████████▋                 | 52/118 [00:03<00:04, 15.06it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▏                | 54/118 [00:03<00:03, 16.13it/s]\u001b[A\n",
      "Iteration:  47%|██████████████▋                | 56/118 [00:04<00:03, 16.94it/s]\u001b[A\n",
      "Iteration:  49%|███████████████▏               | 58/118 [00:04<00:03, 17.42it/s]\u001b[A\n",
      "Iteration:  51%|███████████████▊               | 60/118 [00:04<00:03, 17.96it/s]\u001b[A\n",
      "Iteration:  53%|████████████████▎              | 62/118 [00:04<00:03, 18.37it/s]\u001b[A\n",
      "Iteration:  54%|████████████████▊              | 64/118 [00:04<00:02, 18.68it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▎             | 66/118 [00:04<00:02, 18.90it/s]\u001b[A\n",
      "Iteration:  58%|█████████████████▊             | 68/118 [00:04<00:02, 19.06it/s]\u001b[A\n",
      "Iteration:  59%|██████████████████▍            | 70/118 [00:04<00:02, 19.17it/s]\u001b[A\n",
      "Iteration:  61%|██████████████████▉            | 72/118 [00:04<00:02, 19.26it/s]\u001b[A\n",
      "Iteration:  63%|███████████████████▍           | 74/118 [00:05<00:02, 19.32it/s]\u001b[A\n",
      "Iteration:  64%|███████████████████▉           | 76/118 [00:05<00:02, 19.34it/s]\u001b[A\n",
      "Iteration:  66%|████████████████████▍          | 78/118 [00:05<00:02, 19.35it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████          | 80/118 [00:05<00:01, 19.38it/s]\u001b[A\n",
      "Iteration:  69%|█████████████████████▌         | 82/118 [00:05<00:01, 19.40it/s]\u001b[A\n",
      "Iteration:  71%|██████████████████████         | 84/118 [00:05<00:01, 19.41it/s]\u001b[A\n",
      "Iteration:  73%|██████████████████████▌        | 86/118 [00:05<00:01, 19.43it/s]\u001b[A\n",
      "Iteration:  75%|███████████████████████        | 88/118 [00:05<00:01, 19.44it/s]\u001b[A\n",
      "Iteration:  76%|███████████████████████▋       | 90/118 [00:05<00:01, 19.45it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▏      | 92/118 [00:05<00:01, 19.45it/s]\u001b[A\n",
      "Iteration:  80%|████████████████████████▋      | 94/118 [00:06<00:01, 19.38it/s]\u001b[A\n",
      "Iteration:  81%|█████████████████████████▏     | 96/118 [00:06<00:01, 19.38it/s]\u001b[A\n",
      "Iteration:  83%|█████████████████████████▋     | 98/118 [00:06<00:01, 19.41it/s]\u001b[A\n",
      "Iteration:  85%|█████████████████████████▍    | 100/118 [00:06<00:00, 19.41it/s]\u001b[A\n",
      "Iteration:  86%|█████████████████████████▉    | 102/118 [00:06<00:00, 19.43it/s]\u001b[A\n",
      "Iteration:  88%|██████████████████████████▍   | 104/118 [00:06<00:00, 19.44it/s]\u001b[A\n",
      "Iteration:  90%|██████████████████████████▉   | 106/118 [00:06<00:00, 19.44it/s]\u001b[A\n",
      "Iteration:  92%|███████████████████████████▍  | 108/118 [00:06<00:00, 19.41it/s]\u001b[A\n",
      "Iteration:  93%|███████████████████████████▉  | 110/118 [00:06<00:00, 19.42it/s]\u001b[A\n",
      "Iteration:  95%|████████████████████████████▍ | 112/118 [00:06<00:00, 19.43it/s]\u001b[A\n",
      "Iteration:  97%|████████████████████████████▉ | 114/118 [00:07<00:00, 19.44it/s]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████| 118/118 [00:07<00:00, 16.20it/s]\u001b[A\n",
      "Epoch:  40%|██████████████▍                     | 10/25 [01:15<01:50,  7.39s/it]\n",
      "Iteration:   0%|                                        | 0/118 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▌                               | 2/118 [00:00<00:05, 19.41it/s]\u001b[A\n",
      "Iteration:   3%|█                               | 4/118 [00:00<00:05, 19.43it/s]\u001b[A\n",
      "Iteration:   5%|█▋                              | 6/118 [00:00<00:05, 19.43it/s]\u001b[A\n",
      "Iteration:   7%|██▏                             | 8/118 [00:00<00:05, 19.42it/s]\u001b[A\n",
      "Iteration:   8%|██▋                            | 10/118 [00:00<00:05, 19.43it/s]\u001b[A\n",
      "Iteration:  10%|███▏                           | 12/118 [00:00<00:05, 19.44it/s]\u001b[A\n",
      "Iteration:  12%|███▋                           | 14/118 [00:00<00:05, 19.40it/s]\u001b[A\n",
      "Iteration:  14%|████▏                          | 16/118 [00:00<00:05, 19.39it/s]\u001b[A\n",
      "Iteration:  15%|████▋                          | 18/118 [00:00<00:05, 19.35it/s]\u001b[A12/20/2023 22:25:06 - INFO - __main__ -   Loading features from cached file ./dataset/bert/classic/m6Am/cached_dev_m6Am_101_dnaprom\n",
      "12/20/2023 22:25:06 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/20/2023 22:25:06 - INFO - __main__ -     Num examples = 937\n",
      "12/20/2023 22:25:06 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                        | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|██████████▎                    | 10/30 [00:00<00:00, 93.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|████████████████████▋          | 20/30 [00:00<00:00, 94.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████| 30/30 [00:00<00:00, 94.43it/s]\u001b[A\u001b[A\n",
      "12/20/2023 22:25:06 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/20/2023 22:25:06 - INFO - __main__ -     acc = 0.8367129135538954\n",
      "12/20/2023 22:25:06 - INFO - __main__ -     auc = 0.8998470529151257\n",
      "12/20/2023 22:25:06 - INFO - __main__ -     f1 = 0.8360848611658978\n",
      "12/20/2023 22:25:06 - INFO - __main__ -     mcc = 0.6919668854075446\n",
      "12/20/2023 22:25:06 - INFO - __main__ -     precision = 0.850705064454994\n",
      "12/20/2023 22:25:06 - INFO - __main__ -     recall = 0.8413253892160891\n",
      "/root/.conda/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "{\"eval_acc\": 0.8367129135538954, \"eval_f1\": 0.8360848611658978, \"eval_mcc\": 0.6919668854075446, \"eval_auc\": 0.8998470529151257, \"eval_precision\": 0.850705064454994, \"eval_recall\": 0.8413253892160891, \"learning_rate\": 1.6384180790960454e-05, \"loss\": 0.026277767091523855, \"step\": 1200}\n",
      "12/20/2023 22:25:06 - INFO - transformers.configuration_utils -   Configuration saved in ./results/your_models/bert/classic/m6Am/checkpoint-1200/config.json\n",
      "12/20/2023 22:25:07 - INFO - transformers.modeling_utils -   Model weights saved in ./results/your_models/bert/classic/m6Am/checkpoint-1200/pytorch_model.bin\n",
      "12/20/2023 22:25:07 - INFO - __main__ -   Saving model checkpoint to ./results/your_models/bert/classic/m6Am/checkpoint-1200\n",
      "12/20/2023 22:25:07 - INFO - __main__ -   Saving optimizer and scheduler states to ./results/your_models/bert/classic/m6Am/checkpoint-1200\n",
      "\n",
      "Iteration:  17%|█████▎                         | 20/118 [00:02<00:23,  4.20it/s]\u001b[A\n",
      "Iteration:  19%|█████▊                         | 22/118 [00:02<00:17,  5.51it/s]\u001b[A\n",
      "Iteration:  20%|██████▎                        | 24/118 [00:02<00:13,  7.05it/s]\u001b[A\n",
      "Iteration:  22%|██████▊                        | 26/118 [00:02<00:10,  8.74it/s]\u001b[A\n",
      "Iteration:  24%|███████▎                       | 28/118 [00:02<00:08, 10.48it/s]\u001b[A\n",
      "Iteration:  25%|███████▉                       | 30/118 [00:02<00:07, 12.16it/s]\u001b[A\n",
      "Iteration:  27%|████████▍                      | 32/118 [00:02<00:06, 13.69it/s]\u001b[A\n",
      "Iteration:  29%|████████▉                      | 34/118 [00:02<00:05, 15.02it/s]\u001b[A\n",
      "Iteration:  31%|█████████▍                     | 36/118 [00:03<00:05, 16.12it/s]\u001b[A\n",
      "Iteration:  32%|█████████▉                     | 38/118 [00:03<00:04, 16.99it/s]\u001b[A\n",
      "Iteration:  34%|██████████▌                    | 40/118 [00:03<00:04, 17.67it/s]\u001b[A\n",
      "Iteration:  36%|███████████                    | 42/118 [00:03<00:04, 18.16it/s]\u001b[A\n",
      "Iteration:  37%|███████████▌                   | 44/118 [00:03<00:03, 18.52it/s]\u001b[A\n",
      "Iteration:  39%|████████████                   | 46/118 [00:03<00:03, 18.80it/s]\u001b[A\n",
      "Iteration:  41%|████████████▌                  | 48/118 [00:03<00:03, 19.00it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▏                 | 50/118 [00:03<00:03, 19.15it/s]\u001b[A\n",
      "Iteration:  44%|█████████████▋                 | 52/118 [00:03<00:03, 19.22it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▏                | 54/118 [00:03<00:03, 19.24it/s]\u001b[A\n",
      "Iteration:  47%|██████████████▋                | 56/118 [00:04<00:03, 19.30it/s]\u001b[A\n",
      "Iteration:  49%|███████████████▏               | 58/118 [00:04<00:03, 19.35it/s]\u001b[A\n",
      "Iteration:  51%|███████████████▊               | 60/118 [00:04<00:02, 19.38it/s]\u001b[A\n",
      "Iteration:  53%|████████████████▎              | 62/118 [00:04<00:02, 19.40it/s]\u001b[A\n",
      "Iteration:  54%|████████████████▊              | 64/118 [00:04<00:02, 19.42it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▎             | 66/118 [00:04<00:02, 19.43it/s]\u001b[A\n",
      "Iteration:  58%|█████████████████▊             | 68/118 [00:04<00:02, 19.41it/s]\u001b[A\n",
      "Iteration:  59%|██████████████████▍            | 70/118 [00:04<00:02, 19.40it/s]\u001b[A\n",
      "Iteration:  61%|██████████████████▉            | 72/118 [00:04<00:02, 19.42it/s]\u001b[A\n",
      "Iteration:  63%|███████████████████▍           | 74/118 [00:05<00:02, 19.42it/s]\u001b[A\n",
      "Iteration:  64%|███████████████████▉           | 76/118 [00:05<00:02, 19.42it/s]\u001b[A\n",
      "Iteration:  66%|████████████████████▍          | 78/118 [00:05<00:02, 19.44it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████          | 80/118 [00:05<00:01, 19.44it/s]\u001b[A\n",
      "Iteration:  69%|█████████████████████▌         | 82/118 [00:05<00:01, 19.44it/s]\u001b[A\n",
      "Iteration:  71%|██████████████████████         | 84/118 [00:05<00:01, 19.45it/s]\u001b[A\n",
      "Iteration:  73%|██████████████████████▌        | 86/118 [00:05<00:01, 19.46it/s]\u001b[A\n",
      "Iteration:  75%|███████████████████████        | 88/118 [00:05<00:01, 19.45it/s]\u001b[A\n",
      "Iteration:  76%|███████████████████████▋       | 90/118 [00:05<00:01, 19.46it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▏      | 92/118 [00:05<00:01, 19.46it/s]\u001b[A\n",
      "Iteration:  80%|████████████████████████▋      | 94/118 [00:06<00:01, 19.47it/s]\u001b[A\n",
      "Iteration:  81%|█████████████████████████▏     | 96/118 [00:06<00:01, 19.46it/s]\u001b[A\n",
      "Iteration:  83%|█████████████████████████▋     | 98/118 [00:06<00:01, 19.48it/s]\u001b[A\n",
      "Iteration:  85%|█████████████████████████▍    | 100/118 [00:06<00:00, 19.47it/s]\u001b[A\n",
      "Iteration:  86%|█████████████████████████▉    | 102/118 [00:06<00:00, 19.47it/s]\u001b[A\n",
      "Iteration:  88%|██████████████████████████▍   | 104/118 [00:06<00:00, 19.47it/s]\u001b[A\n",
      "Iteration:  90%|██████████████████████████▉   | 106/118 [00:06<00:00, 19.46it/s]\u001b[A\n",
      "Iteration:  92%|███████████████████████████▍  | 108/118 [00:06<00:00, 19.41it/s]\u001b[A\n",
      "Iteration:  93%|███████████████████████████▉  | 110/118 [00:06<00:00, 19.42it/s]\u001b[A\n",
      "Iteration:  95%|████████████████████████████▍ | 112/118 [00:06<00:00, 19.38it/s]\u001b[A\n",
      "Iteration:  97%|████████████████████████████▉ | 114/118 [00:07<00:00, 19.35it/s]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████| 118/118 [00:07<00:00, 16.21it/s]\u001b[A\n",
      "Epoch:  44%|███████████████▊                    | 11/25 [01:22<01:43,  7.36s/it]\n",
      "Iteration:   0%|                                        | 0/118 [00:00<?, ?it/s]\u001b[A12/20/2023 22:25:12 - INFO - __main__ -   Loading features from cached file ./dataset/bert/classic/m6Am/cached_dev_m6Am_101_dnaprom\n",
      "12/20/2023 22:25:12 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/20/2023 22:25:12 - INFO - __main__ -     Num examples = 937\n",
      "12/20/2023 22:25:12 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                        | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|██████████▎                    | 10/30 [00:00<00:00, 94.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|████████████████████▋          | 20/30 [00:00<00:00, 94.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████| 30/30 [00:00<00:00, 94.93it/s]\u001b[A\u001b[A\n",
      "12/20/2023 22:25:13 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/20/2023 22:25:13 - INFO - __main__ -     acc = 0.8420490928495198\n",
      "12/20/2023 22:25:13 - INFO - __main__ -     auc = 0.900287631831256\n",
      "12/20/2023 22:25:13 - INFO - __main__ -     f1 = 0.8412853192697174\n",
      "12/20/2023 22:25:13 - INFO - __main__ -     mcc = 0.7054294919759587\n",
      "12/20/2023 22:25:13 - INFO - __main__ -     precision = 0.8585066742134805\n",
      "12/20/2023 22:25:13 - INFO - __main__ -     recall = 0.847016390448797\n",
      "/root/.conda/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "{\"eval_acc\": 0.8420490928495198, \"eval_f1\": 0.8412853192697174, \"eval_mcc\": 0.7054294919759587, \"eval_auc\": 0.900287631831256, \"eval_precision\": 0.8585066742134805, \"eval_recall\": 0.847016390448797, \"learning_rate\": 1.5254237288135592e-05, \"loss\": 0.017346974851679988, \"step\": 1300}\n",
      "12/20/2023 22:25:13 - INFO - transformers.configuration_utils -   Configuration saved in ./results/your_models/bert/classic/m6Am/checkpoint-1300/config.json\n",
      "12/20/2023 22:25:13 - INFO - transformers.modeling_utils -   Model weights saved in ./results/your_models/bert/classic/m6Am/checkpoint-1300/pytorch_model.bin\n",
      "12/20/2023 22:25:13 - INFO - __main__ -   Saving model checkpoint to ./results/your_models/bert/classic/m6Am/checkpoint-1300\n",
      "12/20/2023 22:25:13 - INFO - __main__ -   Saving optimizer and scheduler states to ./results/your_models/bert/classic/m6Am/checkpoint-1300\n",
      "\n",
      "Iteration:   2%|▌                               | 2/118 [00:01<01:16,  1.52it/s]\u001b[A\n",
      "Iteration:   3%|█                               | 4/118 [00:01<00:34,  3.32it/s]\u001b[A\n",
      "Iteration:   5%|█▋                              | 6/118 [00:01<00:21,  5.33it/s]\u001b[A\n",
      "Iteration:   7%|██▏                             | 8/118 [00:01<00:14,  7.48it/s]\u001b[A\n",
      "Iteration:   8%|██▋                            | 10/118 [00:01<00:11,  9.61it/s]\u001b[A\n",
      "Iteration:  10%|███▏                           | 12/118 [00:01<00:09, 11.60it/s]\u001b[A\n",
      "Iteration:  12%|███▋                           | 14/118 [00:01<00:07, 13.36it/s]\u001b[A\n",
      "Iteration:  14%|████▏                          | 16/118 [00:02<00:06, 14.83it/s]\u001b[A\n",
      "Iteration:  15%|████▋                          | 18/118 [00:02<00:06, 16.03it/s]\u001b[A\n",
      "Iteration:  17%|█████▎                         | 20/118 [00:02<00:05, 16.95it/s]\u001b[A\n",
      "Iteration:  19%|█████▊                         | 22/118 [00:02<00:05, 17.65it/s]\u001b[A\n",
      "Iteration:  20%|██████▎                        | 24/118 [00:02<00:05, 18.16it/s]\u001b[A\n",
      "Iteration:  22%|██████▊                        | 26/118 [00:02<00:04, 18.54it/s]\u001b[A\n",
      "Iteration:  24%|███████▎                       | 28/118 [00:02<00:04, 18.81it/s]\u001b[A\n",
      "Iteration:  25%|███████▉                       | 30/118 [00:02<00:04, 18.83it/s]\u001b[A\n",
      "Iteration:  27%|████████▍                      | 32/118 [00:02<00:04, 19.01it/s]\u001b[AGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "\n",
      "Iteration:  30%|█████████▏                     | 35/118 [00:03<00:04, 19.73it/s]\u001b[A\n",
      "Iteration:  31%|█████████▋                     | 37/118 [00:03<00:04, 19.65it/s]\u001b[A\n",
      "Iteration:  33%|██████████▏                    | 39/118 [00:03<00:04, 19.60it/s]\u001b[A\n",
      "Iteration:  35%|██████████▊                    | 41/118 [00:03<00:03, 19.49it/s]\u001b[A\n",
      "Iteration:  36%|███████████▎                   | 43/118 [00:03<00:03, 19.27it/s]\u001b[A\n",
      "Iteration:  38%|███████████▊                   | 45/118 [00:03<00:03, 19.29it/s]\u001b[A\n",
      "Iteration:  40%|████████████▎                  | 47/118 [00:03<00:03, 19.35it/s]\u001b[A\n",
      "Iteration:  42%|████████████▊                  | 49/118 [00:03<00:03, 19.37it/s]\u001b[A\n",
      "Iteration:  43%|█████████████▍                 | 51/118 [00:03<00:03, 19.37it/s]\u001b[A\n",
      "Iteration:  45%|█████████████▉                 | 53/118 [00:03<00:03, 19.39it/s]\u001b[A\n",
      "Iteration:  47%|██████████████▍                | 55/118 [00:04<00:03, 19.41it/s]\u001b[A\n",
      "Iteration:  48%|██████████████▉                | 57/118 [00:04<00:03, 19.42it/s]\u001b[A\n",
      "Iteration:  50%|███████████████▌               | 59/118 [00:04<00:03, 19.44it/s]\u001b[A\n",
      "Iteration:  52%|████████████████               | 61/118 [00:04<00:02, 19.44it/s]\u001b[A\n",
      "Iteration:  53%|████████████████▌              | 63/118 [00:04<00:02, 19.45it/s]\u001b[A\n",
      "Iteration:  55%|█████████████████              | 65/118 [00:04<00:02, 19.45it/s]\u001b[A\n",
      "Iteration:  57%|█████████████████▌             | 67/118 [00:04<00:02, 19.45it/s]\u001b[A\n",
      "Iteration:  58%|██████████████████▏            | 69/118 [00:04<00:02, 19.45it/s]\u001b[A\n",
      "Iteration:  60%|██████████████████▋            | 71/118 [00:04<00:02, 19.45it/s]\u001b[A\n",
      "Iteration:  62%|███████████████████▏           | 73/118 [00:04<00:02, 19.46it/s]\u001b[A\n",
      "Iteration:  64%|███████████████████▋           | 75/118 [00:05<00:02, 19.45it/s]\u001b[A\n",
      "Iteration:  65%|████████████████████▏          | 77/118 [00:05<00:02, 19.46it/s]\u001b[A\n",
      "Iteration:  67%|████████████████████▊          | 79/118 [00:05<00:02, 19.45it/s]\u001b[A\n",
      "Iteration:  69%|█████████████████████▎         | 81/118 [00:05<00:01, 19.46it/s]\u001b[A\n",
      "Iteration:  70%|█████████████████████▊         | 83/118 [00:05<00:01, 19.41it/s]\u001b[A\n",
      "Iteration:  72%|██████████████████████▎        | 85/118 [00:05<00:01, 19.35it/s]\u001b[A\n",
      "Iteration:  74%|██████████████████████▊        | 87/118 [00:05<00:01, 19.38it/s]\u001b[A\n",
      "Iteration:  75%|███████████████████████▍       | 89/118 [00:05<00:01, 19.39it/s]\u001b[A\n",
      "Iteration:  77%|███████████████████████▉       | 91/118 [00:05<00:01, 19.37it/s]\u001b[A\n",
      "Iteration:  79%|████████████████████████▍      | 93/118 [00:05<00:01, 19.39it/s]\u001b[A\n",
      "Iteration:  81%|████████████████████████▉      | 95/118 [00:06<00:01, 19.41it/s]\u001b[A\n",
      "Iteration:  82%|█████████████████████████▍     | 97/118 [00:06<00:01, 19.43it/s]\u001b[A\n",
      "Iteration:  84%|██████████████████████████     | 99/118 [00:06<00:00, 19.44it/s]\u001b[A\n",
      "Iteration:  86%|█████████████████████████▋    | 101/118 [00:06<00:00, 19.45it/s]\u001b[A12/20/2023 22:25:19 - INFO - __main__ -   Loading features from cached file ./dataset/bert/classic/m6Am/cached_dev_m6Am_101_dnaprom\n",
      "12/20/2023 22:25:19 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/20/2023 22:25:19 - INFO - __main__ -     Num examples = 937\n",
      "12/20/2023 22:25:19 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                        | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|██████████▎                    | 10/30 [00:00<00:00, 93.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|████████████████████▋          | 20/30 [00:00<00:00, 94.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████| 30/30 [00:00<00:00, 94.65it/s]\u001b[A\u001b[A\n",
      "12/20/2023 22:25:19 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/20/2023 22:25:19 - INFO - __main__ -     acc = 0.8441835645677694\n",
      "12/20/2023 22:25:19 - INFO - __main__ -     auc = 0.8964959138017624\n",
      "12/20/2023 22:25:19 - INFO - __main__ -     f1 = 0.8435203740814963\n",
      "12/20/2023 22:25:19 - INFO - __main__ -     mcc = 0.7082503654814734\n",
      "12/20/2023 22:25:19 - INFO - __main__ -     precision = 0.8593678004193976\n",
      "12/20/2023 22:25:19 - INFO - __main__ -     recall = 0.8489590467059307\n",
      "/root/.conda/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "{\"eval_acc\": 0.8441835645677694, \"eval_f1\": 0.8435203740814963, \"eval_mcc\": 0.7082503654814734, \"eval_auc\": 0.8964959138017624, \"eval_precision\": 0.8593678004193976, \"eval_recall\": 0.8489590467059307, \"learning_rate\": 1.4124293785310734e-05, \"loss\": 0.018533992441371085, \"step\": 1400}\n",
      "12/20/2023 22:25:19 - INFO - transformers.configuration_utils -   Configuration saved in ./results/your_models/bert/classic/m6Am/checkpoint-1400/config.json\n",
      "12/20/2023 22:25:19 - INFO - transformers.modeling_utils -   Model weights saved in ./results/your_models/bert/classic/m6Am/checkpoint-1400/pytorch_model.bin\n",
      "12/20/2023 22:25:19 - INFO - __main__ -   Saving model checkpoint to ./results/your_models/bert/classic/m6Am/checkpoint-1400\n",
      "12/20/2023 22:25:20 - INFO - __main__ -   Saving optimizer and scheduler states to ./results/your_models/bert/classic/m6Am/checkpoint-1400\n",
      "\n",
      "Iteration:  87%|██████████████████████████▏   | 103/118 [00:07<00:03,  4.29it/s]\u001b[A\n",
      "Iteration:  89%|██████████████████████████▋   | 105/118 [00:07<00:02,  5.60it/s]\u001b[A\n",
      "Iteration:  91%|███████████████████████████▏  | 107/118 [00:07<00:01,  7.11it/s]\u001b[A\n",
      "Iteration:  92%|███████████████████████████▋  | 109/118 [00:08<00:01,  8.79it/s]\u001b[A\n",
      "Iteration:  94%|████████████████████████████▏ | 111/118 [00:08<00:00, 10.51it/s]\u001b[A\n",
      "Iteration:  96%|████████████████████████████▋ | 113/118 [00:08<00:00, 12.19it/s]\u001b[A\n",
      "Iteration:  97%|█████████████████████████████▏| 115/118 [00:08<00:00, 13.73it/s]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████| 118/118 [00:08<00:00, 13.90it/s]\u001b[A\n",
      "Epoch:  48%|█████████████████▎                  | 12/25 [01:31<01:40,  7.70s/it]\n",
      "Iteration:   0%|                                        | 0/118 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▌                               | 2/118 [00:00<00:05, 19.48it/s]\u001b[A\n",
      "Iteration:   3%|█                               | 4/118 [00:00<00:05, 19.45it/s]\u001b[A\n",
      "Iteration:   5%|█▋                              | 6/118 [00:00<00:05, 19.44it/s]\u001b[A\n",
      "Iteration:   7%|██▏                             | 8/118 [00:00<00:05, 19.45it/s]\u001b[A\n",
      "Iteration:   8%|██▋                            | 10/118 [00:00<00:05, 19.45it/s]\u001b[A\n",
      "Iteration:  10%|███▏                           | 12/118 [00:00<00:05, 19.46it/s]\u001b[A\n",
      "Iteration:  12%|███▋                           | 14/118 [00:00<00:05, 19.47it/s]\u001b[A\n",
      "Iteration:  14%|████▏                          | 16/118 [00:00<00:05, 19.47it/s]\u001b[A\n",
      "Iteration:  15%|████▋                          | 18/118 [00:00<00:05, 19.46it/s]\u001b[A\n",
      "Iteration:  17%|█████▎                         | 20/118 [00:01<00:05, 19.42it/s]\u001b[A\n",
      "Iteration:  19%|█████▊                         | 22/118 [00:01<00:04, 19.43it/s]\u001b[A\n",
      "Iteration:  20%|██████▎                        | 24/118 [00:01<00:04, 19.44it/s]\u001b[A\n",
      "Iteration:  22%|██████▊                        | 26/118 [00:01<00:04, 19.44it/s]\u001b[A\n",
      "Iteration:  24%|███████▎                       | 28/118 [00:01<00:04, 19.43it/s]\u001b[A\n",
      "Iteration:  25%|███████▉                       | 30/118 [00:01<00:04, 19.43it/s]\u001b[A\n",
      "Iteration:  27%|████████▍                      | 32/118 [00:01<00:04, 19.44it/s]\u001b[A\n",
      "Iteration:  29%|████████▉                      | 34/118 [00:01<00:04, 19.45it/s]\u001b[A\n",
      "Iteration:  31%|█████████▍                     | 36/118 [00:01<00:04, 19.46it/s]\u001b[A\n",
      "Iteration:  32%|█████████▉                     | 38/118 [00:01<00:04, 19.46it/s]\u001b[A\n",
      "Iteration:  34%|██████████▌                    | 40/118 [00:02<00:04, 19.44it/s]\u001b[A\n",
      "Iteration:  36%|███████████                    | 42/118 [00:02<00:03, 19.44it/s]\u001b[A\n",
      "Iteration:  37%|███████████▌                   | 44/118 [00:02<00:03, 19.45it/s]\u001b[A\n",
      "Iteration:  39%|████████████                   | 46/118 [00:02<00:03, 19.46it/s]\u001b[A\n",
      "Iteration:  41%|████████████▌                  | 48/118 [00:02<00:03, 19.45it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▏                 | 50/118 [00:02<00:03, 19.46it/s]\u001b[A\n",
      "Iteration:  44%|█████████████▋                 | 52/118 [00:02<00:03, 19.46it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▏                | 54/118 [00:02<00:03, 19.47it/s]\u001b[A\n",
      "Iteration:  47%|██████████████▋                | 56/118 [00:02<00:03, 19.46it/s]\u001b[A\n",
      "Iteration:  49%|███████████████▏               | 58/118 [00:02<00:03, 19.41it/s]\u001b[A\n",
      "Iteration:  51%|███████████████▊               | 60/118 [00:03<00:02, 19.41it/s]\u001b[A\n",
      "Iteration:  53%|████████████████▎              | 62/118 [00:03<00:02, 19.43it/s]\u001b[A\n",
      "Iteration:  54%|████████████████▊              | 64/118 [00:03<00:02, 19.43it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▎             | 66/118 [00:03<00:02, 19.43it/s]\u001b[A\n",
      "Iteration:  58%|█████████████████▊             | 68/118 [00:03<00:02, 19.42it/s]\u001b[A\n",
      "Iteration:  59%|██████████████████▍            | 70/118 [00:03<00:02, 19.41it/s]\u001b[A\n",
      "Iteration:  61%|██████████████████▉            | 72/118 [00:03<00:02, 19.42it/s]\u001b[A\n",
      "Iteration:  63%|███████████████████▍           | 74/118 [00:03<00:02, 19.44it/s]\u001b[A\n",
      "Iteration:  64%|███████████████████▉           | 76/118 [00:03<00:02, 19.45it/s]\u001b[A\n",
      "Iteration:  66%|████████████████████▍          | 78/118 [00:04<00:02, 19.45it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████          | 80/118 [00:04<00:01, 19.45it/s]\u001b[A\n",
      "Iteration:  69%|█████████████████████▌         | 82/118 [00:04<00:01, 19.46it/s]\u001b[A12/20/2023 22:25:25 - INFO - __main__ -   Loading features from cached file ./dataset/bert/classic/m6Am/cached_dev_m6Am_101_dnaprom\n",
      "12/20/2023 22:25:25 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/20/2023 22:25:25 - INFO - __main__ -     Num examples = 937\n",
      "12/20/2023 22:25:25 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                        | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|██████████▎                    | 10/30 [00:00<00:00, 93.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|████████████████████▋          | 20/30 [00:00<00:00, 94.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████| 30/30 [00:00<00:00, 94.77it/s]\u001b[A\u001b[A\n",
      "12/20/2023 22:25:25 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/20/2023 22:25:25 - INFO - __main__ -     acc = 0.8463180362860192\n",
      "12/20/2023 22:25:25 - INFO - __main__ -     auc = 0.8983221476510067\n",
      "12/20/2023 22:25:25 - INFO - __main__ -     f1 = 0.8458247634718223\n",
      "12/20/2023 22:25:25 - INFO - __main__ -     mcc = 0.7097022845995506\n",
      "12/20/2023 22:25:25 - INFO - __main__ -     precision = 0.859045909639061\n",
      "12/20/2023 22:25:25 - INFO - __main__ -     recall = 0.8507053828242706\n",
      "/root/.conda/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "{\"eval_acc\": 0.8463180362860192, \"eval_f1\": 0.8458247634718223, \"eval_mcc\": 0.7097022845995506, \"eval_auc\": 0.8983221476510067, \"eval_precision\": 0.859045909639061, \"eval_recall\": 0.8507053828242706, \"learning_rate\": 1.2994350282485876e-05, \"loss\": 0.01638774835591903, \"step\": 1500}\n",
      "12/20/2023 22:25:25 - INFO - transformers.configuration_utils -   Configuration saved in ./results/your_models/bert/classic/m6Am/checkpoint-1500/config.json\n",
      "12/20/2023 22:25:26 - INFO - transformers.modeling_utils -   Model weights saved in ./results/your_models/bert/classic/m6Am/checkpoint-1500/pytorch_model.bin\n",
      "12/20/2023 22:25:26 - INFO - __main__ -   Saving model checkpoint to ./results/your_models/bert/classic/m6Am/checkpoint-1500\n",
      "12/20/2023 22:25:26 - INFO - __main__ -   Saving optimizer and scheduler states to ./results/your_models/bert/classic/m6Am/checkpoint-1500\n",
      "\n",
      "Iteration:  71%|██████████████████████         | 84/118 [00:05<00:07,  4.30it/s]\u001b[A\n",
      "Iteration:  73%|██████████████████████▌        | 86/118 [00:05<00:05,  5.60it/s]\u001b[A\n",
      "Iteration:  75%|███████████████████████        | 88/118 [00:05<00:04,  7.12it/s]\u001b[A\n",
      "Iteration:  76%|███████████████████████▋       | 90/118 [00:05<00:03,  8.79it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▏      | 92/118 [00:05<00:02, 10.52it/s]\u001b[A\n",
      "Iteration:  80%|████████████████████████▋      | 94/118 [00:06<00:01, 12.20it/s]\u001b[A\n",
      "Iteration:  81%|█████████████████████████▏     | 96/118 [00:06<00:01, 13.74it/s]\u001b[A\n",
      "Iteration:  83%|█████████████████████████▋     | 98/118 [00:06<00:01, 15.07it/s]\u001b[A\n",
      "Iteration:  85%|█████████████████████████▍    | 100/118 [00:06<00:01, 16.17it/s]\u001b[A\n",
      "Iteration:  86%|█████████████████████████▉    | 102/118 [00:06<00:00, 17.04it/s]\u001b[A\n",
      "Iteration:  88%|██████████████████████████▍   | 104/118 [00:06<00:00, 17.69it/s]\u001b[A\n",
      "Iteration:  90%|██████████████████████████▉   | 106/118 [00:06<00:00, 18.19it/s]\u001b[A\n",
      "Iteration:  92%|███████████████████████████▍  | 108/118 [00:06<00:00, 18.57it/s]\u001b[A\n",
      "Iteration:  93%|███████████████████████████▉  | 110/118 [00:06<00:00, 18.83it/s]\u001b[A\n",
      "Iteration:  95%|████████████████████████████▍ | 112/118 [00:06<00:00, 18.97it/s]\u001b[A\n",
      "Iteration:  97%|████████████████████████████▉ | 114/118 [00:07<00:00, 19.11it/s]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████| 118/118 [00:07<00:00, 16.23it/s]\u001b[A\n",
      "Epoch:  52%|██████████████████▋                 | 13/25 [01:38<01:30,  7.57s/it]\n",
      "Iteration:   0%|                                        | 0/118 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▌                               | 2/118 [00:00<00:05, 19.49it/s]\u001b[A\n",
      "Iteration:   3%|█                               | 4/118 [00:00<00:05, 19.48it/s]\u001b[A\n",
      "Iteration:   5%|█▋                              | 6/118 [00:00<00:05, 19.45it/s]\u001b[A\n",
      "Iteration:   7%|██▏                             | 8/118 [00:00<00:05, 19.44it/s]\u001b[A\n",
      "Iteration:   8%|██▋                            | 10/118 [00:00<00:05, 19.45it/s]\u001b[A\n",
      "Iteration:  10%|███▏                           | 12/118 [00:00<00:05, 19.45it/s]\u001b[A\n",
      "Iteration:  12%|███▋                           | 14/118 [00:00<00:05, 19.47it/s]\u001b[A\n",
      "Iteration:  14%|████▏                          | 16/118 [00:00<00:05, 19.47it/s]\u001b[A\n",
      "Iteration:  15%|████▋                          | 18/118 [00:00<00:05, 19.47it/s]\u001b[A\n",
      "Iteration:  17%|█████▎                         | 20/118 [00:01<00:05, 19.47it/s]\u001b[A\n",
      "Iteration:  19%|█████▊                         | 22/118 [00:01<00:04, 19.48it/s]\u001b[A\n",
      "Iteration:  20%|██████▎                        | 24/118 [00:01<00:04, 19.48it/s]\u001b[A\n",
      "Iteration:  22%|██████▊                        | 26/118 [00:01<00:04, 19.48it/s]\u001b[A\n",
      "Iteration:  24%|███████▎                       | 28/118 [00:01<00:04, 19.47it/s]\u001b[A\n",
      "Iteration:  25%|███████▉                       | 30/118 [00:01<00:04, 19.48it/s]\u001b[A\n",
      "Iteration:  27%|████████▍                      | 32/118 [00:01<00:04, 19.47it/s]\u001b[A\n",
      "Iteration:  29%|████████▉                      | 34/118 [00:01<00:04, 19.41it/s]\u001b[A\n",
      "Iteration:  31%|█████████▍                     | 36/118 [00:01<00:04, 19.42it/s]\u001b[A\n",
      "Iteration:  32%|█████████▉                     | 38/118 [00:01<00:04, 19.45it/s]\u001b[A\n",
      "Iteration:  34%|██████████▌                    | 40/118 [00:02<00:04, 19.45it/s]\u001b[A\n",
      "Iteration:  36%|███████████                    | 42/118 [00:02<00:03, 19.46it/s]\u001b[A\n",
      "Iteration:  37%|███████████▌                   | 44/118 [00:02<00:03, 19.45it/s]\u001b[A\n",
      "Iteration:  39%|████████████                   | 46/118 [00:02<00:03, 19.43it/s]\u001b[A\n",
      "Iteration:  41%|████████████▌                  | 48/118 [00:02<00:03, 19.44it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▏                 | 50/118 [00:02<00:03, 19.45it/s]\u001b[A\n",
      "Iteration:  44%|█████████████▋                 | 52/118 [00:02<00:03, 19.46it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▏                | 54/118 [00:02<00:03, 19.47it/s]\u001b[A\n",
      "Iteration:  47%|██████████████▋                | 56/118 [00:02<00:03, 19.48it/s]\u001b[A\n",
      "Iteration:  49%|███████████████▏               | 58/118 [00:02<00:03, 19.48it/s]\u001b[A\n",
      "Iteration:  51%|███████████████▊               | 60/118 [00:03<00:02, 19.48it/s]\u001b[A\n",
      "Iteration:  53%|████████████████▎              | 62/118 [00:03<00:02, 19.49it/s]\u001b[A\n",
      "Iteration:  54%|████████████████▊              | 64/118 [00:03<00:02, 19.49it/s]\u001b[A12/20/2023 22:25:31 - INFO - __main__ -   Loading features from cached file ./dataset/bert/classic/m6Am/cached_dev_m6Am_101_dnaprom\n",
      "12/20/2023 22:25:31 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/20/2023 22:25:31 - INFO - __main__ -     Num examples = 937\n",
      "12/20/2023 22:25:31 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                        | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|██████████▎                    | 10/30 [00:00<00:00, 93.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|████████████████████▋          | 20/30 [00:00<00:00, 94.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████| 30/30 [00:00<00:00, 94.71it/s]\u001b[A\u001b[A\n",
      "12/20/2023 22:25:32 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/20/2023 22:25:32 - INFO - __main__ -     acc = 0.8377801494130203\n",
      "12/20/2023 22:25:32 - INFO - __main__ -     auc = 0.896486782632516\n",
      "12/20/2023 22:25:32 - INFO - __main__ -     f1 = 0.8369464670055411\n",
      "12/20/2023 22:25:32 - INFO - __main__ -     mcc = 0.6975086180750059\n",
      "12/20/2023 22:25:32 - INFO - __main__ -     precision = 0.8547741661154682\n",
      "12/20/2023 22:25:32 - INFO - __main__ -     recall = 0.8428365977263388\n",
      "/root/.conda/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "{\"eval_acc\": 0.8377801494130203, \"eval_f1\": 0.8369464670055411, \"eval_mcc\": 0.6975086180750059, \"eval_auc\": 0.896486782632516, \"eval_precision\": 0.8547741661154682, \"eval_recall\": 0.8428365977263388, \"learning_rate\": 1.1864406779661018e-05, \"loss\": 0.014248675782291685, \"step\": 1600}\n",
      "12/20/2023 22:25:32 - INFO - transformers.configuration_utils -   Configuration saved in ./results/your_models/bert/classic/m6Am/checkpoint-1600/config.json\n",
      "12/20/2023 22:25:32 - INFO - transformers.modeling_utils -   Model weights saved in ./results/your_models/bert/classic/m6Am/checkpoint-1600/pytorch_model.bin\n",
      "12/20/2023 22:25:32 - INFO - __main__ -   Saving model checkpoint to ./results/your_models/bert/classic/m6Am/checkpoint-1600\n",
      "12/20/2023 22:25:32 - INFO - __main__ -   Saving optimizer and scheduler states to ./results/your_models/bert/classic/m6Am/checkpoint-1600\n",
      "\n",
      "Iteration:  56%|█████████████████▎             | 66/118 [00:04<00:12,  4.32it/s]\u001b[A\n",
      "Iteration:  58%|█████████████████▊             | 68/118 [00:04<00:08,  5.62it/s]\u001b[A\n",
      "Iteration:  59%|██████████████████▍            | 70/118 [00:04<00:06,  7.14it/s]\u001b[A\n",
      "Iteration:  61%|██████████████████▉            | 72/118 [00:04<00:05,  8.82it/s]\u001b[A\n",
      "Iteration:  63%|███████████████████▍           | 74/118 [00:05<00:04, 10.55it/s]\u001b[A\n",
      "Iteration:  64%|███████████████████▉           | 76/118 [00:05<00:03, 12.24it/s]\u001b[A\n",
      "Iteration:  66%|████████████████████▍          | 78/118 [00:05<00:02, 13.75it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████          | 80/118 [00:05<00:02, 15.08it/s]\u001b[A\n",
      "Iteration:  69%|█████████████████████▌         | 82/118 [00:05<00:02, 16.17it/s]\u001b[A\n",
      "Iteration:  71%|██████████████████████         | 84/118 [00:05<00:01, 17.05it/s]\u001b[A\n",
      "Iteration:  73%|██████████████████████▌        | 86/118 [00:05<00:01, 17.70it/s]\u001b[A\n",
      "Iteration:  75%|███████████████████████        | 88/118 [00:05<00:01, 18.16it/s]\u001b[A\n",
      "Iteration:  76%|███████████████████████▋       | 90/118 [00:05<00:01, 18.54it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▏      | 92/118 [00:05<00:01, 18.82it/s]\u001b[A\n",
      "Iteration:  80%|████████████████████████▋      | 94/118 [00:06<00:01, 19.00it/s]\u001b[A\n",
      "Iteration:  81%|█████████████████████████▏     | 96/118 [00:06<00:01, 19.15it/s]\u001b[A\n",
      "Iteration:  83%|█████████████████████████▋     | 98/118 [00:06<00:01, 19.24it/s]\u001b[A\n",
      "Iteration:  85%|█████████████████████████▍    | 100/118 [00:06<00:00, 19.31it/s]\u001b[A\n",
      "Iteration:  86%|█████████████████████████▉    | 102/118 [00:06<00:00, 19.33it/s]\u001b[A\n",
      "Iteration:  88%|██████████████████████████▍   | 104/118 [00:06<00:00, 19.36it/s]\u001b[A\n",
      "Iteration:  90%|██████████████████████████▉   | 106/118 [00:06<00:00, 19.39it/s]\u001b[A\n",
      "Iteration:  92%|███████████████████████████▍  | 108/118 [00:06<00:00, 19.42it/s]\u001b[A\n",
      "Iteration:  93%|███████████████████████████▉  | 110/118 [00:06<00:00, 19.43it/s]\u001b[A\n",
      "Iteration:  95%|████████████████████████████▍ | 112/118 [00:06<00:00, 19.44it/s]\u001b[A\n",
      "Iteration:  97%|████████████████████████████▉ | 114/118 [00:07<00:00, 19.46it/s]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████| 118/118 [00:07<00:00, 16.25it/s]\u001b[A\n",
      "Epoch:  56%|████████████████████▏               | 14/25 [01:45<01:22,  7.48s/it]\n",
      "Iteration:   0%|                                        | 0/118 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▌                               | 2/118 [00:00<00:05, 19.48it/s]\u001b[A\n",
      "Iteration:   3%|█                               | 4/118 [00:00<00:05, 19.48it/s]\u001b[A\n",
      "Iteration:   5%|█▋                              | 6/118 [00:00<00:05, 19.49it/s]\u001b[A\n",
      "Iteration:   7%|██▏                             | 8/118 [00:00<00:05, 19.38it/s]\u001b[A\n",
      "Iteration:   8%|██▋                            | 10/118 [00:00<00:05, 19.38it/s]\u001b[A\n",
      "Iteration:  10%|███▏                           | 12/118 [00:00<00:05, 19.41it/s]\u001b[A\n",
      "Iteration:  12%|███▋                           | 14/118 [00:00<00:05, 19.43it/s]\u001b[A\n",
      "Iteration:  14%|████▏                          | 16/118 [00:00<00:05, 19.43it/s]\u001b[A\n",
      "Iteration:  15%|████▋                          | 18/118 [00:00<00:05, 19.44it/s]\u001b[A\n",
      "Iteration:  17%|█████▎                         | 20/118 [00:01<00:05, 19.45it/s]\u001b[A\n",
      "Iteration:  19%|█████▊                         | 22/118 [00:01<00:04, 19.45it/s]\u001b[A\n",
      "Iteration:  20%|██████▎                        | 24/118 [00:01<00:04, 19.42it/s]\u001b[A\n",
      "Iteration:  22%|██████▊                        | 26/118 [00:01<00:04, 19.43it/s]\u001b[A\n",
      "Iteration:  24%|███████▎                       | 28/118 [00:01<00:04, 19.43it/s]\u001b[A\n",
      "Iteration:  25%|███████▉                       | 30/118 [00:01<00:04, 19.44it/s]\u001b[A\n",
      "Iteration:  27%|████████▍                      | 32/118 [00:01<00:04, 19.44it/s]\u001b[A\n",
      "Iteration:  29%|████████▉                      | 34/118 [00:01<00:04, 19.45it/s]\u001b[A\n",
      "Iteration:  31%|█████████▍                     | 36/118 [00:01<00:04, 19.45it/s]\u001b[A\n",
      "Iteration:  32%|█████████▉                     | 38/118 [00:01<00:04, 19.45it/s]\u001b[A\n",
      "Iteration:  34%|██████████▌                    | 40/118 [00:02<00:04, 19.46it/s]\u001b[A\n",
      "Iteration:  36%|███████████                    | 42/118 [00:02<00:03, 19.47it/s]\u001b[A\n",
      "Iteration:  37%|███████████▌                   | 44/118 [00:02<00:03, 19.48it/s]\u001b[A\n",
      "Iteration:  39%|████████████                   | 46/118 [00:02<00:03, 19.48it/s]\u001b[A12/20/2023 22:25:38 - INFO - __main__ -   Loading features from cached file ./dataset/bert/classic/m6Am/cached_dev_m6Am_101_dnaprom\n",
      "12/20/2023 22:25:38 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/20/2023 22:25:38 - INFO - __main__ -     Num examples = 937\n",
      "12/20/2023 22:25:38 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                        | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|██████████▎                    | 10/30 [00:00<00:00, 93.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|████████████████████▋          | 20/30 [00:00<00:00, 94.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████| 30/30 [00:00<00:00, 94.69it/s]\u001b[A\u001b[A\n",
      "12/20/2023 22:25:38 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/20/2023 22:25:38 - INFO - __main__ -     acc = 0.8377801494130203\n",
      "12/20/2023 22:25:38 - INFO - __main__ -     auc = 0.8932178240423687\n",
      "12/20/2023 22:25:38 - INFO - __main__ -     f1 = 0.8369464670055411\n",
      "12/20/2023 22:25:38 - INFO - __main__ -     mcc = 0.6975086180750059\n",
      "12/20/2023 22:25:38 - INFO - __main__ -     precision = 0.8547741661154682\n",
      "12/20/2023 22:25:38 - INFO - __main__ -     recall = 0.8428365977263388\n",
      "/root/.conda/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "{\"eval_acc\": 0.8377801494130203, \"eval_f1\": 0.8369464670055411, \"eval_mcc\": 0.6975086180750059, \"eval_auc\": 0.8932178240423687, \"eval_precision\": 0.8547741661154682, \"eval_recall\": 0.8428365977263388, \"learning_rate\": 1.0734463276836158e-05, \"loss\": 0.012398026749724522, \"step\": 1700}\n",
      "12/20/2023 22:25:38 - INFO - transformers.configuration_utils -   Configuration saved in ./results/your_models/bert/classic/m6Am/checkpoint-1700/config.json\n",
      "12/20/2023 22:25:38 - INFO - transformers.modeling_utils -   Model weights saved in ./results/your_models/bert/classic/m6Am/checkpoint-1700/pytorch_model.bin\n",
      "12/20/2023 22:25:38 - INFO - __main__ -   Saving model checkpoint to ./results/your_models/bert/classic/m6Am/checkpoint-1700\n",
      "12/20/2023 22:25:39 - INFO - __main__ -   Saving optimizer and scheduler states to ./results/your_models/bert/classic/m6Am/checkpoint-1700\n",
      "\n",
      "Iteration:  41%|████████████▌                  | 48/118 [00:03<00:16,  4.30it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▏                 | 50/118 [00:03<00:12,  5.59it/s]\u001b[A\n",
      "Iteration:  44%|█████████████▋                 | 52/118 [00:03<00:09,  7.11it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▏                | 54/118 [00:03<00:07,  8.79it/s]\u001b[A\n",
      "Iteration:  47%|██████████████▋                | 56/118 [00:04<00:05, 10.52it/s]\u001b[A\n",
      "Iteration:  49%|███████████████▏               | 58/118 [00:04<00:04, 12.20it/s]\u001b[A\n",
      "Iteration:  51%|███████████████▊               | 60/118 [00:04<00:04, 13.75it/s]\u001b[A\n",
      "Iteration:  53%|████████████████▎              | 62/118 [00:04<00:03, 15.06it/s]\u001b[A\n",
      "Iteration:  54%|████████████████▊              | 64/118 [00:04<00:03, 16.14it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▎             | 66/118 [00:04<00:03, 17.02it/s]\u001b[A\n",
      "Iteration:  58%|█████████████████▊             | 68/118 [00:04<00:02, 17.69it/s]\u001b[A\n",
      "Iteration:  59%|██████████████████▍            | 70/118 [00:04<00:02, 18.18it/s]\u001b[A\n",
      "Iteration:  61%|██████████████████▉            | 72/118 [00:04<00:02, 18.54it/s]\u001b[A\n",
      "Iteration:  63%|███████████████████▍           | 74/118 [00:05<00:02, 18.81it/s]\u001b[A\n",
      "Iteration:  64%|███████████████████▉           | 76/118 [00:05<00:02, 19.01it/s]\u001b[A\n",
      "Iteration:  66%|████████████████████▍          | 78/118 [00:05<00:02, 19.15it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████          | 80/118 [00:05<00:01, 19.22it/s]\u001b[A\n",
      "Iteration:  69%|█████████████████████▌         | 82/118 [00:05<00:01, 19.28it/s]\u001b[A\n",
      "Iteration:  71%|██████████████████████         | 84/118 [00:05<00:01, 19.34it/s]\u001b[A\n",
      "Iteration:  73%|██████████████████████▌        | 86/118 [00:05<00:01, 19.38it/s]\u001b[A\n",
      "Iteration:  75%|███████████████████████        | 88/118 [00:05<00:01, 19.40it/s]\u001b[A\n",
      "Iteration:  76%|███████████████████████▋       | 90/118 [00:05<00:01, 19.42it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▏      | 92/118 [00:05<00:01, 19.44it/s]\u001b[A\n",
      "Iteration:  80%|████████████████████████▋      | 94/118 [00:06<00:01, 19.33it/s]\u001b[A\n",
      "Iteration:  81%|█████████████████████████▏     | 96/118 [00:06<00:01, 19.17it/s]\u001b[A\n",
      "Iteration:  83%|█████████████████████████▋     | 98/118 [00:06<00:01, 19.24it/s]\u001b[A\n",
      "Iteration:  85%|█████████████████████████▍    | 100/118 [00:06<00:00, 19.31it/s]\u001b[A\n",
      "Iteration:  86%|█████████████████████████▉    | 102/118 [00:06<00:00, 19.31it/s]\u001b[A\n",
      "Iteration:  88%|██████████████████████████▍   | 104/118 [00:06<00:00, 19.36it/s]\u001b[A\n",
      "Iteration:  90%|██████████████████████████▉   | 106/118 [00:06<00:00, 19.39it/s]\u001b[A\n",
      "Iteration:  92%|███████████████████████████▍  | 108/118 [00:06<00:00, 19.41it/s]\u001b[A\n",
      "Iteration:  93%|███████████████████████████▉  | 110/118 [00:06<00:00, 19.43it/s]\u001b[A\n",
      "Iteration:  95%|████████████████████████████▍ | 112/118 [00:06<00:00, 19.44it/s]\u001b[A\n",
      "Iteration:  97%|████████████████████████████▉ | 114/118 [00:07<00:00, 19.45it/s]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████| 118/118 [00:07<00:00, 16.09it/s]\u001b[A\n",
      "Epoch:  60%|█████████████████████▌              | 15/25 [01:52<01:14,  7.43s/it]\n",
      "Iteration:   0%|                                        | 0/118 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▌                               | 2/118 [00:00<00:05, 19.38it/s]\u001b[A\n",
      "Iteration:   3%|█                               | 4/118 [00:00<00:05, 19.40it/s]\u001b[A\n",
      "Iteration:   5%|█▋                              | 6/118 [00:00<00:05, 19.43it/s]\u001b[A\n",
      "Iteration:   7%|██▏                             | 8/118 [00:00<00:05, 19.44it/s]\u001b[A\n",
      "Iteration:   8%|██▋                            | 10/118 [00:00<00:05, 19.45it/s]\u001b[A\n",
      "Iteration:  10%|███▏                           | 12/118 [00:00<00:05, 19.46it/s]\u001b[A\n",
      "Iteration:  12%|███▋                           | 14/118 [00:00<00:05, 19.45it/s]\u001b[A\n",
      "Iteration:  14%|████▏                          | 16/118 [00:00<00:05, 19.46it/s]\u001b[A\n",
      "Iteration:  15%|████▋                          | 18/118 [00:00<00:05, 19.47it/s]\u001b[A\n",
      "Iteration:  17%|█████▎                         | 20/118 [00:01<00:05, 19.47it/s]\u001b[A\n",
      "Iteration:  19%|█████▊                         | 22/118 [00:01<00:04, 19.42it/s]\u001b[A\n",
      "Iteration:  20%|██████▎                        | 24/118 [00:01<00:04, 19.44it/s]\u001b[A\n",
      "Iteration:  22%|██████▊                        | 26/118 [00:01<00:04, 19.45it/s]\u001b[A\n",
      "Iteration:  24%|███████▎                       | 28/118 [00:01<00:04, 19.45it/s]\u001b[A12/20/2023 22:25:44 - INFO - __main__ -   Loading features from cached file ./dataset/bert/classic/m6Am/cached_dev_m6Am_101_dnaprom\n",
      "12/20/2023 22:25:44 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/20/2023 22:25:44 - INFO - __main__ -     Num examples = 937\n",
      "12/20/2023 22:25:44 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                        | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|██████████▎                    | 10/30 [00:00<00:00, 93.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|████████████████████▋          | 20/30 [00:00<00:00, 94.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████| 30/30 [00:00<00:00, 94.52it/s]\u001b[A\u001b[A\n",
      "12/20/2023 22:25:44 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/20/2023 22:25:44 - INFO - __main__ -     acc = 0.8431163287086446\n",
      "12/20/2023 22:25:44 - INFO - __main__ -     auc = 0.8923549285486008\n",
      "12/20/2023 22:25:44 - INFO - __main__ -     f1 = 0.8424703986532003\n",
      "12/20/2023 22:25:44 - INFO - __main__ -     mcc = 0.705708079797261\n",
      "12/20/2023 22:25:44 - INFO - __main__ -     precision = 0.8579398637538173\n",
      "12/20/2023 22:25:44 - INFO - __main__ -     recall = 0.8478404784732685\n",
      "/root/.conda/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "{\"eval_acc\": 0.8431163287086446, \"eval_f1\": 0.8424703986532003, \"eval_mcc\": 0.705708079797261, \"eval_auc\": 0.8923549285486008, \"eval_precision\": 0.8579398637538173, \"eval_recall\": 0.8478404784732685, \"learning_rate\": 9.6045197740113e-06, \"loss\": 0.010096673914813437, \"step\": 1800}\n",
      "12/20/2023 22:25:44 - INFO - transformers.configuration_utils -   Configuration saved in ./results/your_models/bert/classic/m6Am/checkpoint-1800/config.json\n",
      "12/20/2023 22:25:45 - INFO - transformers.modeling_utils -   Model weights saved in ./results/your_models/bert/classic/m6Am/checkpoint-1800/pytorch_model.bin\n",
      "12/20/2023 22:25:45 - INFO - __main__ -   Saving model checkpoint to ./results/your_models/bert/classic/m6Am/checkpoint-1800\n",
      "12/20/2023 22:25:45 - INFO - __main__ -   Saving optimizer and scheduler states to ./results/your_models/bert/classic/m6Am/checkpoint-1800\n",
      "\n",
      "Iteration:  25%|███████▉                       | 30/118 [00:02<00:20,  4.29it/s]\u001b[A\n",
      "Iteration:  27%|████████▍                      | 32/118 [00:02<00:15,  5.60it/s]\u001b[A\n",
      "Iteration:  29%|████████▉                      | 34/118 [00:02<00:11,  7.12it/s]\u001b[A\n",
      "Iteration:  31%|█████████▍                     | 36/118 [00:03<00:09,  8.80it/s]\u001b[A\n",
      "Iteration:  32%|█████████▉                     | 38/118 [00:03<00:07, 10.52it/s]\u001b[A\n",
      "Iteration:  34%|██████████▌                    | 40/118 [00:03<00:06, 12.21it/s]\u001b[A\n",
      "Iteration:  36%|███████████                    | 42/118 [00:03<00:05, 13.75it/s]\u001b[A\n",
      "Iteration:  37%|███████████▌                   | 44/118 [00:03<00:04, 15.07it/s]\u001b[A\n",
      "Iteration:  39%|████████████                   | 46/118 [00:03<00:04, 16.17it/s]\u001b[A\n",
      "Iteration:  41%|████████████▌                  | 48/118 [00:03<00:04, 17.04it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▏                 | 50/118 [00:03<00:03, 17.70it/s]\u001b[A\n",
      "Iteration:  44%|█████████████▋                 | 52/118 [00:03<00:03, 18.20it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▏                | 54/118 [00:03<00:03, 18.57it/s]\u001b[A\n",
      "Iteration:  47%|██████████████▋                | 56/118 [00:04<00:03, 18.82it/s]\u001b[A\n",
      "Iteration:  49%|███████████████▏               | 58/118 [00:04<00:03, 18.98it/s]\u001b[A\n",
      "Iteration:  51%|███████████████▊               | 60/118 [00:04<00:03, 19.12it/s]\u001b[A\n",
      "Iteration:  53%|████████████████▎              | 62/118 [00:04<00:02, 19.22it/s]\u001b[A\n",
      "Iteration:  54%|████████████████▊              | 64/118 [00:04<00:02, 19.29it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▎             | 66/118 [00:04<00:02, 19.35it/s]\u001b[A\n",
      "Iteration:  58%|█████████████████▊             | 68/118 [00:04<00:02, 19.39it/s]\u001b[A\n",
      "Iteration:  59%|██████████████████▍            | 70/118 [00:04<00:02, 19.40it/s]\u001b[A\n",
      "Iteration:  61%|██████████████████▉            | 72/118 [00:04<00:02, 19.43it/s]\u001b[A\n",
      "Iteration:  63%|███████████████████▍           | 74/118 [00:05<00:02, 19.45it/s]\u001b[A\n",
      "Iteration:  64%|███████████████████▉           | 76/118 [00:05<00:02, 19.39it/s]\u001b[A\n",
      "Iteration:  66%|████████████████████▍          | 78/118 [00:05<00:02, 19.37it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████          | 80/118 [00:05<00:01, 19.38it/s]\u001b[A\n",
      "Iteration:  69%|█████████████████████▌         | 82/118 [00:05<00:01, 19.39it/s]\u001b[A\n",
      "Iteration:  71%|██████████████████████         | 84/118 [00:05<00:01, 19.38it/s]\u001b[A\n",
      "Iteration:  73%|██████████████████████▌        | 86/118 [00:05<00:01, 19.40it/s]\u001b[A\n",
      "Iteration:  75%|███████████████████████        | 88/118 [00:05<00:01, 19.41it/s]\u001b[A\n",
      "Iteration:  76%|███████████████████████▋       | 90/118 [00:05<00:01, 19.43it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▏      | 92/118 [00:05<00:01, 19.44it/s]\u001b[A\n",
      "Iteration:  80%|████████████████████████▋      | 94/118 [00:06<00:01, 19.45it/s]\u001b[A\n",
      "Iteration:  81%|█████████████████████████▏     | 96/118 [00:06<00:01, 19.44it/s]\u001b[A\n",
      "Iteration:  83%|█████████████████████████▋     | 98/118 [00:06<00:01, 19.42it/s]\u001b[A\n",
      "Iteration:  85%|█████████████████████████▍    | 100/118 [00:06<00:00, 19.43it/s]\u001b[A\n",
      "Iteration:  86%|█████████████████████████▉    | 102/118 [00:06<00:00, 19.45it/s]\u001b[A\n",
      "Iteration:  88%|██████████████████████████▍   | 104/118 [00:06<00:00, 19.44it/s]\u001b[A\n",
      "Iteration:  90%|██████████████████████████▉   | 106/118 [00:06<00:00, 19.44it/s]\u001b[A\n",
      "Iteration:  92%|███████████████████████████▍  | 108/118 [00:06<00:00, 19.45it/s]\u001b[A\n",
      "Iteration:  93%|███████████████████████████▉  | 110/118 [00:06<00:00, 19.45it/s]\u001b[A\n",
      "Iteration:  95%|████████████████████████████▍ | 112/118 [00:06<00:00, 19.45it/s]\u001b[A\n",
      "Iteration:  97%|████████████████████████████▉ | 114/118 [00:07<00:00, 19.43it/s]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████| 118/118 [00:07<00:00, 16.23it/s]\u001b[A\n",
      "Epoch:  64%|███████████████████████             | 16/25 [02:00<01:06,  7.39s/it]\n",
      "Iteration:   0%|                                        | 0/118 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▌                               | 2/118 [00:00<00:05, 19.41it/s]\u001b[A\n",
      "Iteration:   3%|█                               | 4/118 [00:00<00:05, 19.44it/s]\u001b[A\n",
      "Iteration:   5%|█▋                              | 6/118 [00:00<00:05, 19.44it/s]\u001b[A\n",
      "Iteration:   7%|██▏                             | 8/118 [00:00<00:05, 19.38it/s]\u001b[A\n",
      "Iteration:   8%|██▋                            | 10/118 [00:00<00:05, 19.41it/s]\u001b[A12/20/2023 22:25:50 - INFO - __main__ -   Loading features from cached file ./dataset/bert/classic/m6Am/cached_dev_m6Am_101_dnaprom\n",
      "12/20/2023 22:25:50 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/20/2023 22:25:50 - INFO - __main__ -     Num examples = 937\n",
      "12/20/2023 22:25:50 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                        | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|██████████▎                    | 10/30 [00:00<00:00, 93.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|████████████████████▋          | 20/30 [00:00<00:00, 94.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████| 30/30 [00:00<00:00, 94.34it/s]\u001b[A\u001b[A\n",
      "12/20/2023 22:25:51 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/20/2023 22:25:51 - INFO - __main__ -     acc = 0.8505869797225186\n",
      "12/20/2023 22:25:51 - INFO - __main__ -     auc = 0.8980756060813587\n",
      "12/20/2023 22:25:51 - INFO - __main__ -     f1 = 0.8502415679541158\n",
      "12/20/2023 22:25:51 - INFO - __main__ -     mcc = 0.7156805506392586\n",
      "12/20/2023 22:25:51 - INFO - __main__ -     precision = 0.8611196354675221\n",
      "12/20/2023 22:25:51 - INFO - __main__ -     recall = 0.854590695338538\n",
      "/root/.conda/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "{\"eval_acc\": 0.8505869797225186, \"eval_f1\": 0.8502415679541158, \"eval_mcc\": 0.7156805506392586, \"eval_auc\": 0.8980756060813587, \"eval_precision\": 0.8611196354675221, \"eval_recall\": 0.854590695338538, \"learning_rate\": 8.47457627118644e-06, \"loss\": 0.008991491934139048, \"step\": 1900}\n",
      "12/20/2023 22:25:51 - INFO - transformers.configuration_utils -   Configuration saved in ./results/your_models/bert/classic/m6Am/checkpoint-1900/config.json\n",
      "12/20/2023 22:25:51 - INFO - transformers.modeling_utils -   Model weights saved in ./results/your_models/bert/classic/m6Am/checkpoint-1900/pytorch_model.bin\n",
      "12/20/2023 22:25:51 - INFO - __main__ -   Saving model checkpoint to ./results/your_models/bert/classic/m6Am/checkpoint-1900\n",
      "12/20/2023 22:25:52 - INFO - __main__ -   Saving optimizer and scheduler states to ./results/your_models/bert/classic/m6Am/checkpoint-1900\n",
      "\n",
      "Iteration:  10%|███▏                           | 12/118 [00:01<00:27,  3.89it/s]\u001b[A\n",
      "Iteration:  12%|███▋                           | 14/118 [00:01<00:19,  5.26it/s]\u001b[A\n",
      "Iteration:  14%|████▏                          | 16/118 [00:02<00:14,  6.85it/s]\u001b[A\n",
      "Iteration:  15%|████▋                          | 18/118 [00:02<00:11,  8.59it/s]\u001b[A\n",
      "Iteration:  17%|█████▎                         | 20/118 [00:02<00:09, 10.38it/s]\u001b[A\n",
      "Iteration:  19%|█████▊                         | 22/118 [00:02<00:07, 12.11it/s]\u001b[A\n",
      "Iteration:  20%|██████▎                        | 24/118 [00:02<00:06, 13.67it/s]\u001b[A\n",
      "Iteration:  22%|██████▊                        | 26/118 [00:02<00:06, 15.02it/s]\u001b[A\n",
      "Iteration:  24%|███████▎                       | 28/118 [00:02<00:05, 16.13it/s]\u001b[A\n",
      "Iteration:  25%|███████▉                       | 30/118 [00:02<00:05, 17.00it/s]\u001b[A\n",
      "Iteration:  27%|████████▍                      | 32/118 [00:02<00:04, 17.67it/s]\u001b[A\n",
      "Iteration:  29%|████████▉                      | 34/118 [00:02<00:04, 18.16it/s]\u001b[A\n",
      "Iteration:  31%|█████████▍                     | 36/118 [00:03<00:04, 18.50it/s]\u001b[A\n",
      "Iteration:  32%|█████████▉                     | 38/118 [00:03<00:04, 18.77it/s]\u001b[A\n",
      "Iteration:  34%|██████████▌                    | 40/118 [00:03<00:04, 18.97it/s]\u001b[A\n",
      "Iteration:  36%|███████████                    | 42/118 [00:03<00:03, 19.13it/s]\u001b[A\n",
      "Iteration:  37%|███████████▌                   | 44/118 [00:03<00:03, 19.22it/s]\u001b[A\n",
      "Iteration:  39%|████████████                   | 46/118 [00:03<00:03, 19.30it/s]\u001b[A\n",
      "Iteration:  41%|████████████▌                  | 48/118 [00:03<00:03, 19.34it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▏                 | 50/118 [00:03<00:03, 19.35it/s]\u001b[A\n",
      "Iteration:  44%|█████████████▋                 | 52/118 [00:03<00:03, 19.32it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▏                | 54/118 [00:03<00:03, 19.33it/s]\u001b[A\n",
      "Iteration:  47%|██████████████▋                | 56/118 [00:04<00:03, 19.36it/s]\u001b[A\n",
      "Iteration:  49%|███████████████▏               | 58/118 [00:04<00:03, 19.38it/s]\u001b[A\n",
      "Iteration:  51%|███████████████▊               | 60/118 [00:04<00:02, 19.40it/s]\u001b[A\n",
      "Iteration:  53%|████████████████▎              | 62/118 [00:04<00:02, 19.39it/s]\u001b[A\n",
      "Iteration:  54%|████████████████▊              | 64/118 [00:04<00:02, 19.40it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▎             | 66/118 [00:04<00:02, 19.41it/s]\u001b[A\n",
      "Iteration:  58%|█████████████████▊             | 68/118 [00:04<00:02, 19.43it/s]\u001b[A\n",
      "Iteration:  59%|██████████████████▍            | 70/118 [00:04<00:02, 19.42it/s]\u001b[A\n",
      "Iteration:  61%|██████████████████▉            | 72/118 [00:04<00:02, 19.43it/s]\u001b[A\n",
      "Iteration:  63%|███████████████████▍           | 74/118 [00:05<00:02, 19.42it/s]\u001b[A\n",
      "Iteration:  64%|███████████████████▉           | 76/118 [00:05<00:02, 19.37it/s]\u001b[A\n",
      "Iteration:  66%|████████████████████▍          | 78/118 [00:05<00:02, 19.40it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████          | 80/118 [00:05<00:01, 19.41it/s]\u001b[A\n",
      "Iteration:  69%|█████████████████████▌         | 82/118 [00:05<00:01, 19.41it/s]\u001b[A\n",
      "Iteration:  71%|██████████████████████         | 84/118 [00:05<00:01, 19.21it/s]\u001b[A\n",
      "Iteration:  73%|██████████████████████▌        | 86/118 [00:05<00:01, 19.24it/s]\u001b[A\n",
      "Iteration:  75%|███████████████████████        | 88/118 [00:05<00:01, 19.31it/s]\u001b[A\n",
      "Iteration:  76%|███████████████████████▋       | 90/118 [00:05<00:01, 19.31it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▏      | 92/118 [00:05<00:01, 19.34it/s]\u001b[A\n",
      "Iteration:  80%|████████████████████████▋      | 94/118 [00:06<00:01, 19.38it/s]\u001b[A\n",
      "Iteration:  81%|█████████████████████████▏     | 96/118 [00:06<00:01, 19.41it/s]\u001b[A\n",
      "Iteration:  83%|█████████████████████████▋     | 98/118 [00:06<00:01, 19.43it/s]\u001b[A\n",
      "Iteration:  85%|█████████████████████████▍    | 100/118 [00:06<00:00, 19.44it/s]\u001b[A\n",
      "Iteration:  86%|█████████████████████████▉    | 102/118 [00:06<00:00, 19.45it/s]\u001b[A\n",
      "Iteration:  88%|██████████████████████████▍   | 104/118 [00:06<00:00, 19.45it/s]\u001b[A\n",
      "Iteration:  90%|██████████████████████████▉   | 106/118 [00:06<00:00, 19.45it/s]\u001b[A\n",
      "Iteration:  92%|███████████████████████████▍  | 108/118 [00:06<00:00, 19.46it/s]\u001b[A\n",
      "Iteration:  93%|███████████████████████████▉  | 110/118 [00:06<00:00, 19.46it/s]\u001b[A12/20/2023 22:25:57 - INFO - __main__ -   Loading features from cached file ./dataset/bert/classic/m6Am/cached_dev_m6Am_101_dnaprom\n",
      "12/20/2023 22:25:57 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/20/2023 22:25:57 - INFO - __main__ -     Num examples = 937\n",
      "12/20/2023 22:25:57 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                        | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|██████████▎                    | 10/30 [00:00<00:00, 92.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|████████████████████▋          | 20/30 [00:00<00:00, 93.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████| 30/30 [00:00<00:00, 94.05it/s]\u001b[A\u001b[A\n",
      "12/20/2023 22:25:57 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/20/2023 22:25:57 - INFO - __main__ -     acc = 0.8527214514407684\n",
      "12/20/2023 22:25:57 - INFO - __main__ -     auc = 0.8963703602246267\n",
      "12/20/2023 22:25:57 - INFO - __main__ -     f1 = 0.8523809741261998\n",
      "12/20/2023 22:25:57 - INFO - __main__ -     mcc = 0.7199977079278339\n",
      "12/20/2023 22:25:57 - INFO - __main__ -     precision = 0.8632979960013019\n",
      "12/20/2023 22:25:57 - INFO - __main__ -     recall = 0.8567296717344657\n",
      "/root/.conda/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "{\"eval_acc\": 0.8527214514407684, \"eval_f1\": 0.8523809741261998, \"eval_mcc\": 0.7199977079278339, \"eval_auc\": 0.8963703602246267, \"eval_precision\": 0.8632979960013019, \"eval_recall\": 0.8567296717344657, \"learning_rate\": 7.3446327683615825e-06, \"loss\": 0.008912162376363995, \"step\": 2000}\n",
      "12/20/2023 22:25:57 - INFO - transformers.configuration_utils -   Configuration saved in ./results/your_models/bert/classic/m6Am/checkpoint-2000/config.json\n",
      "12/20/2023 22:25:57 - INFO - transformers.modeling_utils -   Model weights saved in ./results/your_models/bert/classic/m6Am/checkpoint-2000/pytorch_model.bin\n",
      "12/20/2023 22:25:57 - INFO - __main__ -   Saving model checkpoint to ./results/your_models/bert/classic/m6Am/checkpoint-2000\n",
      "12/20/2023 22:25:58 - INFO - __main__ -   Saving optimizer and scheduler states to ./results/your_models/bert/classic/m6Am/checkpoint-2000\n",
      "\n",
      "Iteration:  95%|████████████████████████████▍ | 112/118 [00:08<00:01,  4.30it/s]\u001b[A\n",
      "Iteration:  97%|████████████████████████████▉ | 114/118 [00:08<00:00,  5.59it/s]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████| 118/118 [00:08<00:00, 13.89it/s]\u001b[A\n",
      "Epoch:  68%|████████████████████████▍           | 17/25 [02:08<01:01,  7.72s/it]\n",
      "Iteration:   0%|                                        | 0/118 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▌                               | 2/118 [00:00<00:05, 19.42it/s]\u001b[A\n",
      "Iteration:   3%|█                               | 4/118 [00:00<00:05, 19.43it/s]\u001b[A\n",
      "Iteration:   5%|█▋                              | 6/118 [00:00<00:05, 19.44it/s]\u001b[A\n",
      "Iteration:   7%|██▏                             | 8/118 [00:00<00:05, 19.43it/s]\u001b[A\n",
      "Iteration:   8%|██▋                            | 10/118 [00:00<00:05, 19.43it/s]\u001b[A\n",
      "Iteration:  10%|███▏                           | 12/118 [00:00<00:05, 19.42it/s]\u001b[A\n",
      "Iteration:  12%|███▋                           | 14/118 [00:00<00:05, 19.42it/s]\u001b[A\n",
      "Iteration:  14%|████▏                          | 16/118 [00:00<00:05, 19.43it/s]\u001b[A\n",
      "Iteration:  15%|████▋                          | 18/118 [00:00<00:05, 19.45it/s]\u001b[A\n",
      "Iteration:  17%|█████▎                         | 20/118 [00:01<00:05, 19.44it/s]\u001b[A\n",
      "Iteration:  19%|█████▊                         | 22/118 [00:01<00:04, 19.44it/s]\u001b[A\n",
      "Iteration:  20%|██████▎                        | 24/118 [00:01<00:04, 19.45it/s]\u001b[A\n",
      "Iteration:  22%|██████▊                        | 26/118 [00:01<00:04, 19.35it/s]\u001b[A\n",
      "Iteration:  24%|███████▎                       | 28/118 [00:01<00:04, 19.33it/s]\u001b[A\n",
      "Iteration:  25%|███████▉                       | 30/118 [00:01<00:04, 19.35it/s]\u001b[A\n",
      "Iteration:  27%|████████▍                      | 32/118 [00:01<00:04, 19.37it/s]\u001b[A\n",
      "Iteration:  29%|████████▉                      | 34/118 [00:01<00:04, 19.40it/s]\u001b[A\n",
      "Iteration:  31%|█████████▍                     | 36/118 [00:01<00:04, 19.41it/s]\u001b[A\n",
      "Iteration:  32%|█████████▉                     | 38/118 [00:01<00:04, 19.43it/s]\u001b[A\n",
      "Iteration:  34%|██████████▌                    | 40/118 [00:02<00:04, 19.44it/s]\u001b[A\n",
      "Iteration:  36%|███████████                    | 42/118 [00:02<00:03, 19.45it/s]\u001b[A\n",
      "Iteration:  37%|███████████▌                   | 44/118 [00:02<00:03, 19.46it/s]\u001b[A\n",
      "Iteration:  39%|████████████                   | 46/118 [00:02<00:03, 19.46it/s]\u001b[A\n",
      "Iteration:  41%|████████████▌                  | 48/118 [00:02<00:03, 19.47it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▏                 | 50/118 [00:02<00:03, 19.47it/s]\u001b[A\n",
      "Iteration:  44%|█████████████▋                 | 52/118 [00:02<00:03, 19.43it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▏                | 54/118 [00:02<00:03, 19.44it/s]\u001b[A\n",
      "Iteration:  47%|██████████████▋                | 56/118 [00:02<00:03, 19.45it/s]\u001b[A\n",
      "Iteration:  49%|███████████████▏               | 58/118 [00:02<00:03, 19.45it/s]\u001b[A\n",
      "Iteration:  51%|███████████████▊               | 60/118 [00:03<00:02, 19.46it/s]\u001b[A\n",
      "Iteration:  53%|████████████████▎              | 62/118 [00:03<00:02, 19.45it/s]\u001b[A\n",
      "Iteration:  54%|████████████████▊              | 64/118 [00:03<00:02, 19.43it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▎             | 66/118 [00:03<00:02, 19.39it/s]\u001b[A\n",
      "Iteration:  58%|█████████████████▊             | 68/118 [00:03<00:02, 19.41it/s]\u001b[A\n",
      "Iteration:  59%|██████████████████▍            | 70/118 [00:03<00:02, 19.42it/s]\u001b[A\n",
      "Iteration:  61%|██████████████████▉            | 72/118 [00:03<00:02, 19.42it/s]\u001b[A\n",
      "Iteration:  63%|███████████████████▍           | 74/118 [00:03<00:02, 19.43it/s]\u001b[A\n",
      "Iteration:  64%|███████████████████▉           | 76/118 [00:03<00:02, 19.44it/s]\u001b[A\n",
      "Iteration:  66%|████████████████████▍          | 78/118 [00:04<00:02, 19.47it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████          | 80/118 [00:04<00:01, 19.46it/s]\u001b[A\n",
      "Iteration:  69%|█████████████████████▌         | 82/118 [00:04<00:01, 19.46it/s]\u001b[A\n",
      "Iteration:  71%|██████████████████████         | 84/118 [00:04<00:01, 19.46it/s]\u001b[A\n",
      "Iteration:  73%|██████████████████████▌        | 86/118 [00:04<00:01, 19.46it/s]\u001b[A\n",
      "Iteration:  75%|███████████████████████        | 88/118 [00:04<00:01, 19.45it/s]\u001b[A\n",
      "Iteration:  76%|███████████████████████▋       | 90/118 [00:04<00:01, 19.44it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▏      | 92/118 [00:04<00:01, 19.25it/s]\u001b[A12/20/2023 22:26:03 - INFO - __main__ -   Loading features from cached file ./dataset/bert/classic/m6Am/cached_dev_m6Am_101_dnaprom\n",
      "12/20/2023 22:26:03 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/20/2023 22:26:03 - INFO - __main__ -     Num examples = 937\n",
      "12/20/2023 22:26:03 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                        | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|██████████▎                    | 10/30 [00:00<00:00, 93.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|████████████████████▋          | 20/30 [00:00<00:00, 94.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████| 30/30 [00:00<00:00, 94.58it/s]\u001b[A\u001b[A\n",
      "12/20/2023 22:26:03 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/20/2023 22:26:03 - INFO - __main__ -     acc = 0.8452508004268944\n",
      "12/20/2023 22:26:03 - INFO - __main__ -     auc = 0.8948020819065883\n",
      "12/20/2023 22:26:03 - INFO - __main__ -     f1 = 0.8447351189013694\n",
      "12/20/2023 22:26:03 - INFO - __main__ -     mcc = 0.7078855448680043\n",
      "12/20/2023 22:26:03 - INFO - __main__ -     precision = 0.8582524135608443\n",
      "12/20/2023 22:26:03 - INFO - __main__ -     recall = 0.8496849746610053\n",
      "/root/.conda/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "{\"eval_acc\": 0.8452508004268944, \"eval_f1\": 0.8447351189013694, \"eval_mcc\": 0.7078855448680043, \"eval_auc\": 0.8948020819065883, \"eval_precision\": 0.8582524135608443, \"eval_recall\": 0.8496849746610053, \"learning_rate\": 6.214689265536724e-06, \"loss\": 0.006924836949110613, \"step\": 2100}\n",
      "12/20/2023 22:26:03 - INFO - transformers.configuration_utils -   Configuration saved in ./results/your_models/bert/classic/m6Am/checkpoint-2100/config.json\n",
      "12/20/2023 22:26:04 - INFO - transformers.modeling_utils -   Model weights saved in ./results/your_models/bert/classic/m6Am/checkpoint-2100/pytorch_model.bin\n",
      "12/20/2023 22:26:04 - INFO - __main__ -   Saving model checkpoint to ./results/your_models/bert/classic/m6Am/checkpoint-2100\n",
      "12/20/2023 22:26:04 - INFO - __main__ -   Saving optimizer and scheduler states to ./results/your_models/bert/classic/m6Am/checkpoint-2100\n",
      "\n",
      "Iteration:  80%|████████████████████████▋      | 94/118 [00:06<00:05,  4.30it/s]\u001b[A\n",
      "Iteration:  81%|█████████████████████████▏     | 96/118 [00:06<00:03,  5.59it/s]\u001b[A\n",
      "Iteration:  83%|█████████████████████████▋     | 98/118 [00:06<00:02,  7.11it/s]\u001b[A\n",
      "Iteration:  85%|█████████████████████████▍    | 100/118 [00:06<00:02,  8.78it/s]\u001b[A\n",
      "Iteration:  86%|█████████████████████████▉    | 102/118 [00:06<00:01, 10.51it/s]\u001b[A\n",
      "Iteration:  88%|██████████████████████████▍   | 104/118 [00:06<00:01, 12.19it/s]\u001b[A\n",
      "Iteration:  90%|██████████████████████████▉   | 106/118 [00:06<00:00, 13.73it/s]\u001b[A\n",
      "Iteration:  92%|███████████████████████████▍  | 108/118 [00:06<00:00, 15.03it/s]\u001b[A\n",
      "Iteration:  93%|███████████████████████████▉  | 110/118 [00:06<00:00, 16.12it/s]\u001b[A\n",
      "Iteration:  95%|████████████████████████████▍ | 112/118 [00:06<00:00, 17.00it/s]\u001b[A\n",
      "Iteration:  97%|████████████████████████████▉ | 114/118 [00:07<00:00, 17.67it/s]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████| 118/118 [00:07<00:00, 16.21it/s]\u001b[A\n",
      "Epoch:  72%|█████████████████████████▉          | 18/25 [02:15<00:53,  7.59s/it]\n",
      "Iteration:   0%|                                        | 0/118 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▌                               | 2/118 [00:00<00:06, 19.24it/s]\u001b[A\n",
      "Iteration:   3%|█                               | 4/118 [00:00<00:05, 19.32it/s]\u001b[A\n",
      "Iteration:   5%|█▋                              | 6/118 [00:00<00:05, 19.37it/s]\u001b[A\n",
      "Iteration:   7%|██▏                             | 8/118 [00:00<00:05, 19.34it/s]\u001b[A\n",
      "Iteration:   8%|██▋                            | 10/118 [00:00<00:05, 19.37it/s]\u001b[A\n",
      "Iteration:  10%|███▏                           | 12/118 [00:00<00:05, 19.39it/s]\u001b[A\n",
      "Iteration:  12%|███▋                           | 14/118 [00:00<00:05, 19.41it/s]\u001b[A\n",
      "Iteration:  14%|████▏                          | 16/118 [00:00<00:05, 19.42it/s]\u001b[A\n",
      "Iteration:  15%|████▋                          | 18/118 [00:00<00:05, 19.37it/s]\u001b[A\n",
      "Iteration:  17%|█████▎                         | 20/118 [00:01<00:05, 19.37it/s]\u001b[A\n",
      "Iteration:  19%|█████▊                         | 22/118 [00:01<00:04, 19.40it/s]\u001b[A\n",
      "Iteration:  20%|██████▎                        | 24/118 [00:01<00:04, 19.42it/s]\u001b[A\n",
      "Iteration:  22%|██████▊                        | 26/118 [00:01<00:04, 19.43it/s]\u001b[A\n",
      "Iteration:  24%|███████▎                       | 28/118 [00:01<00:04, 19.45it/s]\u001b[A\n",
      "Iteration:  25%|███████▉                       | 30/118 [00:01<00:04, 19.42it/s]\u001b[A\n",
      "Iteration:  27%|████████▍                      | 32/118 [00:01<00:04, 19.42it/s]\u001b[A\n",
      "Iteration:  29%|████████▉                      | 34/118 [00:01<00:04, 19.43it/s]\u001b[A\n",
      "Iteration:  31%|█████████▍                     | 36/118 [00:01<00:04, 19.44it/s]\u001b[A\n",
      "Iteration:  32%|█████████▉                     | 38/118 [00:01<00:04, 19.44it/s]\u001b[A\n",
      "Iteration:  34%|██████████▌                    | 40/118 [00:02<00:04, 19.39it/s]\u001b[A\n",
      "Iteration:  36%|███████████                    | 42/118 [00:02<00:03, 19.40it/s]\u001b[A\n",
      "Iteration:  37%|███████████▌                   | 44/118 [00:02<00:03, 19.42it/s]\u001b[A\n",
      "Iteration:  39%|████████████                   | 46/118 [00:02<00:03, 19.43it/s]\u001b[A\n",
      "Iteration:  41%|████████████▌                  | 48/118 [00:02<00:03, 19.44it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▏                 | 50/118 [00:02<00:03, 19.45it/s]\u001b[A\n",
      "Iteration:  44%|█████████████▋                 | 52/118 [00:02<00:03, 19.45it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▏                | 54/118 [00:02<00:03, 19.46it/s]\u001b[A\n",
      "Iteration:  47%|██████████████▋                | 56/118 [00:02<00:03, 19.46it/s]\u001b[A\n",
      "Iteration:  49%|███████████████▏               | 58/118 [00:02<00:03, 19.46it/s]\u001b[A\n",
      "Iteration:  51%|███████████████▊               | 60/118 [00:03<00:02, 19.46it/s]\u001b[A\n",
      "Iteration:  53%|████████████████▎              | 62/118 [00:03<00:02, 19.47it/s]\u001b[A\n",
      "Iteration:  54%|████████████████▊              | 64/118 [00:03<00:02, 19.45it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▎             | 66/118 [00:03<00:02, 19.44it/s]\u001b[A\n",
      "Iteration:  58%|█████████████████▊             | 68/118 [00:03<00:02, 19.43it/s]\u001b[A\n",
      "Iteration:  59%|██████████████████▍            | 70/118 [00:03<00:02, 19.41it/s]\u001b[A\n",
      "Iteration:  61%|██████████████████▉            | 72/118 [00:03<00:02, 19.41it/s]\u001b[A\n",
      "Iteration:  63%|███████████████████▍           | 74/118 [00:03<00:02, 19.43it/s]\u001b[A12/20/2023 22:26:09 - INFO - __main__ -   Loading features from cached file ./dataset/bert/classic/m6Am/cached_dev_m6Am_101_dnaprom\n",
      "12/20/2023 22:26:09 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/20/2023 22:26:09 - INFO - __main__ -     Num examples = 937\n",
      "12/20/2023 22:26:09 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                        | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|██████████▎                    | 10/30 [00:00<00:00, 93.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|████████████████████▋          | 20/30 [00:00<00:00, 94.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████| 30/30 [00:00<00:00, 95.03it/s]\u001b[A\u001b[A\n",
      "12/20/2023 22:26:10 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/20/2023 22:26:10 - INFO - __main__ -     acc = 0.8431163287086446\n",
      "12/20/2023 22:26:10 - INFO - __main__ -     auc = 0.8939779938821166\n",
      "12/20/2023 22:26:10 - INFO - __main__ -     f1 = 0.8424703986532003\n",
      "12/20/2023 22:26:10 - INFO - __main__ -     mcc = 0.705708079797261\n",
      "12/20/2023 22:26:10 - INFO - __main__ -     precision = 0.8579398637538173\n",
      "12/20/2023 22:26:10 - INFO - __main__ -     recall = 0.8478404784732685\n",
      "/root/.conda/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "{\"eval_acc\": 0.8431163287086446, \"eval_f1\": 0.8424703986532003, \"eval_mcc\": 0.705708079797261, \"eval_auc\": 0.8939779938821166, \"eval_precision\": 0.8579398637538173, \"eval_recall\": 0.8478404784732685, \"learning_rate\": 5.084745762711865e-06, \"loss\": 0.004140748336649267, \"step\": 2200}\n",
      "12/20/2023 22:26:10 - INFO - transformers.configuration_utils -   Configuration saved in ./results/your_models/bert/classic/m6Am/checkpoint-2200/config.json\n",
      "12/20/2023 22:26:10 - INFO - transformers.modeling_utils -   Model weights saved in ./results/your_models/bert/classic/m6Am/checkpoint-2200/pytorch_model.bin\n",
      "12/20/2023 22:26:10 - INFO - __main__ -   Saving model checkpoint to ./results/your_models/bert/classic/m6Am/checkpoint-2200\n",
      "12/20/2023 22:26:11 - INFO - __main__ -   Saving optimizer and scheduler states to ./results/your_models/bert/classic/m6Am/checkpoint-2200\n",
      "\n",
      "Iteration:  64%|███████████████████▉           | 76/118 [00:05<00:09,  4.32it/s]\u001b[A\n",
      "Iteration:  66%|████████████████████▍          | 78/118 [00:05<00:07,  5.62it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████          | 80/118 [00:05<00:05,  7.15it/s]\u001b[A\n",
      "Iteration:  69%|█████████████████████▌         | 82/118 [00:05<00:04,  8.82it/s]\u001b[A\n",
      "Iteration:  71%|██████████████████████         | 84/118 [00:05<00:03, 10.55it/s]\u001b[A\n",
      "Iteration:  73%|██████████████████████▌        | 86/118 [00:05<00:02, 12.22it/s]\u001b[A\n",
      "Iteration:  75%|███████████████████████        | 88/118 [00:05<00:02, 13.75it/s]\u001b[A\n",
      "Iteration:  76%|███████████████████████▋       | 90/118 [00:05<00:01, 15.08it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▏      | 92/118 [00:05<00:01, 16.17it/s]\u001b[A\n",
      "Iteration:  80%|████████████████████████▋      | 94/118 [00:06<00:01, 17.01it/s]\u001b[A\n",
      "Iteration:  81%|█████████████████████████▏     | 96/118 [00:06<00:01, 17.66it/s]\u001b[A\n",
      "Iteration:  83%|█████████████████████████▋     | 98/118 [00:06<00:01, 18.16it/s]\u001b[A\n",
      "Iteration:  85%|█████████████████████████▍    | 100/118 [00:06<00:00, 18.53it/s]\u001b[A\n",
      "Iteration:  86%|█████████████████████████▉    | 102/118 [00:06<00:00, 18.78it/s]\u001b[A\n",
      "Iteration:  88%|██████████████████████████▍   | 104/118 [00:06<00:00, 18.98it/s]\u001b[A\n",
      "Iteration:  90%|██████████████████████████▉   | 106/118 [00:06<00:00, 19.12it/s]\u001b[A\n",
      "Iteration:  92%|███████████████████████████▍  | 108/118 [00:06<00:00, 19.23it/s]\u001b[A\n",
      "Iteration:  93%|███████████████████████████▉  | 110/118 [00:06<00:00, 19.29it/s]\u001b[A\n",
      "Iteration:  95%|████████████████████████████▍ | 112/118 [00:06<00:00, 19.35it/s]\u001b[A\n",
      "Iteration:  97%|████████████████████████████▉ | 114/118 [00:07<00:00, 19.38it/s]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████| 118/118 [00:07<00:00, 16.23it/s]\u001b[A\n",
      "Epoch:  76%|███████████████████████████▎        | 19/25 [02:23<00:44,  7.49s/it]\n",
      "Iteration:   0%|                                        | 0/118 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▌                               | 2/118 [00:00<00:05, 19.41it/s]\u001b[A\n",
      "Iteration:   3%|█                               | 4/118 [00:00<00:05, 19.43it/s]\u001b[A\n",
      "Iteration:   5%|█▋                              | 6/118 [00:00<00:05, 19.46it/s]\u001b[A\n",
      "Iteration:   7%|██▏                             | 8/118 [00:00<00:05, 19.39it/s]\u001b[A\n",
      "Iteration:   8%|██▋                            | 10/118 [00:00<00:05, 19.40it/s]\u001b[A\n",
      "Iteration:  10%|███▏                           | 12/118 [00:00<00:05, 19.43it/s]\u001b[A\n",
      "Iteration:  12%|███▋                           | 14/118 [00:00<00:05, 19.44it/s]\u001b[A\n",
      "Iteration:  14%|████▏                          | 16/118 [00:00<00:05, 19.37it/s]\u001b[A\n",
      "Iteration:  15%|████▋                          | 18/118 [00:00<00:05, 19.39it/s]\u001b[A\n",
      "Iteration:  17%|█████▎                         | 20/118 [00:01<00:05, 19.41it/s]\u001b[A\n",
      "Iteration:  19%|█████▊                         | 22/118 [00:01<00:04, 19.42it/s]\u001b[A\n",
      "Iteration:  20%|██████▎                        | 24/118 [00:01<00:04, 19.43it/s]\u001b[A\n",
      "Iteration:  22%|██████▊                        | 26/118 [00:01<00:04, 19.44it/s]\u001b[A\n",
      "Iteration:  24%|███████▎                       | 28/118 [00:01<00:04, 19.45it/s]\u001b[A\n",
      "Iteration:  25%|███████▉                       | 30/118 [00:01<00:04, 19.46it/s]\u001b[A\n",
      "Iteration:  27%|████████▍                      | 32/118 [00:01<00:04, 19.46it/s]\u001b[A\n",
      "Iteration:  29%|████████▉                      | 34/118 [00:01<00:04, 19.45it/s]\u001b[A\n",
      "Iteration:  31%|█████████▍                     | 36/118 [00:01<00:04, 19.46it/s]\u001b[A\n",
      "Iteration:  32%|█████████▉                     | 38/118 [00:01<00:04, 19.46it/s]\u001b[A\n",
      "Iteration:  34%|██████████▌                    | 40/118 [00:02<00:04, 19.45it/s]\u001b[A\n",
      "Iteration:  36%|███████████                    | 42/118 [00:02<00:03, 19.46it/s]\u001b[A\n",
      "Iteration:  37%|███████████▌                   | 44/118 [00:02<00:03, 19.42it/s]\u001b[A\n",
      "Iteration:  39%|████████████                   | 46/118 [00:02<00:03, 19.44it/s]\u001b[A\n",
      "Iteration:  41%|████████████▌                  | 48/118 [00:02<00:03, 19.41it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▏                 | 50/118 [00:02<00:03, 19.43it/s]\u001b[A\n",
      "Iteration:  44%|█████████████▋                 | 52/118 [00:02<00:03, 19.44it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▏                | 54/118 [00:02<00:03, 19.39it/s]\u001b[A\n",
      "Iteration:  47%|██████████████▋                | 56/118 [00:02<00:03, 19.40it/s]\u001b[A12/20/2023 22:26:16 - INFO - __main__ -   Loading features from cached file ./dataset/bert/classic/m6Am/cached_dev_m6Am_101_dnaprom\n",
      "12/20/2023 22:26:16 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/20/2023 22:26:16 - INFO - __main__ -     Num examples = 937\n",
      "12/20/2023 22:26:16 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                        | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|██████████▎                    | 10/30 [00:00<00:00, 94.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|████████████████████▋          | 20/30 [00:00<00:00, 94.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████| 30/30 [00:00<00:00, 95.11it/s]\u001b[A\u001b[A\n",
      "12/20/2023 22:26:16 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/20/2023 22:26:16 - INFO - __main__ -     acc = 0.848452508004269\n",
      "12/20/2023 22:26:16 - INFO - __main__ -     auc = 0.894489339359905\n",
      "12/20/2023 22:26:16 - INFO - __main__ -     f1 = 0.8479660862013805\n",
      "12/20/2023 22:26:16 - INFO - __main__ -     mcc = 0.7140308079391964\n",
      "12/20/2023 22:26:16 - INFO - __main__ -     precision = 0.8612357554851315\n",
      "12/20/2023 22:26:16 - INFO - __main__ -     recall = 0.8528443592201982\n",
      "/root/.conda/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "{\"eval_acc\": 0.848452508004269, \"eval_f1\": 0.8479660862013805, \"eval_mcc\": 0.7140308079391964, \"eval_auc\": 0.894489339359905, \"eval_precision\": 0.8612357554851315, \"eval_recall\": 0.8528443592201982, \"learning_rate\": 3.954802259887006e-06, \"loss\": 0.003731224455696065, \"step\": 2300}\n",
      "12/20/2023 22:26:16 - INFO - transformers.configuration_utils -   Configuration saved in ./results/your_models/bert/classic/m6Am/checkpoint-2300/config.json\n",
      "12/20/2023 22:26:16 - INFO - transformers.modeling_utils -   Model weights saved in ./results/your_models/bert/classic/m6Am/checkpoint-2300/pytorch_model.bin\n",
      "12/20/2023 22:26:16 - INFO - __main__ -   Saving model checkpoint to ./results/your_models/bert/classic/m6Am/checkpoint-2300\n",
      "12/20/2023 22:26:17 - INFO - __main__ -   Saving optimizer and scheduler states to ./results/your_models/bert/classic/m6Am/checkpoint-2300\n",
      "\n",
      "Iteration:  49%|███████████████▏               | 58/118 [00:04<00:13,  4.34it/s]\u001b[A\n",
      "Iteration:  51%|███████████████▊               | 60/118 [00:04<00:10,  5.65it/s]\u001b[A\n",
      "Iteration:  53%|████████████████▎              | 62/118 [00:04<00:07,  7.17it/s]\u001b[A\n",
      "Iteration:  54%|████████████████▊              | 64/118 [00:04<00:06,  8.84it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▎             | 66/118 [00:04<00:04, 10.57it/s]\u001b[A\n",
      "Iteration:  58%|█████████████████▊             | 68/118 [00:04<00:04, 12.24it/s]\u001b[A\n",
      "Iteration:  59%|██████████████████▍            | 70/118 [00:04<00:03, 13.74it/s]\u001b[A\n",
      "Iteration:  61%|██████████████████▉            | 72/118 [00:04<00:03, 15.06it/s]\u001b[A\n",
      "Iteration:  63%|███████████████████▍           | 74/118 [00:05<00:02, 16.14it/s]\u001b[A\n",
      "Iteration:  64%|███████████████████▉           | 76/118 [00:05<00:02, 17.01it/s]\u001b[A\n",
      "Iteration:  66%|████████████████████▍          | 78/118 [00:05<00:02, 17.67it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████          | 80/118 [00:05<00:02, 18.14it/s]\u001b[A\n",
      "Iteration:  69%|█████████████████████▌         | 82/118 [00:05<00:01, 18.50it/s]\u001b[A\n",
      "Iteration:  71%|██████████████████████         | 84/118 [00:05<00:01, 18.76it/s]\u001b[A\n",
      "Iteration:  73%|██████████████████████▌        | 86/118 [00:05<00:01, 18.84it/s]\u001b[A\n",
      "Iteration:  75%|███████████████████████        | 88/118 [00:05<00:01, 18.86it/s]\u001b[A\n",
      "Iteration:  76%|███████████████████████▋       | 90/118 [00:05<00:01, 19.01it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▏      | 92/118 [00:05<00:01, 19.12it/s]\u001b[A\n",
      "Iteration:  80%|████████████████████████▋      | 94/118 [00:06<00:01, 19.20it/s]\u001b[A\n",
      "Iteration:  81%|█████████████████████████▏     | 96/118 [00:06<00:01, 19.25it/s]\u001b[A\n",
      "Iteration:  83%|█████████████████████████▋     | 98/118 [00:06<00:01, 19.29it/s]\u001b[A\n",
      "Iteration:  85%|█████████████████████████▍    | 100/118 [00:06<00:00, 19.33it/s]\u001b[A\n",
      "Iteration:  86%|█████████████████████████▉    | 102/118 [00:06<00:00, 19.35it/s]\u001b[A\n",
      "Iteration:  88%|██████████████████████████▍   | 104/118 [00:06<00:00, 19.32it/s]\u001b[A\n",
      "Iteration:  90%|██████████████████████████▉   | 106/118 [00:06<00:00, 19.35it/s]\u001b[A\n",
      "Iteration:  92%|███████████████████████████▍  | 108/118 [00:06<00:00, 19.33it/s]\u001b[A\n",
      "Iteration:  93%|███████████████████████████▉  | 110/118 [00:06<00:00, 19.31it/s]\u001b[A\n",
      "Iteration:  95%|████████████████████████████▍ | 112/118 [00:06<00:00, 19.34it/s]\u001b[A\n",
      "Iteration:  97%|████████████████████████████▉ | 114/118 [00:07<00:00, 19.34it/s]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████| 118/118 [00:07<00:00, 16.22it/s]\u001b[A\n",
      "Epoch:  80%|████████████████████████████▊       | 20/25 [02:30<00:37,  7.43s/it]\n",
      "Iteration:   0%|                                        | 0/118 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▌                               | 2/118 [00:00<00:05, 19.37it/s]\u001b[A\n",
      "Iteration:   3%|█                               | 4/118 [00:00<00:05, 19.40it/s]\u001b[A\n",
      "Iteration:   5%|█▋                              | 6/118 [00:00<00:05, 19.41it/s]\u001b[A\n",
      "Iteration:   7%|██▏                             | 8/118 [00:00<00:05, 19.41it/s]\u001b[A\n",
      "Iteration:   8%|██▋                            | 10/118 [00:00<00:05, 19.40it/s]\u001b[A\n",
      "Iteration:  10%|███▏                           | 12/118 [00:00<00:05, 19.40it/s]\u001b[A\n",
      "Iteration:  12%|███▋                           | 14/118 [00:00<00:05, 19.39it/s]\u001b[A\n",
      "Iteration:  14%|████▏                          | 16/118 [00:00<00:05, 19.39it/s]\u001b[A\n",
      "Iteration:  15%|████▋                          | 18/118 [00:00<00:05, 19.39it/s]\u001b[A\n",
      "Iteration:  17%|█████▎                         | 20/118 [00:01<00:05, 19.38it/s]\u001b[A\n",
      "Iteration:  19%|█████▊                         | 22/118 [00:01<00:04, 19.38it/s]\u001b[A\n",
      "Iteration:  20%|██████▎                        | 24/118 [00:01<00:04, 19.37it/s]\u001b[A\n",
      "Iteration:  22%|██████▊                        | 26/118 [00:01<00:04, 19.33it/s]\u001b[A\n",
      "Iteration:  24%|███████▎                       | 28/118 [00:01<00:04, 19.33it/s]\u001b[A\n",
      "Iteration:  25%|███████▉                       | 30/118 [00:01<00:04, 19.29it/s]\u001b[A\n",
      "Iteration:  27%|████████▍                      | 32/118 [00:01<00:04, 19.32it/s]\u001b[A\n",
      "Iteration:  29%|████████▉                      | 34/118 [00:01<00:04, 19.35it/s]\u001b[A\n",
      "Iteration:  31%|█████████▍                     | 36/118 [00:01<00:04, 19.36it/s]\u001b[A\n",
      "Iteration:  32%|█████████▉                     | 38/118 [00:01<00:04, 19.37it/s]\u001b[A12/20/2023 22:26:22 - INFO - __main__ -   Loading features from cached file ./dataset/bert/classic/m6Am/cached_dev_m6Am_101_dnaprom\n",
      "12/20/2023 22:26:22 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/20/2023 22:26:22 - INFO - __main__ -     Num examples = 937\n",
      "12/20/2023 22:26:22 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                        | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|██████████▎                    | 10/30 [00:00<00:00, 93.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|████████████████████▋          | 20/30 [00:00<00:00, 93.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████| 30/30 [00:00<00:00, 94.00it/s]\u001b[A\u001b[A\n",
      "12/20/2023 22:26:22 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/20/2023 22:26:22 - INFO - __main__ -     acc = 0.847385272145144\n",
      "12/20/2023 22:26:22 - INFO - __main__ -     auc = 0.8941286581746793\n",
      "12/20/2023 22:26:22 - INFO - __main__ -     f1 = 0.846876703468247\n",
      "12/20/2023 22:26:22 - INFO - __main__ -     mcc = 0.7122155864231462\n",
      "12/20/2023 22:26:22 - INFO - __main__ -     precision = 0.8604437958389463\n",
      "12/20/2023 22:26:22 - INFO - __main__ -     recall = 0.8518239510569328\n",
      "/root/.conda/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "{\"eval_acc\": 0.847385272145144, \"eval_f1\": 0.846876703468247, \"eval_mcc\": 0.7122155864231462, \"eval_auc\": 0.8941286581746793, \"eval_precision\": 0.8604437958389463, \"eval_recall\": 0.8518239510569328, \"learning_rate\": 2.824858757062147e-06, \"loss\": 0.00723669648032228, \"step\": 2400}\n",
      "12/20/2023 22:26:22 - INFO - transformers.configuration_utils -   Configuration saved in ./results/your_models/bert/classic/m6Am/checkpoint-2400/config.json\n",
      "12/20/2023 22:26:23 - INFO - transformers.modeling_utils -   Model weights saved in ./results/your_models/bert/classic/m6Am/checkpoint-2400/pytorch_model.bin\n",
      "12/20/2023 22:26:23 - INFO - __main__ -   Saving model checkpoint to ./results/your_models/bert/classic/m6Am/checkpoint-2400\n",
      "12/20/2023 22:26:23 - INFO - __main__ -   Saving optimizer and scheduler states to ./results/your_models/bert/classic/m6Am/checkpoint-2400\n",
      "\n",
      "Iteration:  34%|██████████▌                    | 40/118 [00:03<00:18,  4.29it/s]\u001b[A\n",
      "Iteration:  36%|███████████                    | 42/118 [00:03<00:13,  5.58it/s]\u001b[A\n",
      "Iteration:  37%|███████████▌                   | 44/118 [00:03<00:10,  7.09it/s]\u001b[A\n",
      "Iteration:  39%|████████████                   | 46/118 [00:03<00:08,  8.75it/s]\u001b[A\n",
      "Iteration:  41%|████████████▌                  | 48/118 [00:03<00:06, 10.48it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▏                 | 50/118 [00:03<00:05, 12.14it/s]\u001b[A\n",
      "Iteration:  44%|█████████████▋                 | 52/118 [00:03<00:04, 13.67it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▏                | 54/118 [00:04<00:04, 15.00it/s]\u001b[A\n",
      "Iteration:  47%|██████████████▋                | 56/118 [00:04<00:03, 16.08it/s]\u001b[A\n",
      "Iteration:  49%|███████████████▏               | 58/118 [00:04<00:03, 16.95it/s]\u001b[A\n",
      "Iteration:  51%|███████████████▊               | 60/118 [00:04<00:03, 17.62it/s]\u001b[A\n",
      "Iteration:  53%|████████████████▎              | 62/118 [00:04<00:03, 18.12it/s]\u001b[A\n",
      "Iteration:  54%|████████████████▊              | 64/118 [00:04<00:02, 18.50it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▎             | 66/118 [00:04<00:02, 18.74it/s]\u001b[A\n",
      "Iteration:  58%|█████████████████▊             | 68/118 [00:04<00:02, 18.94it/s]\u001b[A\n",
      "Iteration:  59%|██████████████████▍            | 70/118 [00:04<00:02, 19.07it/s]\u001b[A\n",
      "Iteration:  61%|██████████████████▉            | 72/118 [00:04<00:02, 19.16it/s]\u001b[A\n",
      "Iteration:  63%|███████████████████▍           | 74/118 [00:05<00:02, 19.22it/s]\u001b[A\n",
      "Iteration:  64%|███████████████████▉           | 76/118 [00:05<00:02, 19.27it/s]\u001b[A\n",
      "Iteration:  66%|████████████████████▍          | 78/118 [00:05<00:02, 19.32it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████          | 80/118 [00:05<00:01, 19.33it/s]\u001b[A\n",
      "Iteration:  69%|█████████████████████▌         | 82/118 [00:05<00:01, 19.32it/s]\u001b[A\n",
      "Iteration:  71%|██████████████████████         | 84/118 [00:05<00:01, 19.28it/s]\u001b[A\n",
      "Iteration:  73%|██████████████████████▌        | 86/118 [00:05<00:01, 19.30it/s]\u001b[A\n",
      "Iteration:  75%|███████████████████████        | 88/118 [00:05<00:01, 19.33it/s]\u001b[A\n",
      "Iteration:  76%|███████████████████████▋       | 90/118 [00:05<00:01, 19.35it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▏      | 92/118 [00:05<00:01, 19.36it/s]\u001b[A\n",
      "Iteration:  80%|████████████████████████▋      | 94/118 [00:06<00:01, 19.38it/s]\u001b[A\n",
      "Iteration:  81%|█████████████████████████▏     | 96/118 [00:06<00:01, 19.39it/s]\u001b[A\n",
      "Iteration:  83%|█████████████████████████▋     | 98/118 [00:06<00:01, 19.38it/s]\u001b[A\n",
      "Iteration:  85%|█████████████████████████▍    | 100/118 [00:06<00:00, 19.38it/s]\u001b[A\n",
      "Iteration:  86%|█████████████████████████▉    | 102/118 [00:06<00:00, 19.38it/s]\u001b[A\n",
      "Iteration:  88%|██████████████████████████▍   | 104/118 [00:06<00:00, 19.37it/s]\u001b[A\n",
      "Iteration:  90%|██████████████████████████▉   | 106/118 [00:06<00:00, 19.39it/s]\u001b[A\n",
      "Iteration:  92%|███████████████████████████▍  | 108/118 [00:06<00:00, 19.38it/s]\u001b[A\n",
      "Iteration:  93%|███████████████████████████▉  | 110/118 [00:06<00:00, 19.38it/s]\u001b[A\n",
      "Iteration:  95%|████████████████████████████▍ | 112/118 [00:06<00:00, 19.40it/s]\u001b[A\n",
      "Iteration:  97%|████████████████████████████▉ | 114/118 [00:07<00:00, 19.39it/s]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████| 118/118 [00:07<00:00, 16.17it/s]\u001b[A\n",
      "Epoch:  84%|██████████████████████████████▏     | 21/25 [02:37<00:29,  7.39s/it]\n",
      "Iteration:   0%|                                        | 0/118 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▌                               | 2/118 [00:00<00:06, 19.26it/s]\u001b[A\n",
      "Iteration:   3%|█                               | 4/118 [00:00<00:05, 19.20it/s]\u001b[A\n",
      "Iteration:   5%|█▋                              | 6/118 [00:00<00:05, 19.28it/s]\u001b[A\n",
      "Iteration:   7%|██▏                             | 8/118 [00:00<00:05, 19.32it/s]\u001b[A\n",
      "Iteration:   8%|██▋                            | 10/118 [00:00<00:05, 19.34it/s]\u001b[A\n",
      "Iteration:  10%|███▏                           | 12/118 [00:00<00:05, 19.35it/s]\u001b[A\n",
      "Iteration:  12%|███▋                           | 14/118 [00:00<00:05, 19.38it/s]\u001b[A\n",
      "Iteration:  14%|████▏                          | 16/118 [00:00<00:05, 19.38it/s]\u001b[A\n",
      "Iteration:  15%|████▋                          | 18/118 [00:00<00:05, 19.39it/s]\u001b[A\n",
      "Iteration:  17%|█████▎                         | 20/118 [00:01<00:05, 19.39it/s]\u001b[A12/20/2023 22:26:29 - INFO - __main__ -   Loading features from cached file ./dataset/bert/classic/m6Am/cached_dev_m6Am_101_dnaprom\n",
      "12/20/2023 22:26:29 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/20/2023 22:26:29 - INFO - __main__ -     Num examples = 937\n",
      "12/20/2023 22:26:29 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                        | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|██████████▎                    | 10/30 [00:00<00:00, 92.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|████████████████████▋          | 20/30 [00:00<00:00, 93.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████| 30/30 [00:00<00:00, 93.44it/s]\u001b[A\u001b[A\n",
      "12/20/2023 22:26:29 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/20/2023 22:26:29 - INFO - __main__ -     acc = 0.8452508004268944\n",
      "12/20/2023 22:26:29 - INFO - __main__ -     auc = 0.8937383006894033\n",
      "12/20/2023 22:26:29 - INFO - __main__ -     f1 = 0.844696073422013\n",
      "12/20/2023 22:26:29 - INFO - __main__ -     mcc = 0.7085915397273845\n",
      "12/20/2023 22:26:29 - INFO - __main__ -     precision = 0.858866623572506\n",
      "12/20/2023 22:26:29 - INFO - __main__ -     recall = 0.8497831347304022\n",
      "/root/.conda/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "{\"eval_acc\": 0.8452508004268944, \"eval_f1\": 0.844696073422013, \"eval_mcc\": 0.7085915397273845, \"eval_auc\": 0.8937383006894033, \"eval_precision\": 0.858866623572506, \"eval_recall\": 0.8497831347304022, \"learning_rate\": 1.6949152542372882e-06, \"loss\": 0.004254776616871823, \"step\": 2500}\n",
      "12/20/2023 22:26:29 - INFO - transformers.configuration_utils -   Configuration saved in ./results/your_models/bert/classic/m6Am/checkpoint-2500/config.json\n",
      "12/20/2023 22:26:29 - INFO - transformers.modeling_utils -   Model weights saved in ./results/your_models/bert/classic/m6Am/checkpoint-2500/pytorch_model.bin\n",
      "12/20/2023 22:26:29 - INFO - __main__ -   Saving model checkpoint to ./results/your_models/bert/classic/m6Am/checkpoint-2500\n",
      "12/20/2023 22:26:30 - INFO - __main__ -   Saving optimizer and scheduler states to ./results/your_models/bert/classic/m6Am/checkpoint-2500\n",
      "\n",
      "Iteration:  19%|█████▊                         | 22/118 [00:02<00:22,  4.20it/s]\u001b[A\n",
      "Iteration:  20%|██████▎                        | 24/118 [00:02<00:17,  5.51it/s]\u001b[A\n",
      "Iteration:  22%|██████▊                        | 26/118 [00:02<00:13,  7.03it/s]\u001b[A\n",
      "Iteration:  24%|███████▎                       | 28/118 [00:02<00:10,  8.70it/s]\u001b[A\n",
      "Iteration:  25%|███████▉                       | 30/118 [00:02<00:08, 10.44it/s]\u001b[A\n",
      "Iteration:  27%|████████▍                      | 32/118 [00:02<00:07, 12.12it/s]\u001b[A\n",
      "Iteration:  29%|████████▉                      | 34/118 [00:02<00:06, 13.66it/s]\u001b[A\n",
      "Iteration:  31%|█████████▍                     | 36/118 [00:03<00:05, 14.99it/s]\u001b[A\n",
      "Iteration:  32%|█████████▉                     | 38/118 [00:03<00:04, 16.08it/s]\u001b[A\n",
      "Iteration:  34%|██████████▌                    | 40/118 [00:03<00:04, 16.96it/s]\u001b[A\n",
      "Iteration:  36%|███████████                    | 42/118 [00:03<00:04, 17.62it/s]\u001b[A\n",
      "Iteration:  37%|███████████▌                   | 44/118 [00:03<00:04, 18.12it/s]\u001b[A\n",
      "Iteration:  39%|████████████                   | 46/118 [00:03<00:03, 18.48it/s]\u001b[A\n",
      "Iteration:  41%|████████████▌                  | 48/118 [00:03<00:03, 18.74it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▏                 | 50/118 [00:03<00:03, 18.94it/s]\u001b[A\n",
      "Iteration:  44%|█████████████▋                 | 52/118 [00:03<00:03, 19.07it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▏                | 54/118 [00:04<00:03, 19.17it/s]\u001b[A\n",
      "Iteration:  47%|██████████████▋                | 56/118 [00:04<00:03, 19.24it/s]\u001b[A\n",
      "Iteration:  49%|███████████████▏               | 58/118 [00:04<00:03, 19.22it/s]\u001b[A\n",
      "Iteration:  51%|███████████████▊               | 60/118 [00:04<00:03, 19.24it/s]\u001b[A\n",
      "Iteration:  53%|████████████████▎              | 62/118 [00:04<00:02, 19.28it/s]\u001b[A\n",
      "Iteration:  54%|████████████████▊              | 64/118 [00:04<00:02, 19.33it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▎             | 66/118 [00:04<00:02, 19.34it/s]\u001b[A\n",
      "Iteration:  58%|█████████████████▊             | 68/118 [00:04<00:02, 19.31it/s]\u001b[A\n",
      "Iteration:  59%|██████████████████▍            | 70/118 [00:04<00:02, 19.08it/s]\u001b[A\n",
      "Iteration:  61%|██████████████████▉            | 72/118 [00:04<00:02, 19.14it/s]\u001b[A\n",
      "Iteration:  63%|███████████████████▍           | 74/118 [00:05<00:02, 19.20it/s]\u001b[A\n",
      "Iteration:  64%|███████████████████▉           | 76/118 [00:05<00:02, 19.13it/s]\u001b[A\n",
      "Iteration:  66%|████████████████████▍          | 78/118 [00:05<00:02, 19.19it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████          | 80/118 [00:05<00:01, 19.24it/s]\u001b[A\n",
      "Iteration:  69%|█████████████████████▌         | 82/118 [00:05<00:01, 19.28it/s]\u001b[A\n",
      "Iteration:  71%|██████████████████████         | 84/118 [00:05<00:01, 19.29it/s]\u001b[A\n",
      "Iteration:  73%|██████████████████████▌        | 86/118 [00:05<00:01, 19.32it/s]\u001b[A\n",
      "Iteration:  75%|███████████████████████        | 88/118 [00:05<00:01, 19.33it/s]\u001b[A\n",
      "Iteration:  76%|███████████████████████▋       | 90/118 [00:05<00:01, 19.35it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▏      | 92/118 [00:05<00:01, 19.36it/s]\u001b[A\n",
      "Iteration:  80%|████████████████████████▋      | 94/118 [00:06<00:01, 19.37it/s]\u001b[A\n",
      "Iteration:  81%|█████████████████████████▏     | 96/118 [00:06<00:01, 19.30it/s]\u001b[A\n",
      "Iteration:  83%|█████████████████████████▋     | 98/118 [00:06<00:01, 19.26it/s]\u001b[A\n",
      "Iteration:  85%|█████████████████████████▍    | 100/118 [00:06<00:00, 19.28it/s]\u001b[A\n",
      "Iteration:  86%|█████████████████████████▉    | 102/118 [00:06<00:00, 19.29it/s]\u001b[A\n",
      "Iteration:  88%|██████████████████████████▍   | 104/118 [00:06<00:00, 19.31it/s]\u001b[A\n",
      "Iteration:  90%|██████████████████████████▉   | 106/118 [00:06<00:00, 19.33it/s]\u001b[A\n",
      "Iteration:  92%|███████████████████████████▍  | 108/118 [00:06<00:00, 19.34it/s]\u001b[A\n",
      "Iteration:  93%|███████████████████████████▉  | 110/118 [00:06<00:00, 19.34it/s]\u001b[A\n",
      "Iteration:  95%|████████████████████████████▍ | 112/118 [00:07<00:00, 19.35it/s]\u001b[A\n",
      "Iteration:  97%|████████████████████████████▉ | 114/118 [00:07<00:00, 19.34it/s]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████| 118/118 [00:07<00:00, 16.13it/s]\u001b[A\n",
      "Epoch:  88%|███████████████████████████████▋    | 22/25 [02:45<00:22,  7.37s/it]\n",
      "Iteration:   0%|                                        | 0/118 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▌                               | 2/118 [00:00<00:06, 19.28it/s]\u001b[A12/20/2023 22:26:35 - INFO - __main__ -   Loading features from cached file ./dataset/bert/classic/m6Am/cached_dev_m6Am_101_dnaprom\n",
      "12/20/2023 22:26:35 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/20/2023 22:26:35 - INFO - __main__ -     Num examples = 937\n",
      "12/20/2023 22:26:35 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                        | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|██████████▎                    | 10/30 [00:00<00:00, 92.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|████████████████████▋          | 20/30 [00:00<00:00, 93.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████| 30/30 [00:00<00:00, 93.87it/s]\u001b[A\u001b[A\n",
      "12/20/2023 22:26:35 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/20/2023 22:26:35 - INFO - __main__ -     acc = 0.8441835645677694\n",
      "12/20/2023 22:26:35 - INFO - __main__ -     auc = 0.8937063415970413\n",
      "12/20/2023 22:26:35 - INFO - __main__ -     f1 = 0.8435633193365718\n",
      "12/20/2023 22:26:35 - INFO - __main__ -     mcc = 0.7075091200503025\n",
      "12/20/2023 22:26:35 - INFO - __main__ -     precision = 0.8587168798001991\n",
      "12/20/2023 22:26:35 - INFO - __main__ -     recall = 0.8488608866365338\n",
      "/root/.conda/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "{\"eval_acc\": 0.8441835645677694, \"eval_f1\": 0.8435633193365718, \"eval_mcc\": 0.7075091200503025, \"eval_auc\": 0.8937063415970413, \"eval_precision\": 0.8587168798001991, \"eval_recall\": 0.8488608866365338, \"learning_rate\": 5.649717514124293e-07, \"loss\": 0.003508177014009561, \"step\": 2600}\n",
      "12/20/2023 22:26:35 - INFO - transformers.configuration_utils -   Configuration saved in ./results/your_models/bert/classic/m6Am/checkpoint-2600/config.json\n",
      "12/20/2023 22:26:36 - INFO - transformers.modeling_utils -   Model weights saved in ./results/your_models/bert/classic/m6Am/checkpoint-2600/pytorch_model.bin\n",
      "12/20/2023 22:26:36 - INFO - __main__ -   Saving model checkpoint to ./results/your_models/bert/classic/m6Am/checkpoint-2600\n",
      "12/20/2023 22:26:36 - INFO - __main__ -   Saving optimizer and scheduler states to ./results/your_models/bert/classic/m6Am/checkpoint-2600\n",
      "\n",
      "Iteration:   3%|█                               | 4/118 [00:01<00:46,  2.44it/s]\u001b[A\n",
      "Iteration:   5%|█▋                              | 6/118 [00:01<00:27,  4.05it/s]\u001b[A\n",
      "Iteration:   7%|██▏                             | 8/118 [00:01<00:18,  5.88it/s]\u001b[A\n",
      "Iteration:   8%|██▋                            | 10/118 [00:01<00:13,  7.86it/s]\u001b[A\n",
      "Iteration:  10%|███▏                           | 12/118 [00:01<00:10,  9.84it/s]\u001b[A\n",
      "Iteration:  12%|███▋                           | 14/118 [00:01<00:08, 11.73it/s]\u001b[A\n",
      "Iteration:  14%|████▏                          | 16/118 [00:02<00:07, 13.39it/s]\u001b[A\n",
      "Iteration:  15%|████▋                          | 18/118 [00:02<00:06, 14.80it/s]\u001b[A\n",
      "Iteration:  17%|█████▎                         | 20/118 [00:02<00:06, 15.95it/s]\u001b[A\n",
      "Iteration:  19%|█████▊                         | 22/118 [00:02<00:05, 16.86it/s]\u001b[A\n",
      "Iteration:  20%|██████▎                        | 24/118 [00:02<00:05, 17.55it/s]\u001b[A\n",
      "Iteration:  22%|██████▊                        | 26/118 [00:02<00:05, 18.06it/s]\u001b[A\n",
      "Iteration:  24%|███████▎                       | 28/118 [00:02<00:04, 18.43it/s]\u001b[A\n",
      "Iteration:  25%|███████▉                       | 30/118 [00:02<00:04, 18.70it/s]\u001b[A\n",
      "Iteration:  27%|████████▍                      | 32/118 [00:02<00:04, 18.89it/s]\u001b[A\n",
      "Iteration:  29%|████████▉                      | 34/118 [00:02<00:04, 19.03it/s]\u001b[A\n",
      "Iteration:  31%|█████████▍                     | 36/118 [00:03<00:04, 19.09it/s]\u001b[A\n",
      "Iteration:  32%|█████████▉                     | 38/118 [00:03<00:04, 19.16it/s]\u001b[A\n",
      "Iteration:  34%|██████████▌                    | 40/118 [00:03<00:04, 19.22it/s]\u001b[A\n",
      "Iteration:  36%|███████████                    | 42/118 [00:03<00:03, 19.23it/s]\u001b[A\n",
      "Iteration:  37%|███████████▌                   | 44/118 [00:03<00:03, 19.26it/s]\u001b[A\n",
      "Iteration:  39%|████████████                   | 46/118 [00:03<00:03, 19.30it/s]\u001b[A\n",
      "Iteration:  41%|████████████▌                  | 48/118 [00:03<00:03, 19.32it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▏                 | 50/118 [00:03<00:03, 19.33it/s]\u001b[A\n",
      "Iteration:  44%|█████████████▋                 | 52/118 [00:03<00:03, 19.35it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▏                | 54/118 [00:04<00:03, 19.33it/s]\u001b[A\n",
      "Iteration:  47%|██████████████▋                | 56/118 [00:04<00:03, 19.34it/s]\u001b[A\n",
      "Iteration:  49%|███████████████▏               | 58/118 [00:04<00:03, 19.35it/s]\u001b[A\n",
      "Iteration:  51%|███████████████▊               | 60/118 [00:04<00:02, 19.34it/s]\u001b[A\n",
      "Iteration:  53%|████████████████▎              | 62/118 [00:04<00:02, 19.34it/s]\u001b[A\n",
      "Iteration:  54%|████████████████▊              | 64/118 [00:04<00:02, 19.30it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▎             | 66/118 [00:04<00:02, 19.33it/s]\u001b[A\n",
      "Iteration:  58%|█████████████████▊             | 68/118 [00:04<00:02, 19.34it/s]\u001b[A\n",
      "Iteration:  59%|██████████████████▍            | 70/118 [00:04<00:02, 19.35it/s]\u001b[A\n",
      "Iteration:  61%|██████████████████▉            | 72/118 [00:04<00:02, 19.27it/s]\u001b[A\n",
      "Iteration:  63%|███████████████████▍           | 74/118 [00:05<00:02, 19.29it/s]\u001b[A\n",
      "Iteration:  64%|███████████████████▉           | 76/118 [00:05<00:02, 19.28it/s]\u001b[A\n",
      "Iteration:  66%|████████████████████▍          | 78/118 [00:05<00:02, 19.29it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████          | 80/118 [00:05<00:01, 19.32it/s]\u001b[A\n",
      "Iteration:  69%|█████████████████████▌         | 82/118 [00:05<00:01, 19.34it/s]\u001b[A\n",
      "Iteration:  71%|██████████████████████         | 84/118 [00:05<00:01, 19.34it/s]\u001b[A\n",
      "Iteration:  73%|██████████████████████▌        | 86/118 [00:05<00:01, 19.35it/s]\u001b[A\n",
      "Iteration:  75%|███████████████████████        | 88/118 [00:05<00:01, 19.35it/s]\u001b[A\n",
      "Iteration:  76%|███████████████████████▋       | 90/118 [00:05<00:01, 19.35it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▏      | 92/118 [00:05<00:01, 19.34it/s]\u001b[A\n",
      "Iteration:  80%|████████████████████████▋      | 94/118 [00:06<00:01, 19.34it/s]\u001b[A\n",
      "Iteration:  81%|█████████████████████████▏     | 96/118 [00:06<00:01, 19.34it/s]\u001b[A\n",
      "Iteration:  83%|█████████████████████████▋     | 98/118 [00:06<00:01, 19.35it/s]\u001b[A\n",
      "Iteration:  85%|█████████████████████████▍    | 100/118 [00:06<00:00, 19.36it/s]\u001b[A\n",
      "Iteration:  86%|█████████████████████████▉    | 102/118 [00:06<00:00, 19.35it/s]\u001b[A12/20/2023 22:26:41 - INFO - __main__ -   Loading features from cached file ./dataset/bert/classic/m6Am/cached_dev_m6Am_101_dnaprom\n",
      "12/20/2023 22:26:41 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/20/2023 22:26:41 - INFO - __main__ -     Num examples = 937\n",
      "12/20/2023 22:26:41 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                        | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|██████████▎                    | 10/30 [00:00<00:00, 93.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|████████████████████▋          | 20/30 [00:00<00:00, 93.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████| 30/30 [00:00<00:00, 94.15it/s]\u001b[A\u001b[A\n",
      "12/20/2023 22:26:42 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/20/2023 22:26:42 - INFO - __main__ -     acc = 0.8431163287086446\n",
      "12/20/2023 22:26:42 - INFO - __main__ -     auc = 0.893564808473725\n",
      "12/20/2023 22:26:42 - INFO - __main__ -     f1 = 0.8424703986532003\n",
      "12/20/2023 22:26:42 - INFO - __main__ -     mcc = 0.705708079797261\n",
      "12/20/2023 22:26:42 - INFO - __main__ -     precision = 0.8579398637538173\n",
      "12/20/2023 22:26:42 - INFO - __main__ -     recall = 0.8478404784732685\n",
      "/root/.conda/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "{\"eval_acc\": 0.8431163287086446, \"eval_f1\": 0.8424703986532003, \"eval_mcc\": 0.705708079797261, \"eval_auc\": 0.893564808473725, \"eval_precision\": 0.8579398637538173, \"eval_recall\": 0.8478404784732685, \"learning_rate\": 0.0, \"loss\": 0.0009735377634933684, \"step\": 2700}\n",
      "12/20/2023 22:26:42 - INFO - transformers.configuration_utils -   Configuration saved in ./results/your_models/bert/classic/m6Am/checkpoint-2700/config.json\n",
      "12/20/2023 22:26:42 - INFO - transformers.modeling_utils -   Model weights saved in ./results/your_models/bert/classic/m6Am/checkpoint-2700/pytorch_model.bin\n",
      "12/20/2023 22:26:42 - INFO - __main__ -   Saving model checkpoint to ./results/your_models/bert/classic/m6Am/checkpoint-2700\n",
      "12/20/2023 22:26:43 - INFO - __main__ -   Saving optimizer and scheduler states to ./results/your_models/bert/classic/m6Am/checkpoint-2700\n",
      "\n",
      "Iteration:  88%|██████████████████████████▍   | 104/118 [00:07<00:03,  4.13it/s]\u001b[A\n",
      "Iteration:  90%|██████████████████████████▉   | 106/118 [00:07<00:02,  5.39it/s]\u001b[A\n",
      "Iteration:  92%|███████████████████████████▍  | 108/118 [00:08<00:01,  6.88it/s]\u001b[A\n",
      "Iteration:  93%|███████████████████████████▉  | 110/118 [00:08<00:00,  8.53it/s]\u001b[A\n",
      "Iteration:  95%|████████████████████████████▍ | 112/118 [00:08<00:00, 10.26it/s]\u001b[A\n",
      "Iteration:  97%|████████████████████████████▉ | 114/118 [00:08<00:00, 11.95it/s]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████| 118/118 [00:08<00:00, 13.74it/s]\u001b[A\n",
      "Epoch:  92%|█████████████████████████████████   | 23/25 [02:53<00:15,  7.73s/it]\n",
      "Iteration:   0%|                                        | 0/118 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▌                               | 2/118 [00:00<00:05, 19.38it/s]\u001b[A\n",
      "Iteration:   3%|█                               | 4/118 [00:00<00:05, 19.38it/s]\u001b[A\n",
      "Iteration:   5%|█▋                              | 6/118 [00:00<00:05, 19.20it/s]\u001b[A\n",
      "Iteration:   7%|██▏                             | 8/118 [00:00<00:05, 19.05it/s]\u001b[A\n",
      "Iteration:   8%|██▋                            | 10/118 [00:00<00:05, 19.16it/s]\u001b[A\n",
      "Iteration:  10%|███▏                           | 12/118 [00:00<00:05, 19.19it/s]\u001b[A\n",
      "Iteration:  12%|███▋                           | 14/118 [00:00<00:05, 19.24it/s]\u001b[A\n",
      "Iteration:  14%|████▏                          | 16/118 [00:00<00:05, 19.29it/s]\u001b[A\n",
      "Iteration:  15%|████▋                          | 18/118 [00:00<00:05, 19.33it/s]\u001b[A\n",
      "Iteration:  17%|█████▎                         | 20/118 [00:01<00:05, 19.35it/s]\u001b[A\n",
      "Iteration:  19%|█████▊                         | 22/118 [00:01<00:04, 19.37it/s]\u001b[A\n",
      "Iteration:  20%|██████▎                        | 24/118 [00:01<00:04, 19.39it/s]\u001b[A\n",
      "Iteration:  22%|██████▊                        | 26/118 [00:01<00:04, 19.39it/s]\u001b[A\n",
      "Iteration:  24%|███████▎                       | 28/118 [00:01<00:04, 19.38it/s]\u001b[A\n",
      "Iteration:  25%|███████▉                       | 30/118 [00:01<00:04, 19.39it/s]\u001b[A\n",
      "Iteration:  27%|████████▍                      | 32/118 [00:01<00:04, 19.39it/s]\u001b[A\n",
      "Iteration:  29%|████████▉                      | 34/118 [00:01<00:04, 19.38it/s]\u001b[A\n",
      "Iteration:  31%|█████████▍                     | 36/118 [00:01<00:04, 19.39it/s]\u001b[A\n",
      "Iteration:  32%|█████████▉                     | 38/118 [00:01<00:04, 19.40it/s]\u001b[A\n",
      "Iteration:  34%|██████████▌                    | 40/118 [00:02<00:04, 19.39it/s]\u001b[A\n",
      "Iteration:  36%|███████████                    | 42/118 [00:02<00:03, 19.39it/s]\u001b[A\n",
      "Iteration:  37%|███████████▌                   | 44/118 [00:02<00:03, 19.36it/s]\u001b[A\n",
      "Iteration:  39%|████████████                   | 46/118 [00:02<00:03, 19.33it/s]\u001b[A\n",
      "Iteration:  41%|████████████▌                  | 48/118 [00:02<00:03, 19.36it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▏                 | 50/118 [00:02<00:03, 19.35it/s]\u001b[A\n",
      "Iteration:  44%|█████████████▋                 | 52/118 [00:02<00:03, 19.33it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▏                | 54/118 [00:02<00:03, 19.35it/s]\u001b[A\n",
      "Iteration:  47%|██████████████▋                | 56/118 [00:02<00:03, 19.36it/s]\u001b[A\n",
      "Iteration:  49%|███████████████▏               | 58/118 [00:02<00:03, 19.37it/s]\u001b[A\n",
      "Iteration:  51%|███████████████▊               | 60/118 [00:03<00:02, 19.38it/s]\u001b[A\n",
      "Iteration:  53%|████████████████▎              | 62/118 [00:03<00:02, 19.38it/s]\u001b[A\n",
      "Iteration:  54%|████████████████▊              | 64/118 [00:03<00:02, 19.38it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▎             | 66/118 [00:03<00:02, 19.39it/s]\u001b[A\n",
      "Iteration:  58%|█████████████████▊             | 68/118 [00:03<00:02, 19.39it/s]\u001b[A\n",
      "Iteration:  59%|██████████████████▍            | 70/118 [00:03<00:02, 19.40it/s]\u001b[A\n",
      "Iteration:  61%|██████████████████▉            | 72/118 [00:03<00:02, 19.40it/s]\u001b[A\n",
      "Iteration:  63%|███████████████████▍           | 74/118 [00:03<00:02, 19.39it/s]\u001b[A\n",
      "Iteration:  64%|███████████████████▉           | 76/118 [00:03<00:02, 19.39it/s]\u001b[A\n",
      "Iteration:  66%|████████████████████▍          | 78/118 [00:04<00:02, 19.40it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████          | 80/118 [00:04<00:01, 19.40it/s]\u001b[A\n",
      "Iteration:  69%|█████████████████████▌         | 82/118 [00:04<00:01, 19.41it/s]\u001b[A\n",
      "Iteration:  71%|██████████████████████         | 84/118 [00:04<00:01, 19.35it/s]\u001b[A12/20/2023 22:26:48 - INFO - __main__ -   Loading features from cached file ./dataset/bert/classic/m6Am/cached_dev_m6Am_101_dnaprom\n",
      "12/20/2023 22:26:48 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/20/2023 22:26:48 - INFO - __main__ -     Num examples = 937\n",
      "12/20/2023 22:26:48 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                        | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|██████████▎                    | 10/30 [00:00<00:00, 92.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|████████████████████▋          | 20/30 [00:00<00:00, 93.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████| 30/30 [00:00<00:00, 93.68it/s]\u001b[A\u001b[A\n",
      "12/20/2023 22:26:48 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/20/2023 22:26:48 - INFO - __main__ -     acc = 0.8431163287086446\n",
      "12/20/2023 22:26:48 - INFO - __main__ -     auc = 0.893564808473725\n",
      "12/20/2023 22:26:48 - INFO - __main__ -     f1 = 0.8424703986532003\n",
      "12/20/2023 22:26:48 - INFO - __main__ -     mcc = 0.705708079797261\n",
      "12/20/2023 22:26:48 - INFO - __main__ -     precision = 0.8579398637538173\n",
      "12/20/2023 22:26:48 - INFO - __main__ -     recall = 0.8478404784732685\n",
      "/root/.conda/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "{\"eval_acc\": 0.8431163287086446, \"eval_f1\": 0.8424703986532003, \"eval_mcc\": 0.705708079797261, \"eval_auc\": 0.893564808473725, \"eval_precision\": 0.8579398637538173, \"eval_recall\": 0.8478404784732685, \"learning_rate\": 0.0, \"loss\": 0.003694921361893648, \"step\": 2800}\n",
      "12/20/2023 22:26:48 - INFO - transformers.configuration_utils -   Configuration saved in ./results/your_models/bert/classic/m6Am/checkpoint-2800/config.json\n",
      "12/20/2023 22:26:48 - INFO - transformers.modeling_utils -   Model weights saved in ./results/your_models/bert/classic/m6Am/checkpoint-2800/pytorch_model.bin\n",
      "12/20/2023 22:26:48 - INFO - __main__ -   Saving model checkpoint to ./results/your_models/bert/classic/m6Am/checkpoint-2800\n",
      "12/20/2023 22:26:49 - INFO - __main__ -   Saving optimizer and scheduler states to ./results/your_models/bert/classic/m6Am/checkpoint-2800\n",
      "\n",
      "Iteration:  73%|██████████████████████▌        | 86/118 [00:05<00:07,  4.29it/s]\u001b[A\n",
      "Iteration:  75%|███████████████████████        | 88/118 [00:05<00:05,  5.59it/s]\u001b[A\n",
      "Iteration:  76%|███████████████████████▋       | 90/118 [00:05<00:03,  7.10it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▏      | 92/118 [00:05<00:02,  8.77it/s]\u001b[A\n",
      "Iteration:  80%|████████████████████████▋      | 94/118 [00:06<00:02, 10.50it/s]\u001b[A\n",
      "Iteration:  81%|█████████████████████████▏     | 96/118 [00:06<00:01, 12.17it/s]\u001b[A\n",
      "Iteration:  83%|█████████████████████████▋     | 98/118 [00:06<00:01, 13.70it/s]\u001b[A\n",
      "Iteration:  85%|█████████████████████████▍    | 100/118 [00:06<00:01, 14.99it/s]\u001b[A\n",
      "Iteration:  86%|█████████████████████████▉    | 102/118 [00:06<00:00, 16.09it/s]\u001b[A\n",
      "Iteration:  88%|██████████████████████████▍   | 104/118 [00:06<00:00, 16.96it/s]\u001b[A\n",
      "Iteration:  90%|██████████████████████████▉   | 106/118 [00:06<00:00, 17.62it/s]\u001b[A\n",
      "Iteration:  92%|███████████████████████████▍  | 108/118 [00:06<00:00, 18.08it/s]\u001b[A\n",
      "Iteration:  93%|███████████████████████████▉  | 110/118 [00:06<00:00, 18.43it/s]\u001b[A\n",
      "Iteration:  95%|████████████████████████████▍ | 112/118 [00:06<00:00, 18.70it/s]\u001b[A\n",
      "Iteration:  97%|████████████████████████████▉ | 114/118 [00:07<00:00, 18.89it/s]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████| 118/118 [00:07<00:00, 16.16it/s]\u001b[A\n",
      "Epoch:  96%|██████████████████████████████████▌ | 24/25 [03:00<00:07,  7.60s/it]\n",
      "Iteration:   0%|                                        | 0/118 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▌                               | 2/118 [00:00<00:05, 19.37it/s]\u001b[A\n",
      "Iteration:   3%|█                               | 4/118 [00:00<00:05, 19.37it/s]\u001b[A\n",
      "Iteration:   5%|█▋                              | 6/118 [00:00<00:05, 19.37it/s]\u001b[A\n",
      "Iteration:   7%|██▏                             | 8/118 [00:00<00:05, 19.39it/s]\u001b[A\n",
      "Iteration:   8%|██▋                            | 10/118 [00:00<00:05, 19.38it/s]\u001b[A\n",
      "Iteration:  10%|███▏                           | 12/118 [00:00<00:05, 19.38it/s]\u001b[A\n",
      "Iteration:  12%|███▋                           | 14/118 [00:00<00:05, 19.37it/s]\u001b[A\n",
      "Iteration:  14%|████▏                          | 16/118 [00:00<00:05, 19.37it/s]\u001b[A\n",
      "Iteration:  15%|████▋                          | 18/118 [00:00<00:05, 19.38it/s]\u001b[A\n",
      "Iteration:  17%|█████▎                         | 20/118 [00:01<00:05, 19.32it/s]\u001b[A\n",
      "Iteration:  19%|█████▊                         | 22/118 [00:01<00:04, 19.35it/s]\u001b[A\n",
      "Iteration:  20%|██████▎                        | 24/118 [00:01<00:04, 19.37it/s]\u001b[A\n",
      "Iteration:  22%|██████▊                        | 26/118 [00:01<00:04, 19.38it/s]\u001b[A\n",
      "Iteration:  24%|███████▎                       | 28/118 [00:01<00:04, 19.37it/s]\u001b[A\n",
      "Iteration:  25%|███████▉                       | 30/118 [00:01<00:04, 19.35it/s]\u001b[A\n",
      "Iteration:  27%|████████▍                      | 32/118 [00:01<00:04, 19.36it/s]\u001b[A\n",
      "Iteration:  29%|████████▉                      | 34/118 [00:01<00:04, 19.37it/s]\u001b[A\n",
      "Iteration:  31%|█████████▍                     | 36/118 [00:01<00:04, 19.38it/s]\u001b[A\n",
      "Iteration:  32%|█████████▉                     | 38/118 [00:01<00:04, 19.37it/s]\u001b[A\n",
      "Iteration:  34%|██████████▌                    | 40/118 [00:02<00:04, 19.38it/s]\u001b[A\n",
      "Iteration:  36%|███████████                    | 42/118 [00:02<00:03, 19.38it/s]\u001b[A\n",
      "Iteration:  37%|███████████▌                   | 44/118 [00:02<00:03, 19.38it/s]\u001b[A\n",
      "Iteration:  39%|████████████                   | 46/118 [00:02<00:03, 19.38it/s]\u001b[A\n",
      "Iteration:  41%|████████████▌                  | 48/118 [00:02<00:03, 19.38it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▏                 | 50/118 [00:02<00:03, 19.39it/s]\u001b[A\n",
      "Iteration:  44%|█████████████▋                 | 52/118 [00:02<00:03, 19.40it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▏                | 54/118 [00:02<00:03, 19.39it/s]\u001b[A\n",
      "Iteration:  47%|██████████████▋                | 56/118 [00:02<00:03, 19.40it/s]\u001b[A\n",
      "Iteration:  49%|███████████████▏               | 58/118 [00:02<00:03, 19.35it/s]\u001b[A\n",
      "Iteration:  51%|███████████████▊               | 60/118 [00:03<00:02, 19.34it/s]\u001b[A\n",
      "Iteration:  53%|████████████████▎              | 62/118 [00:03<00:02, 19.35it/s]\u001b[A\n",
      "Iteration:  54%|████████████████▊              | 64/118 [00:03<00:02, 19.35it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▎             | 66/118 [00:03<00:02, 19.36it/s]\u001b[A12/20/2023 22:26:54 - INFO - __main__ -   Loading features from cached file ./dataset/bert/classic/m6Am/cached_dev_m6Am_101_dnaprom\n",
      "12/20/2023 22:26:54 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/20/2023 22:26:54 - INFO - __main__ -     Num examples = 937\n",
      "12/20/2023 22:26:54 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                        | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|██████████▎                    | 10/30 [00:00<00:00, 92.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|████████████████████▋          | 20/30 [00:00<00:00, 93.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████| 30/30 [00:00<00:00, 93.48it/s]\u001b[A\u001b[A\n",
      "12/20/2023 22:26:54 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/20/2023 22:26:54 - INFO - __main__ -     acc = 0.8431163287086446\n",
      "12/20/2023 22:26:54 - INFO - __main__ -     auc = 0.893564808473725\n",
      "12/20/2023 22:26:54 - INFO - __main__ -     f1 = 0.8424703986532003\n",
      "12/20/2023 22:26:54 - INFO - __main__ -     mcc = 0.705708079797261\n",
      "12/20/2023 22:26:54 - INFO - __main__ -     precision = 0.8579398637538173\n",
      "12/20/2023 22:26:54 - INFO - __main__ -     recall = 0.8478404784732685\n",
      "/root/.conda/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "{\"eval_acc\": 0.8431163287086446, \"eval_f1\": 0.8424703986532003, \"eval_mcc\": 0.705708079797261, \"eval_auc\": 0.893564808473725, \"eval_precision\": 0.8579398637538173, \"eval_recall\": 0.8478404784732685, \"learning_rate\": 0.0, \"loss\": 0.004578100638755131, \"step\": 2900}\n",
      "12/20/2023 22:26:54 - INFO - transformers.configuration_utils -   Configuration saved in ./results/your_models/bert/classic/m6Am/checkpoint-2900/config.json\n",
      "12/20/2023 22:26:55 - INFO - transformers.modeling_utils -   Model weights saved in ./results/your_models/bert/classic/m6Am/checkpoint-2900/pytorch_model.bin\n",
      "12/20/2023 22:26:55 - INFO - __main__ -   Saving model checkpoint to ./results/your_models/bert/classic/m6Am/checkpoint-2900\n",
      "12/20/2023 22:26:55 - INFO - __main__ -   Saving optimizer and scheduler states to ./results/your_models/bert/classic/m6Am/checkpoint-2900\n",
      "\n",
      "Iteration:  58%|█████████████████▊             | 68/118 [00:04<00:11,  4.28it/s]\u001b[A\n",
      "Iteration:  59%|██████████████████▍            | 70/118 [00:04<00:08,  5.58it/s]\u001b[A\n",
      "Iteration:  61%|██████████████████▉            | 72/118 [00:04<00:06,  7.09it/s]\u001b[A\n",
      "Iteration:  63%|███████████████████▍           | 74/118 [00:05<00:05,  8.75it/s]\u001b[A\n",
      "Iteration:  64%|███████████████████▉           | 76/118 [00:05<00:04, 10.47it/s]\u001b[A\n",
      "Iteration:  66%|████████████████████▍          | 78/118 [00:05<00:03, 12.15it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████          | 80/118 [00:05<00:02, 13.67it/s]\u001b[A\n",
      "Iteration:  69%|█████████████████████▌         | 82/118 [00:05<00:02, 15.00it/s]\u001b[A\n",
      "Iteration:  71%|██████████████████████         | 84/118 [00:05<00:02, 16.09it/s]\u001b[A\n",
      "Iteration:  73%|██████████████████████▌        | 86/118 [00:05<00:01, 16.94it/s]\u001b[A\n",
      "Iteration:  75%|███████████████████████        | 88/118 [00:05<00:01, 17.60it/s]\u001b[A\n",
      "Iteration:  76%|███████████████████████▋       | 90/118 [00:05<00:01, 18.10it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▏      | 92/118 [00:05<00:01, 18.47it/s]\u001b[A\n",
      "Iteration:  80%|████████████████████████▋      | 94/118 [00:06<00:01, 18.74it/s]\u001b[A\n",
      "Iteration:  81%|█████████████████████████▏     | 96/118 [00:06<00:01, 18.93it/s]\u001b[A\n",
      "Iteration:  83%|█████████████████████████▋     | 98/118 [00:06<00:01, 19.07it/s]\u001b[A\n",
      "Iteration:  85%|█████████████████████████▍    | 100/118 [00:06<00:00, 19.17it/s]\u001b[A\n",
      "Iteration:  86%|█████████████████████████▉    | 102/118 [00:06<00:00, 19.23it/s]\u001b[A\n",
      "Iteration:  88%|██████████████████████████▍   | 104/118 [00:06<00:00, 19.28it/s]\u001b[A\n",
      "Iteration:  90%|██████████████████████████▉   | 106/118 [00:06<00:00, 19.30it/s]\u001b[A\n",
      "Iteration:  92%|███████████████████████████▍  | 108/118 [00:06<00:00, 19.33it/s]\u001b[A\n",
      "Iteration:  93%|███████████████████████████▉  | 110/118 [00:06<00:00, 19.35it/s]\u001b[A\n",
      "Iteration:  95%|████████████████████████████▍ | 112/118 [00:06<00:00, 19.33it/s]\u001b[A\n",
      "Iteration:  97%|████████████████████████████▉ | 114/118 [00:07<00:00, 19.33it/s]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████| 118/118 [00:07<00:00, 16.16it/s]\u001b[A\n",
      "Epoch: 100%|████████████████████████████████████| 25/25 [03:08<00:00,  7.53s/it]\n",
      "12/20/2023 22:26:58 - INFO - __main__ -    global_step = 2950, average loss = 0.07048398692242483\n",
      "12/20/2023 22:26:58 - INFO - __main__ -   Saving model checkpoint to ./results/your_models/bert/classic/m6Am\n",
      "12/20/2023 22:26:58 - INFO - transformers.configuration_utils -   Configuration saved in ./results/your_models/bert/classic/m6Am/config.json\n",
      "12/20/2023 22:26:58 - INFO - transformers.modeling_utils -   Model weights saved in ./results/your_models/bert/classic/m6Am/pytorch_model.bin\n",
      "12/20/2023 22:26:58 - INFO - transformers.configuration_utils -   loading configuration file ./results/your_models/bert/classic/m6Am/config.json\n",
      "12/20/2023 22:26:58 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": \"dnaprom\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"num_rnn_layer\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"rnn\": \"lstm\",\n",
      "  \"rnn_dropout\": 0.0,\n",
      "  \"rnn_hidden\": 768,\n",
      "  \"split\": 0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 69\n",
      "}\n",
      "\n",
      "12/20/2023 22:26:58 - INFO - transformers.modeling_utils -   loading weights file ./results/your_models/bert/classic/m6Am/pytorch_model.bin\n",
      "============================================================\n",
      "<class 'transformers.tokenization_dna.DNATokenizer'>\n",
      "12/20/2023 22:27:00 - INFO - transformers.tokenization_utils -   Model name './results/your_models/bert/classic/m6Am' not found in model shortcut name list (dna3, dna4, dna5, dna6). Assuming './results/your_models/bert/classic/m6Am' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "12/20/2023 22:27:00 - INFO - transformers.tokenization_utils -   Didn't find file ./results/your_models/bert/classic/m6Am/added_tokens.json. We won't load it.\n",
      "12/20/2023 22:27:00 - INFO - transformers.tokenization_utils -   loading file ./results/your_models/bert/classic/m6Am/vocab.txt\n",
      "12/20/2023 22:27:00 - INFO - transformers.tokenization_utils -   loading file None\n",
      "12/20/2023 22:27:00 - INFO - transformers.tokenization_utils -   loading file ./results/your_models/bert/classic/m6Am/special_tokens_map.json\n",
      "12/20/2023 22:27:00 - INFO - transformers.tokenization_utils -   loading file ./results/your_models/bert/classic/m6Am/tokenizer_config.json\n",
      "============================================================\n",
      "<class 'transformers.tokenization_dna.DNATokenizer'>\n",
      "12/20/2023 22:27:00 - INFO - transformers.tokenization_utils -   Model name './results/your_models/bert/classic/m6Am' not found in model shortcut name list (dna3, dna4, dna5, dna6). Assuming './results/your_models/bert/classic/m6Am' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "12/20/2023 22:27:00 - INFO - transformers.tokenization_utils -   Didn't find file ./results/your_models/bert/classic/m6Am/added_tokens.json. We won't load it.\n",
      "12/20/2023 22:27:00 - INFO - transformers.tokenization_utils -   loading file ./results/your_models/bert/classic/m6Am/vocab.txt\n",
      "12/20/2023 22:27:00 - INFO - transformers.tokenization_utils -   loading file None\n",
      "12/20/2023 22:27:00 - INFO - transformers.tokenization_utils -   loading file ./results/your_models/bert/classic/m6Am/special_tokens_map.json\n",
      "12/20/2023 22:27:00 - INFO - transformers.tokenization_utils -   loading file ./results/your_models/bert/classic/m6Am/tokenizer_config.json\n",
      "12/20/2023 22:27:00 - INFO - __main__ -   Evaluate the following checkpoints: ['./results/your_models/bert/classic/m6Am']\n",
      "12/20/2023 22:27:00 - INFO - transformers.configuration_utils -   loading configuration file ./results/your_models/bert/classic/m6Am/config.json\n",
      "12/20/2023 22:27:00 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": \"dnaprom\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"num_rnn_layer\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"rnn\": \"lstm\",\n",
      "  \"rnn_dropout\": 0.0,\n",
      "  \"rnn_hidden\": 768,\n",
      "  \"split\": 0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 69\n",
      "}\n",
      "\n",
      "12/20/2023 22:27:00 - INFO - transformers.modeling_utils -   loading weights file ./results/your_models/bert/classic/m6Am/pytorch_model.bin\n",
      "12/20/2023 22:27:01 - INFO - __main__ -   Loading features from cached file ./dataset/bert/classic/m6Am/cached_dev_m6Am_101_dnaprom\n",
      "12/20/2023 22:27:01 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "12/20/2023 22:27:01 - INFO - __main__ -     Num examples = 937\n",
      "12/20/2023 22:27:01 - INFO - __main__ -     Batch size = 32\n",
      "Evaluating: 100%|███████████████████████████████| 30/30 [00:00<00:00, 88.71it/s]\n",
      "12/20/2023 22:27:01 - INFO - __main__ -   ***** Eval results  *****\n",
      "12/20/2023 22:27:01 - INFO - __main__ -     acc = 0.8431163287086446\n",
      "12/20/2023 22:27:01 - INFO - __main__ -     auc = 0.893564808473725\n",
      "12/20/2023 22:27:01 - INFO - __main__ -     f1 = 0.8424703986532003\n",
      "12/20/2023 22:27:01 - INFO - __main__ -     mcc = 0.705708079797261\n",
      "12/20/2023 22:27:01 - INFO - __main__ -     precision = 0.8579398637538173\n",
      "12/20/2023 22:27:01 - INFO - __main__ -     recall = 0.8478404784732685\n"
     ]
    }
   ],
   "source": [
    "if train_type==\"normal\":\n",
    "    all_data=read_fasta(\"./dataset/CD-fasta/\"+mod_type+\".fasta\")\n",
    "    test_data=read_data(\"./dataset/test\",mod_type)\n",
    "    train_data, val_data = train_test_split(all_data, test_size=0.2, random_state=random_seed)    \n",
    "    def get_bert_data():\n",
    "        if os.path.exists(\"./dataset/bert/classic/\"+mod_type):\n",
    "            print(\"目录已存在\")\n",
    "        else:\n",
    "            os.mkdir(\"./dataset/bert/classic/\"+mod_type)\n",
    "        df=train_data.copy(deep=True)\n",
    "        df[\"sequence\"]=df[\"sequence\"].apply(seq2kmer)\n",
    "        df.to_csv(\"./dataset/bert/classic/\"+mod_type+\"/train.tsv\",sep=\"\\t\",index=None)\n",
    "        df=val_data.copy(deep=True)\n",
    "        df[\"sequence\"]=df[\"sequence\"].apply(seq2kmer)\n",
    "        df.to_csv(\"./dataset/bert/classic/\"+mod_type+\"/dev.tsv\",sep=\"\\t\",index=None)\n",
    "    get_bert_data()\n",
    "    if mod_type==\"hm6A\" or mod_type==\"Atol\":\n",
    "        eph=5\n",
    "    else:\n",
    "        eph=25\n",
    "    !python ./DNABERT/examples/run_finetune.py   --early_stop 10 --model_type dna  --tokenizer_name=dna3  --model_name_or_path ./models/bert/{mod_type}  --task_name dnaprom  --do_train  --do_eval  --data_dir ./dataset/bert/classic/{mod_type} --max_seq_length 101  --per_gpu_eval_batch_size=32    --per_gpu_train_batch_size=32   --learning_rate 3e-5  --num_train_epochs {str(eph)}  --output_dir ./results/your_models/bert/classic/{mod_type}  --evaluate_during_training  --logging_steps 100  --save_steps 100  --warmup_percent 0.1  --hidden_dropout_prob 0.1  --overwrite_output  --weight_decay 0.01  --n_process 40  {use_fp16}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "<class 'transformers.tokenization_dna.DNATokenizer'>\n",
      "============================================================\n",
      "<class 'transformers.tokenization_dna.DNATokenizer'>\n",
      "============================================================\n",
      "<class 'transformers.tokenization_dna.DNATokenizer'>\n",
      "select  3747  items for training...\n",
      "select  937  items for validating...\n",
      "select  100  items for testing...\n"
     ]
    }
   ],
   "source": [
    "from DNABERT.examples.transformers.data.processors.glue import DnaPromProcessor,glue_convert_examples_to_features\n",
    "from DNABERT.examples.transformers.data.processors.utils import InputExample\n",
    "from DNABERT.examples.run_finetune import TensorDataset\n",
    "from transformers import BertConfig,    DNATokenizer,    BertForSequenceClassification\n",
    "bert_train=pd.read_csv(\"./results/your_models/bert/classic/\"+mod_type+\"/eval_results.txt\",sep=\" \",header=None)\n",
    "bert_train['sum'] = bert_train[1] + bert_train[2]\n",
    "max_row = bert_train.loc[bert_train['sum'].idxmax()]\n",
    "model_name = \"./results/your_models/bert/classic/\"+mod_type+\"/checkpoint-\"+str((max_row.name+1)*100)\n",
    "bert=BertForSequenceClassification.from_pretrained(model_name)\n",
    "def tensor2np(tensor_list):\n",
    "    array_list = [tensor.cpu().numpy() for tensor in tensor_list]\n",
    "    array = np.concatenate(array_list, axis=0)\n",
    "    return array\n",
    "def tensor2np2(tensor_list):\n",
    "    array_list = [tensor.detach().cpu().numpy() for tensor in tensor_list]\n",
    "    array = np.concatenate(array_list, axis=0)\n",
    "    return array\n",
    "import itertools\n",
    "from collections import Counter\n",
    "def get_feature(df):\n",
    "    def CKSNAP(sequence, gap=3):\n",
    "        AA = 'ACGT'\n",
    "        code = []\n",
    "        aaPairs = []\n",
    "        for aa1 in AA:\n",
    "            for aa2 in AA:\n",
    "                aaPairs.append(aa1 + aa2)\n",
    "\n",
    "        header = []\n",
    "        for g in range(gap + 1):\n",
    "            for aa in aaPairs:\n",
    "                header.append(aa + '.gap' + str(g))\n",
    "        #encodings.append(header)\n",
    "\n",
    "\n",
    "        for g in range(gap + 1):\n",
    "            myDict = {}\n",
    "            for pair in aaPairs:\n",
    "                myDict[pair] = 0\n",
    "            sum = 0\n",
    "            for index1 in range(len(sequence)):\n",
    "                index2 = index1 + g + 1\n",
    "                if index1 < len(sequence) and index2 < len(sequence) and sequence[index1] in AA and sequence[\n",
    "                    index2] in AA:\n",
    "                    myDict[sequence[index1] + sequence[index2]] = myDict[sequence[index1] + sequence[index2]] + 1\n",
    "                    sum = sum + 1\n",
    "            for pair in aaPairs:\n",
    "                code.append(myDict[pair] / sum)\n",
    "        return code\n",
    "    def Kmer(sequence,k=3):\n",
    "        kmers = []\n",
    "        for i in range(len(sequence) - k + 1):\n",
    "            kmers.append(sequence[i:i + k])\n",
    "        code=[];header = [];count = Counter()\n",
    "        count.update(kmers)\n",
    "        for key in count:\n",
    "            count[key] = count[key] / len(kmers)\n",
    "        NA = 'ACGT'\n",
    "        for kmer in itertools.product(NA, repeat=3):\n",
    "            header.append(''.join(kmer))\n",
    "        for j in range(len(header)):\n",
    "                    if header[j] in count:\n",
    "                        code.append(count[header[j]])\n",
    "                    else:\n",
    "                        code.append(0)\n",
    "        return code\n",
    "    def binary(s):\n",
    "        Encode = {'A':[1,0,0,0],'C':[0,1,0,0],'G':[0,0,1,0],'U':[0,0,0,1],'T':[0,0,0,1],'N':[0,0,0,0]}\n",
    "        return np.array([Encode[x] for x in s])\n",
    "    def cal(c,cb,i):\n",
    "        bases ={'A':[1,1,1], 'C':[0,1,0], 'G':[1,0,0,], 'T':[0,0,1],'U':[0,0,1],'N':[0,0,0]}\n",
    "        p=[]\n",
    "        p=bases[c]\n",
    "        p.append(np.round(cb/float(i+1),2))\n",
    "        return(p)\n",
    "    def ENAC(sequence, window=5):\n",
    "        AA =  'ACGT'\n",
    "        code=[]\n",
    "        sequence=\"NN\"+sequence+\"NN\"\n",
    "        for j in range(len(sequence)):\n",
    "            if j < len(sequence) and j + window <= len(sequence):\n",
    "                count = Counter(sequence[j:j + window])\n",
    "                for key in count:\n",
    "                    count[key] = count[key] / len(sequence[j:j + window])\n",
    "                for aa in AA:\n",
    "                    code.append(count[aa])\n",
    "        return code\n",
    "    def EIIP(s):\n",
    "        dic = {'A':[0.1260],'C':[0.1340],'G':[0.0806],'T':[0.1335],'U':[0.1335],'N':[0]}\n",
    "        return np.array([dic[x] for x in s])\n",
    "    def calculate_chem(s):\n",
    "        p=f=list()\n",
    "        cba=cbc=cbt=cbg=0\n",
    "        for i,c in enumerate(s):\n",
    "            if c=='A':\n",
    "                cba+=1\n",
    "                p=cal(c,cba,i)\n",
    "            elif c=='T':\n",
    "                cbt+=1\n",
    "                p=cal(c,cbt,i)\n",
    "            elif c=='U':\n",
    "                cbt+=1\n",
    "                p=cal(c,cbt,i)\n",
    "            elif c=='C':\n",
    "                cbc+=1\n",
    "                p=cal(c,cbc,i)\n",
    "            elif c=='G':\n",
    "                cbg+=1\n",
    "                p=cal(c,cbg,i)\n",
    "            else:\n",
    "                p=[0,0,0,0]\n",
    "            f.append(p)\n",
    "        return(f)\n",
    "    one_hot=[];chem=[];eiip=[];enac=[];_kmer=[];cksnap=[]\n",
    "    for i in df.sequence.values:\n",
    "        one_hot.append(binary(i))\n",
    "        chem.append(calculate_chem(i))\n",
    "        eiip.append(EIIP(i))\n",
    "        enac.append(np.array(ENAC(i,window=5)).reshape(4,-1).T)\n",
    "        _kmer.append(Kmer(i,k=3))\n",
    "        cksnap.append(CKSNAP(i,gap=3))\n",
    "    one_hot=np.array(one_hot)\n",
    "    chem=np.array(chem)\n",
    "    eiip=np.array(eiip)\n",
    "    enac=np.array(enac)\n",
    "    _kmer=np.array(_kmer)\n",
    "    cksnap=np.array(cksnap)\n",
    "    return one_hot,chem,eiip,enac,_kmer,cksnap\n",
    "def create_examples(df, set_type='dev'):\n",
    "    examples = [];i=1\n",
    "    for line in df.values:\n",
    "        guid = \"%s-%s\" % (set_type, i)\n",
    "        text_a = seq2kmer(line[0])\n",
    "        label = str(line[1])\n",
    "        examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "        i=i+1\n",
    "    return examples\n",
    "def load_and_cache_examples(df):\n",
    "\n",
    "    tokenizer = DNATokenizer.from_pretrained(model_name)\n",
    "    processor = DnaPromProcessor()\n",
    "   \n",
    "    label_list = processor.get_labels()\n",
    "    examples = (create_examples(df))   \n",
    "\n",
    "    max_length = 101\n",
    "    pad_on_left = 0\n",
    "    pad_token = tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0]\n",
    "    \n",
    "    features = glue_convert_examples_to_features(\n",
    "    examples,\n",
    "    tokenizer,\n",
    "    label_list=label_list,\n",
    "    max_length=max_length,\n",
    "    output_mode=\"classification\",\n",
    "    pad_on_left=pad_on_left, \n",
    "    pad_token=pad_token,\n",
    "    pad_token_segment_id=0)\n",
    "            \n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
    "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
    "    all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
    "    return all_input_ids, all_attention_mask, all_token_type_ids, all_labels\n",
    "class MyDataset(Dataset):\n",
    "        def __init__(self,df):\n",
    "            W,X,Y,Z,M,N=get_feature(df)\n",
    "            self.w=torch.tensor(W,dtype=torch.float32)\n",
    "            self.x=torch.tensor(X,dtype=torch.float32)\n",
    "            self.y=torch.tensor(Y,dtype=torch.float32)\n",
    "            self.z=torch.tensor(Z,dtype=torch.float32)\n",
    "            self.m=torch.tensor(M,dtype=torch.float32)\n",
    "            self.n=torch.tensor(N,dtype=torch.float32)\n",
    "            self.len=X.shape[0] \n",
    "            self.label=df.label.values\n",
    "            self.token=load_and_cache_examples(df)\n",
    "        def __getitem__(self, index):\n",
    "        \n",
    "        \n",
    "            return (self.w[index],self.x[index],self.y[index],self.z[index],self.m[index],self.n[index]),torch.tensor(float(self.label[index]),dtype=torch.float32),tuple(tensor[index] for tensor in self.token)\n",
    "    \n",
    "        def __len__(self):\n",
    "            return self.len\n",
    "\n",
    "train_dataset = MyDataset(train_data)\n",
    "val_dataset = MyDataset(val_data)\n",
    "test_dataset = MyDataset(test_data)\n",
    "print(\"select \",len(train_dataset),\" items for training...\")\n",
    "print(\"select \",len(val_dataset),\" items for validating...\")\n",
    "print(\"select \",len(test_dataset),\" items for testing...\")\n",
    "train_dataloader=DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE,shuffle=False,num_workers=0,drop_last=True)\n",
    "val_dataloader=DataLoader(dataset=val_dataset,batch_size=BATCH_SIZE,shuffle=False,num_workers=0,drop_last=True)\n",
    "test_dataloader=DataLoader(dataset=test_dataset,batch_size=BATCH_SIZE,shuffle=False,num_workers=0,drop_last=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.conv1_1 = nn.Conv1d(4, 64, 3, stride=1, padding=1)\n",
    "        self.conv1_2 = nn.Conv1d(64, 32, 3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.max_pool1 = nn.MaxPool1d(2, stride=2)\n",
    "\n",
    "        self.conv2_1 = nn.Conv1d(4, 64, 3, stride=1, padding=1)\n",
    "        self.conv2_2 = nn.Conv1d(64, 32, 3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.max_pool2 = nn.MaxPool1d(2, stride=2)\n",
    "\n",
    "        self.conv3_1 = nn.Conv1d(1, 64, 3, stride=1, padding=1)\n",
    "        self.conv3_2 = nn.Conv1d(64, 16, 3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(16)\n",
    "        self.max_pool3 = nn.MaxPool1d(2, stride=2)\n",
    "\n",
    "        self.conv4_1 = nn.Conv1d(4, 64, 3, stride=1, padding=1)\n",
    "        self.conv4_2 = nn.Conv1d(64, 32, 3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm1d(32)\n",
    "        self.max_pool4 = nn.MaxPool1d(2, stride=2)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "\n",
    "        self.conv4 = nn.Conv1d(112, 32, 3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm1d(32)\n",
    "        self.max_pool4 = nn.MaxPool1d(2, stride=2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.conv5 = nn.Conv1d(32, 16, 3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm1d(16)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(16*25, 64)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(64+16, 32)\n",
    "        self.fc3 = nn.Linear(32, 2)\n",
    "\n",
    "        self.fc5 = nn.Linear(64, 32)\n",
    "        self.dropout5 = nn.Dropout(0.3)\n",
    "        self.bn6=nn.BatchNorm1d(32)\n",
    "        self.fc52 = nn.Linear(32, 8)\n",
    "        \n",
    "        self.fc6 = nn.Linear(64, 32)\n",
    "        self.dropout6 = nn.Dropout(0.3)\n",
    "        self.bn7=nn.BatchNorm1d(32)\n",
    "        self.fc62 = nn.Linear(32, 8)\n",
    "\n",
    "    def forward(self, x1, x2, x3, x4,x5,x6):\n",
    "        x1 = F.relu(self.conv1_1(x1))\n",
    "        x1 = F.relu(self.conv1_2(x1))\n",
    "        x1 = self.bn1(x1)\n",
    "        x1 = self.max_pool1(x1)\n",
    "\n",
    "        x2 = F.relu(self.conv2_1(x2))\n",
    "        x2 = F.relu(self.conv2_2(x2))\n",
    "        x2 = self.bn2(x2)\n",
    "        x2 = self.max_pool2(x2)\n",
    "\n",
    "        x3 = F.relu(self.conv3_1(x3))\n",
    "        x3 = F.relu(self.conv3_2(x3))\n",
    "        x3 = self.bn3(x3)\n",
    "        x3 = self.max_pool3(x3)\n",
    "\n",
    "        x4 = F.relu(self.conv4_1(x4))\n",
    "        x4 = F.relu(self.conv4_2(x4))\n",
    "        x4 = self.bn4(x4)\n",
    "        x4 = self.max_pool4(x4)\n",
    "\n",
    "        x5=self.fc52(self.dropout5(self.bn6(F.relu(self.fc5(x5)))))\n",
    "        x6=self.fc62(self.dropout6(self.bn7(F.relu(self.fc6(x6)))))\n",
    "\n",
    "        x = torch.cat((x1, x2, x3, x4), dim=1)\n",
    "\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.bn4(x)\n",
    "        x = self.max_pool4(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.bn5(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        \n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout3(x)\n",
    "        x=torch.cat((x,x5,x6),dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        \n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        \n",
    "\n",
    "        return x\n",
    "class Pre_Bert(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Pre_Bert, self).__init__()\n",
    "        self.bert=BertForSequenceClassification.from_pretrained(model_name)\n",
    "    def forward(self,a,b,c):\n",
    "        x = {\"input_ids\": a, \"attention_mask\": b, \"labels\": c}\n",
    "        return self.bert(**x)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomModel()\n",
    "net=model.to(device)\n",
    "pre_bert=Pre_Bert().to(device)\n",
    "optimizer=torch.optim.Adam(net.parameters(),lr=0.00005,weight_decay=0.001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=10,gamma=0.8)\n",
    "loss_fn=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda 0 to train...\n",
      " epoch 0 num: 1: val loss=0.01066817343235015916\n",
      " epoch 0: train loss:0.0053647271807080715   train_acc: 0.546875\n",
      "          val   loss:0.005334731058350631   val_acc   :0.6082589285714286\n",
      "          test   loss:0.00533708930015564   test_acc   :0.59\n",
      "best acc updated!\n",
      " epoch 1 num: 1: val loss=0.01058552321046590848\n",
      " epoch 1: train loss:0.0053311535960127564   train_acc: 0.5921336206896551\n",
      "          val   loss:0.005290666262486151   val_acc   :0.7020089285714286\n",
      "          test   loss:0.005295829847455025   test_acc   :0.71\n",
      "best acc updated!\n",
      " epoch 2 num: 1: val loss=0.0104438043199479582\n",
      " epoch 2: train loss:0.00527848727230368   train_acc: 0.6427801724137931\n",
      "          val   loss:0.005216419696807861   val_acc   :0.7611607142857143\n",
      "          test   loss:0.005227317102253437   test_acc   :0.76\n",
      "best acc updated!\n",
      " epoch 3 num: 1: val loss=0.01021041534841060639\n",
      " epoch 3: train loss:0.00518249256279448   train_acc: 0.7165948275862069\n",
      "          val   loss:0.005094335214900119   val_acc   :0.78125\n",
      "          test   loss:0.005114908795803785   test_acc   :0.77\n",
      "best acc updated!\n",
      " epoch 4 num: 1: val loss=0.00980583019554615772\n",
      " epoch 4: train loss:0.00504120307621257   train_acc: 0.7508081896551724\n",
      "          val   loss:0.004886394233575889   val_acc   :0.7912946428571429\n",
      "          test   loss:0.004916287027299404   test_acc   :0.82\n",
      "best acc updated!\n",
      " epoch 5 num: 1: val loss=0.00934504158794884073\n",
      " epoch 5: train loss:0.004843971643853804   train_acc: 0.7696659482758621\n",
      "          val   loss:0.00464534227337156   val_acc   :0.7912946428571429\n",
      "          test   loss:0.004684199579060078   test_acc   :0.82\n",
      " epoch 6 num: 1: val loss=0.00893717398867011724\n",
      " epoch 6: train loss:0.004646898375759865   train_acc: 0.7777478448275862\n",
      "          val   loss:0.004434450729084867   val_acc   :0.7957589285714286\n",
      "          test   loss:0.00448083970695734   test_acc   :0.8\n",
      "best acc updated!\n",
      " epoch 7 num: 1: val loss=0.00866376189514994621\n",
      " epoch 7: train loss:0.004489897027740191   train_acc: 0.7761314655172413\n",
      "          val   loss:0.004292373412421772   val_acc   :0.8013392857142857\n",
      "          test   loss:0.004326098598539829   test_acc   :0.83\n",
      "best acc updated!\n",
      " epoch 8 num: 1: val loss=0.00846330588683486768\n",
      " epoch 8: train loss:0.0043488884633728145   train_acc: 0.7834051724137931\n",
      "          val   loss:0.004193331314516919   val_acc   :0.8069196428571429\n",
      "          test   loss:0.004203475080430508   test_acc   :0.82\n",
      "best acc updated!\n",
      " epoch 9 num: 1: val loss=0.00830119848251342865\n",
      " epoch 9: train loss:0.004255703104467228   train_acc: 0.7898706896551724\n",
      "          val   loss:0.004117211792618036   val_acc   :0.8091517857142857\n",
      "          test   loss:0.004099190700799227   test_acc   :0.82\n",
      "best acc updated!\n",
      " epoch 10 num: 1: val loss=0.00819266075268387815\n",
      " epoch 10: train loss:0.004154699180146743   train_acc: 0.7995689655172413\n",
      "          val   loss:0.004062131978571415   val_acc   :0.8102678571428571\n",
      "          test   loss:0.004002725705504417   test_acc   :0.82\n",
      "best acc updated!\n",
      " epoch 11 num: 1: val loss=0.00802830792963504816\n",
      " epoch 11: train loss:0.00408390406037456   train_acc: 0.8022629310344828\n",
      "          val   loss:0.00398899322109563   val_acc   :0.8158482142857143\n",
      "          test   loss:0.0039152768440544605   test_acc   :0.83\n",
      "best acc updated!\n",
      " epoch 12 num: 1: val loss=0.00790022057481110144\n",
      " epoch 12: train loss:0.004041057947123873   train_acc: 0.8095366379310345\n",
      "          val   loss:0.00393464050388762   val_acc   :0.8136160714285714\n",
      "          test   loss:0.003843659069389105   test_acc   :0.84\n",
      " epoch 13 num: 1: val loss=0.007853629067540169785\n",
      " epoch 13: train loss:0.003989780233402191   train_acc: 0.8141163793103449\n",
      "          val   loss:0.003912362735718489   val_acc   :0.8136160714285714\n",
      "          test   loss:0.0037840339355170727   test_acc   :0.85\n",
      " epoch 14 num: 1: val loss=0.00776007538661360764\n",
      " epoch 14: train loss:0.003940177615731955   train_acc: 0.8151939655172413\n",
      "          val   loss:0.0038754878831761225   val_acc   :0.8169642857142857\n",
      "          test   loss:0.003728604642674327   test_acc   :0.85\n",
      "best acc updated!\n",
      " epoch 15 num: 1: val loss=0.00768404174596071275\n",
      " epoch 15: train loss:0.003903322917377127   train_acc: 0.8170797413793104\n",
      "          val   loss:0.0038441174464034183   val_acc   :0.8191964285714286\n",
      "          test   loss:0.003678933484479785   test_acc   :0.85\n",
      "best acc updated!\n",
      " epoch 16 num: 1: val loss=0.0076290471479296681\n",
      " epoch 16: train loss:0.00387576206362453   train_acc: 0.8208512931034483\n",
      "          val   loss:0.0038232094874339445   val_acc   :0.8225446428571429\n",
      "          test   loss:0.0036341706290841103   test_acc   :0.85\n",
      "best acc updated!\n",
      " epoch 17 num: 1: val loss=0.007566004060208797585\n",
      " epoch 17: train loss:0.0038529598587674313   train_acc: 0.8205818965517241\n",
      "          val   loss:0.003796107568112867   val_acc   :0.8225446428571429\n",
      "          test   loss:0.0036041676066815853   test_acc   :0.85\n",
      " epoch 18 num: 1: val loss=0.00752459676004946247\n",
      " epoch 18: train loss:0.0038294397808354475   train_acc: 0.8219288793103449\n",
      "          val   loss:0.003779434066798006   val_acc   :0.8270089285714286\n",
      "          test   loss:0.0035851201973855495   test_acc   :0.85\n",
      "best acc updated!\n",
      " epoch 19 num: 1: val loss=0.007509744260460138724\n",
      " epoch 19: train loss:0.0037999717149369673   train_acc: 0.8286637931034483\n",
      "          val   loss:0.00376891324828778   val_acc   :0.8258928571428571\n",
      "          test   loss:0.003545221174135804   test_acc   :0.85\n",
      " epoch 20 num: 1: val loss=0.00747212395071983398\n",
      " epoch 20: train loss:0.003795464210019543   train_acc: 0.828125\n",
      "          val   loss:0.0037561828669692788   val_acc   :0.8270089285714286\n",
      "          test   loss:0.00351768103428185   test_acc   :0.86\n",
      " epoch 21 num: 1: val loss=0.00738862808793783212\n",
      " epoch 21: train loss:0.0037907480727881193   train_acc: 0.828125\n",
      "          val   loss:0.003723453065114362   val_acc   :0.8303571428571429\n",
      "          test   loss:0.0034889965318143368   test_acc   :0.88\n",
      "best acc updated!\n",
      " epoch 22 num: 1: val loss=0.00731858541257679569\n",
      " epoch 22: train loss:0.0037525059687423296   train_acc: 0.8324353448275862\n",
      "          val   loss:0.003704864132617201   val_acc   :0.8337053571428571\n",
      "          test   loss:0.0034908733796328306   test_acc   :0.88\n",
      "best acc updated!\n",
      " epoch 23 num: 1: val loss=0.007289177272468805425\n",
      " epoch 23: train loss:0.0037236998724783288   train_acc: 0.8356681034482759\n",
      "          val   loss:0.0036963744621191707   val_acc   :0.8359375\n",
      "          test   loss:0.0034652361646294594   test_acc   :0.88\n",
      "best acc updated!\n",
      " epoch 24 num: 1: val loss=0.00728776748292148153\n",
      " epoch 24: train loss:0.003712786000286197   train_acc: 0.8362068965517241\n",
      "          val   loss:0.0036843844157244477   val_acc   :0.8404017857142857\n",
      "          test   loss:0.0034455100540071726   test_acc   :0.89\n",
      "best acc updated!\n",
      " epoch 25 num: 1: val loss=0.00723617547191679589\n",
      " epoch 25: train loss:0.003704535710657465   train_acc: 0.8353987068965517\n",
      "          val   loss:0.0036728450296712773   val_acc   :0.8426339285714286\n",
      "          test   loss:0.003446491900831461   test_acc   :0.88\n",
      "best acc updated!\n",
      " epoch 26 num: 1: val loss=0.00718776299618184687\n",
      " epoch 26: train loss:0.003694739096380513   train_acc: 0.8402478448275862\n",
      "          val   loss:0.0036607504090560333   val_acc   :0.8426339285714286\n",
      "          test   loss:0.0034439093433320522   test_acc   :0.88\n",
      " epoch 27 num: 1: val loss=0.00717806373722851307\n",
      " epoch 27: train loss:0.0036781677044928074   train_acc: 0.8421336206896551\n",
      "          val   loss:0.0036498047411441803   val_acc   :0.8426339285714286\n",
      "          test   loss:0.0034523147623986006   test_acc   :0.87\n",
      " epoch 28 num: 1: val loss=0.0071530528366565704\n",
      " epoch 28: train loss:0.0036746805116277315   train_acc: 0.8407866379310345\n",
      "          val   loss:0.003653076311041202   val_acc   :0.8381696428571429\n",
      "          test   loss:0.003471361007541418   test_acc   :0.88\n",
      " epoch 29 num: 1: val loss=0.00714598037302494057\n",
      " epoch 29: train loss:0.003660102270479346   train_acc: 0.8469827586206896\n",
      "          val   loss:0.0036558914663536207   val_acc   :0.8415178571428571\n",
      "          test   loss:0.0034532686695456505   test_acc   :0.88\n",
      " epoch 30 num: 1: val loss=0.00715243420563638281\n",
      " epoch 30: train loss:0.0036775937368129863   train_acc: 0.8415948275862069\n",
      "          val   loss:0.003666084891717349   val_acc   :0.8370535714285714\n",
      "          test   loss:0.0035287782084196806   test_acc   :0.86\n",
      " epoch 31 num: 1: val loss=0.00710168853402137764\n",
      " epoch 31: train loss:0.0036492628157781116   train_acc: 0.8472521551724138\n",
      "          val   loss:0.003642671275883913   val_acc   :0.8370535714285714\n",
      "          test   loss:0.0035182798746973276   test_acc   :0.86\n",
      " epoch 32 num: 1: val loss=0.0071303192526102077\n",
      " epoch 32: train loss:0.0036586641157367104   train_acc: 0.8424030172413793\n",
      "          val   loss:0.0036618657343621764   val_acc   :0.8392857142857143\n",
      "          test   loss:0.0035556491930037737   test_acc   :0.85\n",
      " epoch 33 num: 1: val loss=0.00714690913446247664\n",
      " epoch 33: train loss:0.0036349788966492332   train_acc: 0.845635775862069\n",
      "          val   loss:0.00367269036360085   val_acc   :0.8392857142857143\n",
      "          test   loss:0.003605411620810628   test_acc   :0.84\n",
      " epoch 34 num: 1: val loss=0.00707323756068944962\n",
      " epoch 34: train loss:0.003614268980213794   train_acc: 0.8515625\n",
      "          val   loss:0.00363604022589113   val_acc   :0.8370535714285714\n",
      "          test   loss:0.003493664553388953   test_acc   :0.86\n",
      " epoch 35 num: 1: val loss=0.00708344089798629379\n",
      " epoch 35: train loss:0.0036027561873197556   train_acc: 0.8521012931034483\n",
      "          val   loss:0.0036499230856341974   val_acc   :0.8348214285714286\n",
      "          test   loss:0.003492036135867238   test_acc   :0.87\n",
      " epoch 36 num: 1: val loss=0.00713070388883352399\n",
      " epoch 36: train loss:0.003616183707169418   train_acc: 0.849676724137931\n",
      "          val   loss:0.003671636206230947   val_acc   :0.8381696428571429\n",
      "          test   loss:0.003591520246118307   test_acc   :0.85\n",
      " epoch 37 num: 1: val loss=0.00704201497137546596\n",
      " epoch 37: train loss:0.003602454230060865   train_acc: 0.8521012931034483\n",
      "          val   loss:0.0036285872504647288   val_acc   :0.8404017857142857\n",
      "          test   loss:0.003476655576378107   test_acc   :0.87\n",
      " epoch 38 num: 1: val loss=0.00706915557384491514\n",
      " epoch 38: train loss:0.0036024271574770584   train_acc: 0.8491379310344828\n",
      "          val   loss:0.0036371475351708277   val_acc   :0.8415178571428571\n",
      "          test   loss:0.003532091621309519   test_acc   :0.86\n",
      " epoch 39 num: 1: val loss=0.0070317897479981187\n",
      " epoch 39: train loss:0.0036043042461548387   train_acc: 0.8480603448275862\n",
      "          val   loss:0.0036110980436205864   val_acc   :0.8415178571428571\n",
      "          test   loss:0.003474833909422159   test_acc   :0.87\n",
      " epoch 40 num: 1: val loss=0.00711951870471239113\n",
      " epoch 40: train loss:0.003563492968357329   train_acc: 0.8537176724137931\n",
      "          val   loss:0.003656163179714765   val_acc   :0.8426339285714286\n",
      "          test   loss:0.0035883253440260887   test_acc   :0.84\n",
      " epoch 41 num: 1: val loss=0.00700924010016024117\n",
      " epoch 41: train loss:0.0035836095359690233   train_acc: 0.8523706896551724\n",
      "          val   loss:0.0036086669923471554   val_acc   :0.8448660714285714\n",
      "          test   loss:0.0034600982908159494   test_acc   :0.87\n",
      "best acc updated!\n",
      " epoch 42 num: 1: val loss=0.00699267792515456754\n",
      " epoch 42: train loss:0.0035863450310867407   train_acc: 0.8545258620689655\n",
      "          val   loss:0.0036062063383204596   val_acc   :0.8470982142857143\n",
      "          test   loss:0.003476969664916396   test_acc   :0.87\n",
      "best acc updated!\n",
      " epoch 43 num: 1: val loss=0.00705823977477848509\n",
      " epoch 43: train loss:0.0035650923944495873   train_acc: 0.8556034482758621\n",
      "          val   loss:0.003633866219648293   val_acc   :0.8381696428571429\n",
      "          test   loss:0.0035670108627527952   test_acc   :0.86\n",
      " epoch 44 num: 1: val loss=0.00700471131131053255\n",
      " epoch 44: train loss:0.003565285675998392   train_acc: 0.853448275862069\n",
      "          val   loss:0.0036124452349862884   val_acc   :0.8426339285714286\n",
      "          test   loss:0.0035133259370923042   test_acc   :0.86\n",
      " epoch 45 num: 1: val loss=0.00700879492796957555\n",
      " epoch 45: train loss:0.0035668082438923164   train_acc: 0.8547952586206896\n",
      "          val   loss:0.0036055221488433225   val_acc   :0.84375\n",
      "          test   loss:0.0034618221689015627   test_acc   :0.87\n",
      " epoch 46 num: 1: val loss=0.00700450548902154026\n",
      " epoch 46: train loss:0.0035716703900232397   train_acc: 0.8558728448275862\n",
      "          val   loss:0.0036107058902936323   val_acc   :0.8392857142857143\n",
      "          test   loss:0.0034552665892988443   test_acc   :0.87\n",
      " epoch 47 num: 1: val loss=0.00695313676260411737\n",
      " epoch 47: train loss:0.003563000635918358   train_acc: 0.8569504310344828\n",
      "          val   loss:0.0035946308967790435   val_acc   :0.8426339285714286\n",
      "          test   loss:0.003490561619400978   test_acc   :0.85\n",
      " epoch 48 num: 1: val loss=0.00698639778420329109\n",
      " epoch 48: train loss:0.0035527017898857594   train_acc: 0.8537176724137931\n",
      "          val   loss:0.003599359620628612   val_acc   :0.8470982142857143\n",
      "          test   loss:0.0035705682821571827   test_acc   :0.84\n",
      " epoch 49 num: 1: val loss=0.00700828596018254871\n",
      " epoch 49: train loss:0.0035505788400769234   train_acc: 0.8569504310344828\n",
      "          val   loss:0.0036091927904635668   val_acc   :0.8448660714285714\n",
      "          test   loss:0.003586284816265106   test_acc   :0.83\n",
      " epoch 50 num: 1: val loss=0.00697876629419624829\n",
      " epoch 50: train loss:0.0035368266097944357   train_acc: 0.8572198275862069\n",
      "          val   loss:0.003585967268528683   val_acc   :0.8470982142857143\n",
      "          test   loss:0.0034898826852440834   test_acc   :0.86\n",
      " epoch 51 num: 1: val loss=0.00698993215337395751\n",
      " epoch 51: train loss:0.003546504560729553   train_acc: 0.8572198275862069\n",
      "          val   loss:0.003584139947114246   val_acc   :0.8537946428571429\n",
      "          test   loss:0.003556022886186838   test_acc   :0.85\n",
      "best acc updated!\n",
      " epoch 52 num: 1: val loss=0.00696062389761209591\n",
      " epoch 52: train loss:0.003547759132523989   train_acc: 0.8556034482758621\n",
      "          val   loss:0.0035668398985373123   val_acc   :0.8549107142857143\n",
      "          test   loss:0.003607135498896241   test_acc   :0.84\n",
      "best acc updated!\n",
      " epoch 53 num: 1: val loss=0.00696826912462711397\n",
      " epoch 53: train loss:0.0035215967238463207   train_acc: 0.861260775862069\n",
      "          val   loss:0.0035733425257993595   val_acc   :0.8504464285714286\n",
      "          test   loss:0.003518233308568597   test_acc   :0.86\n",
      " epoch 54 num: 1: val loss=0.00696090469136834142\n",
      " epoch 54: train loss:0.0035105831921100616   train_acc: 0.8647629310344828\n",
      "          val   loss:0.003587328961917332   val_acc   :0.8526785714285714\n",
      "          test   loss:0.0035550224129110575   test_acc   :0.85\n",
      " epoch 55 num: 1: val loss=0.00693979370407760179\n",
      " epoch 55: train loss:0.0035235816853698984   train_acc: 0.8566810344827587\n",
      "          val   loss:0.003577413203726922   val_acc   :0.8526785714285714\n",
      "          test   loss:0.003493271069601178   test_acc   :0.86\n",
      " epoch 56 num: 1: val loss=0.00692542735487222782\n",
      " epoch 56: train loss:0.0035134869836399266   train_acc: 0.8620689655172413\n",
      "          val   loss:0.003564418459843312   val_acc   :0.8515625\n",
      "          test   loss:0.003467096947133541   test_acc   :0.86\n",
      " epoch 57 num: 1: val loss=0.00694787967950105772\n",
      " epoch 57: train loss:0.0035106814386130407   train_acc: 0.8642241379310345\n",
      "          val   loss:0.0035649037786892484   val_acc   :0.8504464285714286\n",
      "          test   loss:0.003445289097726345   test_acc   :0.88\n",
      " epoch 58 num: 1: val loss=0.00698271580040454983\n",
      " epoch 58: train loss:0.0035174087107438467   train_acc: 0.8617995689655172\n",
      "          val   loss:0.003575611294114164   val_acc   :0.8448660714285714\n",
      "          test   loss:0.003405812429264188   test_acc   :0.88\n",
      " epoch 59 num: 1: val loss=0.00695624877698719576\n",
      " epoch 59: train loss:0.0034952730598763145   train_acc: 0.8639547413793104\n",
      "          val   loss:0.0035694361930446966   val_acc   :0.8515625\n",
      "          test   loss:0.0034099724143743515   test_acc   :0.89\n",
      " epoch 60 num: 1: val loss=0.00694484938867390161\n",
      " epoch 60: train loss:0.003493370287569946   train_acc: 0.8642241379310345\n",
      "          val   loss:0.0035562203930956976   val_acc   :0.8526785714285714\n",
      "          test   loss:0.003455106634646654   test_acc   :0.88\n",
      " epoch 61 num: 1: val loss=0.0069207467604428531\n",
      " epoch 61: train loss:0.0034782824524003885   train_acc: 0.865301724137931\n",
      "          val   loss:0.003555996942200831   val_acc   :0.8493303571428571\n",
      "          test   loss:0.0034449321683496237   test_acc   :0.88\n",
      " epoch 62 num: 1: val loss=0.00692948279902339153\n",
      " epoch 62: train loss:0.003456493614819543   train_acc: 0.8704202586206896\n",
      "          val   loss:0.0035550496208348443   val_acc   :0.859375\n",
      "          test   loss:0.003534897929057479   test_acc   :0.85\n",
      "best acc updated!\n",
      " epoch 63 num: 1: val loss=0.00691529852338135277\n",
      " epoch 63: train loss:0.003490892479774253   train_acc: 0.8642241379310345\n",
      "          val   loss:0.003554252441972494   val_acc   :0.8537946428571429\n",
      "          test   loss:0.0034479149617254734   test_acc   :0.87\n",
      " epoch 64 num: 1: val loss=0.00694450992159545428\n",
      " epoch 64: train loss:0.0034693371713289925   train_acc: 0.8677262931034483\n",
      "          val   loss:0.003571141178586653   val_acc   :0.8415178571428571\n",
      "          test   loss:0.0033996549900621176   test_acc   :0.88\n",
      " epoch 65 num: 1: val loss=0.00691401609219610795\n",
      " epoch 65: train loss:0.0034612567115831993   train_acc: 0.8696120689655172\n",
      "          val   loss:0.0035593360995075534   val_acc   :0.8571428571428571\n",
      "          test   loss:0.0035252957604825497   test_acc   :0.85\n",
      " epoch 66 num: 1: val loss=0.00692196609452366842\n",
      " epoch 66: train loss:0.0034848631696839786   train_acc: 0.8647629310344828\n",
      "          val   loss:0.0035615963861346245   val_acc   :0.8504464285714286\n",
      "          test   loss:0.003421693341806531   test_acc   :0.88\n",
      " epoch 67 num: 1: val loss=0.00689056748524308292\n",
      " epoch 67: train loss:0.0034738305699208685   train_acc: 0.8666487068965517\n",
      "          val   loss:0.0035603923856147696   val_acc   :0.8560267857142857\n",
      "          test   loss:0.003424958325922489   test_acc   :0.86\n",
      " epoch 68 num: 1: val loss=0.00691067101433873285\n",
      " epoch 68: train loss:0.0034609088545729375   train_acc: 0.8671875\n",
      "          val   loss:0.0035626519404883894   val_acc   :0.8537946428571429\n",
      "          test   loss:0.003415555926039815   test_acc   :0.88\n",
      " epoch 69 num: 1: val loss=0.00692147528752684603\n",
      " epoch 69: train loss:0.0034699900926829412   train_acc: 0.8677262931034483\n",
      "          val   loss:0.0035611571012330906   val_acc   :0.8526785714285714\n",
      "          test   loss:0.0033909508492797613   test_acc   :0.88\n",
      " epoch 70 num: 1: val loss=0.00692257704213261698\n",
      " epoch 70: train loss:0.003460198857596722   train_acc: 0.869073275862069\n",
      "          val   loss:0.003548345761373639   val_acc   :0.8582589285714286\n",
      "          test   loss:0.0034877448342740536   test_acc   :0.85\n",
      " epoch 71 num: 1: val loss=0.00692246900871396105\n",
      " epoch 71: train loss:0.0034330355822397717   train_acc: 0.873114224137931\n",
      "          val   loss:0.0035576026753655504   val_acc   :0.8537946428571429\n",
      "          test   loss:0.003504770342260599   test_acc   :0.85\n",
      " epoch 72 num: 1: val loss=0.00696465303190052585\n",
      " epoch 72: train loss:0.003488012083709754   train_acc: 0.8636853448275862\n",
      "          val   loss:0.0035765154752880335   val_acc   :0.8504464285714286\n",
      "          test   loss:0.0033580916933715343   test_acc   :0.89\n",
      " epoch 73 num: 1: val loss=0.00693799625150859361\n",
      " epoch 73: train loss:0.003459765086078952   train_acc: 0.8682650862068966\n",
      "          val   loss:0.0035467660387179683   val_acc   :0.8549107142857143\n",
      "          test   loss:0.0034075293224304914   test_acc   :0.88\n",
      " epoch 74 num: 1: val loss=0.00691588968038559765\n",
      " epoch 74: train loss:0.0034676234575052715   train_acc: 0.865301724137931\n",
      "          val   loss:0.003545676690659353   val_acc   :0.8571428571428571\n",
      "          test   loss:0.003422932932153344   test_acc   :0.88\n",
      " epoch 75 num: 1: val loss=0.006907696835696697935\n",
      " epoch 75: train loss:0.003454377344841587   train_acc: 0.8723060344827587\n",
      "          val   loss:0.003536229520770056   val_acc   :0.8571428571428571\n",
      "          test   loss:0.003435098798945546   test_acc   :0.87\n",
      " epoch 76 num: 1: val loss=0.00688832090236246606\n",
      " epoch 76: train loss:0.003438598885811095   train_acc: 0.8706896551724138\n",
      "          val   loss:0.003524917543732694   val_acc   :0.8560267857142857\n",
      "          test   loss:0.003419108921661973   test_acc   :0.88\n",
      " epoch 77 num: 1: val loss=0.00685664149932563314\n",
      " epoch 77: train loss:0.003435781771509812   train_acc: 0.8712284482758621\n",
      "          val   loss:0.0035272295187626567   val_acc   :0.859375\n",
      "          test   loss:0.003416358260437846   test_acc   :0.88\n",
      " epoch 78 num: 1: val loss=0.00683908490464091386\n",
      " epoch 78: train loss:0.0034314911443226294   train_acc: 0.8720366379310345\n",
      "          val   loss:0.0035329098214528392   val_acc   :0.8604910714285714\n",
      "          test   loss:0.0034296850208193064   test_acc   :0.88\n",
      "best acc updated!\n",
      " epoch 79 num: 1: val loss=0.00683542178012430744\n",
      " epoch 79: train loss:0.0034486008005152487   train_acc: 0.8674568965517241\n",
      "          val   loss:0.0035324693391365664   val_acc   :0.8604910714285714\n",
      "          test   loss:0.0034394210670143366   test_acc   :0.87\n",
      " epoch 80 num: 1: val loss=0.00687906402163207597\n",
      " epoch 80: train loss:0.0034305359606213613   train_acc: 0.8723060344827587\n",
      "          val   loss:0.003535598250372069   val_acc   :0.8560267857142857\n",
      "          test   loss:0.0034437081776559353   test_acc   :0.87\n",
      " epoch 81 num: 1: val loss=0.00691254017874598531\n",
      " epoch 81: train loss:0.003414967378344515   train_acc: 0.8755387931034483\n",
      "          val   loss:0.003534336740683232   val_acc   :0.859375\n",
      "          test   loss:0.003517009085044265   test_acc   :0.85\n",
      " epoch 82 num: 1: val loss=0.00689538824371993534\n",
      " epoch 82: train loss:0.003414816383657784   train_acc: 0.8736530172413793\n",
      "          val   loss:0.0035378208184348686   val_acc   :0.8560267857142857\n",
      "          test   loss:0.003555449889972806   test_acc   :0.85\n",
      " epoch 83 num: 1: val loss=0.00688889180310070558\n",
      " epoch 83: train loss:0.0034140542968465336   train_acc: 0.8744612068965517\n",
      "          val   loss:0.003535296202504209   val_acc   :0.8560267857142857\n",
      "          test   loss:0.0035504112020134926   test_acc   :0.85\n",
      " epoch 84 num: 1: val loss=0.00688334996812045622\n",
      " epoch 84: train loss:0.0034055783782668157   train_acc: 0.8774245689655172\n",
      "          val   loss:0.003528276358598045   val_acc   :0.8571428571428571\n",
      "          test   loss:0.0035665857139974833   test_acc   :0.85\n",
      " epoch 85 num: 1: val loss=0.00689695682376623153\n",
      " epoch 85: train loss:0.0033844775472093246   train_acc: 0.8795797413793104\n",
      "          val   loss:0.0035487995150366   val_acc   :0.8537946428571429\n",
      "          test   loss:0.0034203962422907352   test_acc   :0.87\n",
      " epoch 86 num: 1: val loss=0.00690661603584885627\n",
      " epoch 86: train loss:0.003426647118838697   train_acc: 0.8717672413793104\n",
      "          val   loss:0.003531553815784199   val_acc   :0.859375\n",
      "          test   loss:0.0034968482796102762   test_acc   :0.86\n",
      " epoch 87 num: 1: val loss=0.00689259497448802565\n",
      " epoch 87: train loss:0.0034242491799824195   train_acc: 0.8704202586206896\n",
      "          val   loss:0.003523085532443864   val_acc   :0.859375\n",
      "          test   loss:0.0035218042321503162   test_acc   :0.86\n",
      " epoch 88 num: 1: val loss=0.00687279715202748815\n",
      " epoch 88: train loss:0.0034114840793712385   train_acc: 0.8744612068965517\n",
      "          val   loss:0.003521079762971827   val_acc   :0.859375\n",
      "          test   loss:0.0035014592576771975   test_acc   :0.86\n",
      " epoch 89 num: 1: val loss=0.00692350091412663552\n",
      " epoch 89: train loss:0.0033909339891297035   train_acc: 0.8801185344827587\n",
      "          val   loss:0.003521643445960113   val_acc   :0.8582589285714286\n",
      "          test   loss:0.0035969505552202463   test_acc   :0.85\n",
      " epoch 90 num: 1: val loss=0.00688482029363513291\n",
      " epoch 90: train loss:0.0033877043631570093   train_acc: 0.8790409482758621\n",
      "          val   loss:0.003522345829488976   val_acc   :0.859375\n",
      "          test   loss:0.003500057151541114   test_acc   :0.86\n",
      " epoch 91 num: 1: val loss=0.00688101816922426218\n",
      " epoch 91: train loss:0.003393686753858266   train_acc: 0.8766163793103449\n",
      "          val   loss:0.003530855057761073   val_acc   :0.8560267857142857\n",
      "          test   loss:0.0035102800466120243   test_acc   :0.86\n",
      " epoch 92 num: 1: val loss=0.00689727277494967509\n",
      " epoch 92: train loss:0.003381524676585506   train_acc: 0.8779633620689655\n",
      "          val   loss:0.003527589508199266   val_acc   :0.8560267857142857\n",
      "          test   loss:0.0034737286623567343   test_acc   :0.86\n",
      " epoch 93 num: 1: val loss=0.006914421683177352765\n",
      " epoch 93: train loss:0.003408864485742203   train_acc: 0.873114224137931\n",
      "          val   loss:0.003526240387665374   val_acc   :0.8582589285714286\n",
      "          test   loss:0.0035253153182566166   test_acc   :0.86\n",
      " epoch 94 num: 1: val loss=0.0069355312734842301\n",
      " epoch 94: train loss:0.0034033969959950654   train_acc: 0.8755387931034483\n",
      "          val   loss:0.0035415994602122475   val_acc   :0.8549107142857143\n",
      "          test   loss:0.003458596533164382   test_acc   :0.85\n",
      " epoch 95 num: 1: val loss=0.00690079201012849891\n",
      " epoch 95: train loss:0.0034188922285905174   train_acc: 0.8760775862068966\n",
      "          val   loss:0.003522722549470408   val_acc   :0.8560267857142857\n",
      "          test   loss:0.0034901127219200134   test_acc   :0.86\n",
      " epoch 96 num: 1: val loss=0.0069161737337708473\n",
      " epoch 96: train loss:0.0034132854097747595   train_acc: 0.873114224137931\n",
      "          val   loss:0.0035230080663625684   val_acc   :0.8571428571428571\n",
      "          test   loss:0.00345015455968678   test_acc   :0.86\n",
      " epoch 97 num: 1: val loss=0.00690720719285309356\n",
      " epoch 97: train loss:0.003388314194784596   train_acc: 0.875\n",
      "          val   loss:0.0035243298791881117   val_acc   :0.8560267857142857\n",
      "          test   loss:0.0034503296483308077   test_acc   :0.86\n",
      " epoch 98 num: 1: val loss=0.0069020742084831097\n",
      " epoch 98: train loss:0.003403137124882176   train_acc: 0.8752693965517241\n",
      "          val   loss:0.0035189190613371985   val_acc   :0.859375\n",
      "          test   loss:0.0034523140639066696   test_acc   :0.85\n",
      " epoch 99 num: 1: val loss=0.00688269059173762896\n",
      " epoch 99: train loss:0.0033758617295273417   train_acc: 0.8793103448275862\n",
      "          val   loss:0.0035214299402598825   val_acc   :0.8604910714285714\n",
      "          test   loss:0.0034438753500580788   test_acc   :0.85\n",
      " epoch 100 num: 1: val loss=0.00691873580217361459\n",
      " epoch 100: train loss:0.0033761970297401323   train_acc: 0.8820043103448276\n",
      "          val   loss:0.0035341098305902313   val_acc   :0.8582589285714286\n",
      "          test   loss:0.0034294838551431894   test_acc   :0.86\n",
      " epoch 101 num: 1: val loss=0.00688279233872890599\n",
      " epoch 101: train loss:0.0033608828831849426   train_acc: 0.8793103448275862\n",
      "          val   loss:0.003513790400964873   val_acc   :0.8571428571428571\n",
      "          test   loss:0.0034739580005407333   test_acc   :0.86\n",
      " epoch 102 num: 1: val loss=0.00687673501670360649\n",
      " epoch 102: train loss:0.003363821928485714   train_acc: 0.8814655172413793\n",
      "          val   loss:0.0035205549294395106   val_acc   :0.8627232142857143\n",
      "          test   loss:0.0034578919876366854   test_acc   :0.85\n",
      "best acc updated!\n",
      " epoch 103 num: 1: val loss=0.00688801729120314182\n",
      " epoch 103: train loss:0.0033807135508235157   train_acc: 0.8782327586206896\n",
      "          val   loss:0.003512323834002018   val_acc   :0.8604910714285714\n",
      "          test   loss:0.0034852041862905025   test_acc   :0.86\n",
      " epoch 104 num: 1: val loss=0.00688040233217179895\n",
      " epoch 104: train loss:0.0033522078783476145   train_acc: 0.8822737068965517\n",
      "          val   loss:0.003512624484886016   val_acc   :0.859375\n",
      "          test   loss:0.0034950224217027426   test_acc   :0.86\n",
      " epoch 105 num: 1: val loss=0.00689848605543375125\n",
      " epoch 105: train loss:0.003374585873800619   train_acc: 0.8776939655172413\n",
      "          val   loss:0.0035126169345208575   val_acc   :0.8582589285714286\n",
      "          test   loss:0.0034810397773981094   test_acc   :0.86\n",
      " epoch 106 num: 1: val loss=0.00691578094847500361\n",
      " epoch 106: train loss:0.0033567558315678917   train_acc: 0.8820043103448276\n",
      "          val   loss:0.003520317708275148   val_acc   :0.8604910714285714\n",
      "          test   loss:0.0034519964829087257   test_acc   :0.86\n",
      " epoch 107 num: 1: val loss=0.00690085557289421663\n",
      " epoch 107: train loss:0.0033405868352615626   train_acc: 0.8841594827586207\n",
      "          val   loss:0.003515770625589149   val_acc   :0.8582589285714286\n",
      "          test   loss:0.0034763305447995663   test_acc   :0.86\n",
      " epoch 108 num: 1: val loss=0.00687717506662011153\n",
      " epoch 108: train loss:0.0033824818112469955   train_acc: 0.8793103448275862\n",
      "          val   loss:0.0035121358066265073   val_acc   :0.859375\n",
      "          test   loss:0.0034557993058115244   test_acc   :0.86\n",
      " epoch 109 num: 1: val loss=0.00689011975191533617\n",
      " epoch 109: train loss:0.0033315060467555605   train_acc: 0.8871228448275862\n",
      "          val   loss:0.0035059910068022354   val_acc   :0.859375\n",
      "          test   loss:0.0034736052621155977   test_acc   :0.86\n",
      " epoch 110 num: 1: val loss=0.00695423712022602662\n",
      " epoch 110: train loss:0.003389168474502091   train_acc: 0.8755387931034483\n",
      "          val   loss:0.0035322379719998154   val_acc   :0.8571428571428571\n",
      "          test   loss:0.0033524087630212307   test_acc   :0.88\n",
      " epoch 111 num: 1: val loss=0.00697116577066481193\n",
      " epoch 111: train loss:0.0033596091871631556   train_acc: 0.8795797413793104\n",
      "          val   loss:0.0035287596484912293   val_acc   :0.8571428571428571\n",
      "          test   loss:0.0033837517257779837   test_acc   :0.87\n",
      " epoch 112 num: 1: val loss=0.00697945780120790082\n",
      " epoch 112: train loss:0.0033319797527430386   train_acc: 0.8838900862068966\n",
      "          val   loss:0.0035352714559329407   val_acc   :0.8549107142857143\n",
      "          test   loss:0.0033986279740929604   test_acc   :0.87\n",
      " epoch 113 num: 1: val loss=0.0068882070481777197\n",
      " epoch 113: train loss:0.0033348708349312172   train_acc: 0.8852370689655172\n",
      "          val   loss:0.003498531279287168   val_acc   :0.8616071428571429\n",
      "          test   loss:0.0034909227397292852   test_acc   :0.86\n",
      " epoch 114 num: 1: val loss=0.00697535672225058126\n",
      " epoch 114: train loss:0.0033287290114395575   train_acc: 0.8884698275862069\n",
      "          val   loss:0.0035260984940188272   val_acc   :0.859375\n",
      "          test   loss:0.0034226190764456987   test_acc   :0.85\n",
      " epoch 115 num: 1: val loss=0.00697858817875385363\n",
      " epoch 115: train loss:0.0033333924807736584   train_acc: 0.8855064655172413\n",
      "          val   loss:0.0035299289905067   val_acc   :0.8549107142857143\n",
      "          test   loss:0.0033561275340616703   test_acc   :0.88\n",
      " epoch 116 num: 1: val loss=0.00692610931582748909\n",
      " epoch 116: train loss:0.0033258945873842157   train_acc: 0.8855064655172413\n",
      "          val   loss:0.003507329383864999   val_acc   :0.8627232142857143\n",
      "          test   loss:0.0034441486932337284   test_acc   :0.88\n",
      " epoch 117 num: 1: val loss=0.00690679694525897566\n",
      " epoch 117: train loss:0.0033208491473362364   train_acc: 0.8855064655172413\n",
      "          val   loss:0.003497041529044509   val_acc   :0.8627232142857143\n",
      "          test   loss:0.0034440415911376476   test_acc   :0.87\n",
      " epoch 118 num: 1: val loss=0.0068991605658084151\n",
      " epoch 118: train loss:0.0033513176383386397   train_acc: 0.8811961206896551\n",
      "          val   loss:0.003500821900420955   val_acc   :0.8616071428571429\n",
      "          test   loss:0.0034653597977012396   test_acc   :0.85\n",
      " epoch 119 num: 1: val loss=0.0068864224012941128\n",
      " epoch 119: train loss:0.00335204125992183   train_acc: 0.8817349137931034\n",
      "          val   loss:0.0035014989719327006   val_acc   :0.8616071428571429\n",
      "          test   loss:0.003499095095321536   test_acc   :0.86\n",
      " epoch 120 num: 1: val loss=0.006862282520160079026\n",
      " epoch 120: train loss:0.0033131472220451668   train_acc: 0.8876616379310345\n",
      "          val   loss:0.0034952868840524127   val_acc   :0.8649553571428571\n",
      "          test   loss:0.003531702794134617   test_acc   :0.86\n",
      "best acc updated!\n",
      " epoch 121 num: 1: val loss=0.00690180924721062242\n",
      " epoch 121: train loss:0.0033157106633458673   train_acc: 0.888739224137931\n",
      "          val   loss:0.003514018275641969   val_acc   :0.8604910714285714\n",
      "          test   loss:0.0034867795184254646   test_acc   :0.85\n",
      " epoch 122 num: 1: val loss=0.00700452108867466454\n",
      " epoch 122: train loss:0.0032986840393779606   train_acc: 0.8900862068965517\n",
      "          val   loss:0.0035443632597369806   val_acc   :0.8493303571428571\n",
      "          test   loss:0.003351039020344615   test_acc   :0.88\n",
      " epoch 123 num: 1: val loss=0.0069105052389204546\n",
      " epoch 123: train loss:0.003329679353869167   train_acc: 0.8855064655172413\n",
      "          val   loss:0.0035035734597061363   val_acc   :0.8616071428571429\n",
      "          test   loss:0.0035203283187001944   test_acc   :0.86\n",
      " epoch 124 num: 1: val loss=0.0069203854072839024\n",
      " epoch 124: train loss:0.0033329934491936504   train_acc: 0.8841594827586207\n",
      "          val   loss:0.0035042823957545416   val_acc   :0.8582589285714286\n",
      "          test   loss:0.003484078450128436   test_acc   :0.85\n",
      " epoch 125 num: 1: val loss=0.00702299410477280655\n",
      " epoch 125: train loss:0.0033111658172103866   train_acc: 0.8876616379310345\n",
      "          val   loss:0.003554810304194689   val_acc   :0.8504464285714286\n",
      "          test   loss:0.0033279031049460173   test_acc   :0.88\n",
      " epoch 126 num: 1: val loss=0.00701015652157366315\n",
      " epoch 126: train loss:0.0032837757648065172   train_acc: 0.8938577586206896\n",
      "          val   loss:0.003530248301103711   val_acc   :0.8537946428571429\n",
      "          test   loss:0.003372738603502512   test_acc   :0.88\n",
      " epoch 127 num: 1: val loss=0.00701278052292764265\n",
      " epoch 127: train loss:0.0033144350083352163   train_acc: 0.8882004310344828\n",
      "          val   loss:0.003522000209029232   val_acc   :0.8616071428571429\n",
      "          test   loss:0.0034021029714494944   test_acc   :0.87\n",
      " epoch 128 num: 1: val loss=0.006998587399721146005\n",
      " epoch 128: train loss:0.003324433478751573   train_acc: 0.8868534482758621\n",
      "          val   loss:0.0035126651637256145   val_acc   :0.8616071428571429\n",
      "          test   loss:0.003501774976029992   test_acc   :0.85\n",
      " epoch 129 num: 1: val loss=0.007025297731161117605\n",
      " epoch 129: train loss:0.003348993635254687   train_acc: 0.8814655172413793\n",
      "          val   loss:0.003528115140008075   val_acc   :0.859375\n",
      "          test   loss:0.003455117344856262   test_acc   :0.85\n",
      " epoch 130 num: 1: val loss=0.00699440692551434664\n",
      " epoch 130: train loss:0.003322131289490338   train_acc: 0.8890086206896551\n",
      "          val   loss:0.0035149268473365475   val_acc   :0.8549107142857143\n",
      "          test   loss:0.0034349856432527304   test_acc   :0.87\n",
      " epoch 131 num: 1: val loss=0.00697289290837943556\n",
      " epoch 131: train loss:0.0033257909054900036   train_acc: 0.8882004310344828\n",
      "          val   loss:0.003503955069131085   val_acc   :0.8616071428571429\n",
      "          test   loss:0.003444035304710269   test_acc   :0.86\n",
      " epoch 132 num: 1: val loss=0.00695512606762349669\n",
      " epoch 132: train loss:0.003324415799679941   train_acc: 0.8868534482758621\n",
      "          val   loss:0.0035030505886035307   val_acc   :0.8604910714285714\n",
      "          test   loss:0.003481952240690589   test_acc   :0.86\n",
      " epoch 133 num: 1: val loss=0.00687557645142078472\n",
      " epoch 133: train loss:0.0032896939343933403   train_acc: 0.8895474137931034\n",
      "          val   loss:0.0034894702423896107   val_acc   :0.8627232142857143\n",
      "          test   loss:0.0034890822134912014   test_acc   :0.86\n",
      " epoch 134 num: 1: val loss=0.00689956825226545356\n",
      " epoch 134: train loss:0.0033116083720634722   train_acc: 0.8860452586206896\n",
      "          val   loss:0.003487588438604559   val_acc   :0.8604910714285714\n",
      "          test   loss:0.003473997348919511   test_acc   :0.86\n",
      " epoch 135 num: 1: val loss=0.00688566151075065158\n",
      " epoch 135: train loss:0.0032769969889316066   train_acc: 0.8933189655172413\n",
      "          val   loss:0.003489176875778607   val_acc   :0.8627232142857143\n",
      "          test   loss:0.0034307625610381365   test_acc   :0.86\n",
      " epoch 136 num: 1: val loss=0.00688097579404711725\n",
      " epoch 136: train loss:0.003273533448448469   train_acc: 0.8930495689655172\n",
      "          val   loss:0.0034935527614184786   val_acc   :0.8638392857142857\n",
      "          test   loss:0.0034431966487318277   test_acc   :0.86\n",
      " epoch 137 num: 1: val loss=0.00688077299855649516\n",
      " epoch 137: train loss:0.003259706391214297   train_acc: 0.8949353448275862\n",
      "          val   loss:0.0034891848918050528   val_acc   :0.8627232142857143\n",
      "          test   loss:0.003473263466730714   test_acc   :0.86\n",
      " epoch 138 num: 1: val loss=0.00684316386468708519\n",
      " epoch 138: train loss:0.003267990184369786   train_acc: 0.8949353448275862\n",
      "          val   loss:0.0034742228287671295   val_acc   :0.8627232142857143\n",
      "          test   loss:0.003466594498604536   test_acc   :0.86\n",
      " epoch 139 num: 1: val loss=0.00684241065755486502\n",
      " epoch 139: train loss:0.003244017282950467   train_acc: 0.8970905172413793\n",
      "          val   loss:0.0034713762413178173   val_acc   :0.8627232142857143\n",
      "          test   loss:0.003472560318186879   test_acc   :0.86\n",
      " epoch 140 num: 1: val loss=0.00689333537593483929\n",
      " epoch 140: train loss:0.003260750572421941   train_acc: 0.8938577586206896\n",
      "          val   loss:0.0034885702521673272   val_acc   :0.8616071428571429\n",
      "          test   loss:0.0034505908843129873   test_acc   :0.86\n",
      " epoch 141 num: 1: val loss=0.00697657116688787986\n",
      " epoch 141: train loss:0.003262597979206977   train_acc: 0.8954741379310345\n",
      "          val   loss:0.0035112303282533374   val_acc   :0.859375\n",
      "          test   loss:0.003375479020178318   test_acc   :0.88\n",
      " epoch 142 num: 1: val loss=0.00697344774380326396\n",
      " epoch 142: train loss:0.003282081456212648   train_acc: 0.8914331896551724\n",
      "          val   loss:0.0035139316293810096   val_acc   :0.8582589285714286\n",
      "          test   loss:0.003365937387570739   test_acc   :0.88\n",
      " epoch 143 num: 1: val loss=0.00688471435569226706\n",
      " epoch 143: train loss:0.0032457218842645145   train_acc: 0.8976293103448276\n",
      "          val   loss:0.0034822004714182447   val_acc   :0.8649553571428571\n",
      "          test   loss:0.0034017162397503853   test_acc   :0.88\n",
      " epoch 144 num: 1: val loss=0.00690635526552796423\n",
      " epoch 144: train loss:0.0032540888461316453   train_acc: 0.8946659482758621\n",
      "          val   loss:0.003488697011822036   val_acc   :0.8627232142857143\n",
      "          test   loss:0.0034096413291990757   test_acc   :0.87\n",
      " epoch 145 num: 1: val loss=0.0069978951942175634\n",
      " epoch 145: train loss:0.0032727892976254225   train_acc: 0.8943965517241379\n",
      "          val   loss:0.003530387034905808   val_acc   :0.8571428571428571\n",
      "          test   loss:0.003344099037349224   test_acc   :0.87\n",
      " epoch 146 num: 1: val loss=0.00697906361892819402\n",
      " epoch 146: train loss:0.003255128715958061   train_acc: 0.8946659482758621\n",
      "          val   loss:0.003516625579712646   val_acc   :0.8582589285714286\n",
      "          test   loss:0.0033701511565595865   test_acc   :0.88\n",
      " epoch 147 num: 1: val loss=0.00686547881923615952\n",
      " epoch 147: train loss:0.0032643337396840596   train_acc: 0.8935883620689655\n",
      "          val   loss:0.003470618909757052   val_acc   :0.8649553571428571\n",
      "          test   loss:0.0034039008896797895   test_acc   :0.87\n",
      " epoch 148 num: 1: val loss=0.00700954277999699181\n",
      " epoch 148: train loss:0.003240371114927633   train_acc: 0.8992456896551724\n",
      "          val   loss:0.0035177110029118402   val_acc   :0.8582589285714286\n",
      "          test   loss:0.0033579564187675714   test_acc   :0.87\n",
      " epoch 149 num: 1: val loss=0.00689595798030495649\n",
      " epoch 149: train loss:0.0032448975192704075   train_acc: 0.8968211206896551\n",
      "          val   loss:0.0034768169945372   val_acc   :0.8660714285714286\n",
      "          test   loss:0.0033832178451120853   test_acc   :0.88\n",
      "best acc updated!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pre_bert.eval()\n",
    "train_pred_bert=[];val_pred_bert=[];test_pred_bert=[]\n",
    "with torch.no_grad():\n",
    "   \n",
    "    for ind,(data,y,token) in enumerate(train_dataloader):\n",
    "        train_pred_bert.append(pre_bert(token[0].to(device), token[1].to(device),token[3].to(device)))\n",
    "    train_pred_bert=tensor2np(train_pred_bert)\n",
    "    for ind,(data,y,token) in enumerate(val_dataloader):\n",
    "        val_pred_bert.append(pre_bert(token[0].to(device), token[1].to(device),token[3].to(device)))\n",
    "    val_pred_bert=tensor2np(val_pred_bert)\n",
    "    for ind,(data,y,token) in enumerate(test_dataloader):\n",
    "        test_pred_bert.append(pre_bert(token[0].to(device), token[1].to(device),token[3].to(device)))\n",
    "    test_pred_bert=tensor2np(test_pred_bert)\n",
    "\n",
    "\n",
    "\n",
    "epoch=150\n",
    "\n",
    "print(\"using \"+str(device.type)+\" \"+str(device.index)+\" to train...\")\n",
    "acc=[];best_acc=0\n",
    "train_pred_cnn=[];val_pred_cnn=[];test_pred_cnn=[];mouse_pred_cnn=[]\n",
    "for i in range(epoch):\n",
    "    net.train()\n",
    "    train_pred_cnn_tmp=[];test_pred_cnn_tmp=[];val_pred_cnn_tmp=[]\n",
    "    train_loss=[];train_acc=[]\n",
    "    val_acc=[];val_loss=[]  \n",
    "\n",
    "    test_acc=[];test_loss=[]\n",
    "    mouse_acc=[]\n",
    "    correct = 0\n",
    "    correct_test=0\n",
    "    correct_train=0\n",
    "    correct_mouse=0\n",
    "    for ind,(data,y,token) in enumerate(train_dataloader):\n",
    "        W,X,Y,Z,M,N=data\n",
    "        W=W.permute(0,2,1)\n",
    "        X=X.permute(0,2,1)\n",
    "        Y=Y.permute(0,2,1)\n",
    "        Z=Z.permute(0,2,1)\n",
    "        W=W.to(device)\n",
    "        X=X.to(device)\n",
    "        Y=Y.to(device)\n",
    "        Z=Z.to(device)\n",
    "        M=M.to(device)\n",
    "        N=N.to(device)\n",
    "        y=y.to(device)\n",
    "        y_pred=net(W,X,Y,Z,M,N)\n",
    "        train_pred_cnn_tmp.append(y_pred)\n",
    "        # print(avg_tensor.shape)\n",
    "        #y_pred=y_pred.squeeze(1)\n",
    "        loss=loss_fn(y_pred,y.long())\n",
    "        correct_train += (torch.argmax(y_pred,axis=1)== y).sum().item()\n",
    "        #train_acc.append(cal_acc(y_pred.to(\"cpu\").detach().numpy(),y.to(\"cpu\").detach().numpy()))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.tolist())\n",
    "        #lr_scheduler.step()\n",
    "        if(ind%49==1):\n",
    "            sys.stdout.write(\"\\r epoch \"+str(i)+\" num: \"+str(ind)+\": train loss=\"+str(sum(train_loss)/(BATCH_SIZE*ind)))\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "    net.eval()\n",
    "    for ind,(data,y,token) in enumerate(val_dataloader):\n",
    "        W,X,Y,Z,M,N=data\n",
    "        W=W.permute(0,2,1)\n",
    "        X=X.permute(0,2,1)\n",
    "        Y=Y.permute(0,2,1)\n",
    "        Z=Z.permute(0,2,1)\n",
    "        W=W.to(device)\n",
    "        X=X.to(device)\n",
    "        Y=Y.to(device)\n",
    "        Z=Z.to(device)\n",
    "        M=M.to(device)\n",
    "        N=N.to(device)\n",
    "        y=y.to(device)\n",
    "        y_pred=net(W,X,Y,Z,M,N)\n",
    "        val_pred_cnn_tmp.append(y_pred)\n",
    "        # print(avg_tensor.shape)\n",
    "        #y_pred=y_pred.squeeze(1)\n",
    "        loss=loss_fn(y_pred,y.long())\n",
    "        correct += (torch.argmax(y_pred,axis=1) == y).sum().item()\n",
    "        val_loss.append(loss.tolist())\n",
    "        if(ind%49==1):\n",
    "            sys.stdout.write(\"\\r epoch \"+str(i)+\" num: \"+str(ind)+\": val loss=\"+str(sum(val_loss)/(BATCH_SIZE*ind)))\n",
    "            sys.stdout.flush()\n",
    "    print(\"\\n epoch \"+str(i)+\": train loss:\"+str(sum(train_loss)/(BATCH_SIZE*len(train_dataloader))),\"  train_acc: \"+str(correct_train/(BATCH_SIZE*len(train_dataloader))))\n",
    "    if(i%1==0):\n",
    "\n",
    "        print(\"          val   loss:\"+str(sum(val_loss)/(BATCH_SIZE*len(val_dataloader))),\"  val_acc   :\"+str(correct/(BATCH_SIZE*len(val_dataloader))))\n",
    "        acc.append(correct_train/(BATCH_SIZE*len(train_dataloader)))\n",
    "\n",
    "    for ind,(data,y,token) in enumerate(test_dataloader):\n",
    "        W,X,Y,Z,M,N=data\n",
    "        W=W.permute(0,2,1)\n",
    "        X=X.permute(0,2,1)\n",
    "        Y=Y.permute(0,2,1)\n",
    "        Z=Z.permute(0,2,1)\n",
    "        W=W.to(device)\n",
    "        X=X.to(device)\n",
    "        Y=Y.to(device)\n",
    "        Z=Z.to(device)\n",
    "        M=M.to(device)\n",
    "        N=N.to(device)\n",
    "        y=y.to(device)\n",
    "        y_pred=net(W,X,Y,Z,M,N)\n",
    "        test_pred_cnn_tmp.append(y_pred)\n",
    "        # print(avg_tensor.shape)\n",
    "        loss=loss_fn(y_pred,y.long())\n",
    "\n",
    "        correct_test += (torch.argmax(y_pred,axis=1) == y).sum().item()\n",
    "        test_loss.append(loss.tolist())\n",
    "      \n",
    "    print(\"          test   loss:\"+str(sum(test_loss)/(BATCH_SIZE*len(test_dataloader))),\"  test_acc   :\"+str(correct_test/len(test_data)))\n",
    "   \n",
    "    if(correct/(BATCH_SIZE*len(val_dataloader))>best_acc):\n",
    "        print(\"best acc updated!\")\n",
    "        best_acc=correct/(BATCH_SIZE*len(val_dataloader))\n",
    "        val_pred_cnn=val_pred_cnn_tmp;train_pred_cnn=train_pred_cnn_tmp;test_pred_cnn=test_pred_cnn_tmp\n",
    "\n",
    "        torch.save(net,\"./results/your_models/cnn/\"+mod_type+\".pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_cnn=tensor2np2(train_pred_cnn)\n",
    "val_pred_cnn=tensor2np2(val_pred_cnn)\n",
    "test_pred_cnn=tensor2np2(test_pred_cnn)\n",
    "train_pred=np.hstack((train_pred_cnn,train_pred_bert))\n",
    "val_pred=np.hstack((val_pred_cnn,val_pred_bert))\n",
    "test_pred=np.hstack((test_pred_cnn,test_pred_bert))\n",
    "X_train=pd.DataFrame(train_pred,columns=[\"A\",\"B\",\"C\",\"D\"])\n",
    "X_train[\"label\"]=train_data.iloc[:128 * (len(train_data) // 128)].label.values\n",
    "X_val=pd.DataFrame(val_pred,columns=[\"A\",\"B\",\"C\",\"D\"])\n",
    "X_val[\"label\"]=val_data.iloc[:128 * (len(val_data) // 128)].label.values\n",
    "X_test=pd.DataFrame(test_pred,columns=[\"A\",\"B\",\"C\",\"D\"])\n",
    "X_test[\"label\"]=test_data.label.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc :0.751953125\n",
      "val_acc :0.7529296875\n",
      "val_acc :0.7587890625\n",
      "val_acc :0.7568359375\n",
      "val_acc :0.7578125\n",
      "val_acc :0.7578125\n",
      "val_acc :0.7568359375\n",
      "val_acc :0.755859375\n",
      "val_acc :0.7548828125\n",
      "val_acc :0.7548828125\n",
      "ACC: 88.0000%\n",
      "Sn : 84.0000%\n",
      "Sp : 92.0000%\n",
      "MCC: 76.2444%\n",
      "F1 : 0.8749999999999999 \n",
      "AUC: 0.9608 \n",
      "precision: 0.9130434782608695 \n",
      "recall: 0.84\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 10)   # 输入层到隐藏层的线性变换\n",
    "        self.bn1 = nn.BatchNorm1d(10)\n",
    "        self.fc2 = nn.Linear(10, 1)   # 隐藏层到输出层的线性变换\n",
    "        self.sigmoid = nn.Sigmoid()   # 输出层激活函数\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)   # 隐藏层使用ReLU激活函数\n",
    "        x=self.bn1(x)\n",
    "        x = self.sigmoid(self.fc2(x)) # 输出层使用Sigmoid激活函数\n",
    "        return x\n",
    "\n",
    "# 实例化模型\n",
    "remix = Net()\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(remix.parameters(), lr=0.01)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Remix_Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super(Remix_Dataset, self).__init__()\n",
    "        self.data = data.drop(columns=[\"label\"]).values\n",
    "        self.labels = data.label.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = torch.tensor(self.data[index], dtype=torch.float32)\n",
    "        y = torch.tensor(self.labels[index], dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "# 定义数据集\n",
    "\n",
    "train_dataset_ = Remix_Dataset(X_train)\n",
    "val_dataset_ = Remix_Dataset(X_val)\n",
    "test_dataset_ = Remix_Dataset(X_test)\n",
    "\n",
    "# 定义数据加载器\n",
    "batch_size = 256\n",
    "train_dataloader_ = DataLoader(train_dataset_, batch_size=batch_size, shuffle=False)\n",
    "val_dataloader_ = DataLoader(val_dataset_, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader_ = DataLoader(test_dataset_, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for i in range(10):\n",
    "# 遍历数据加载器\n",
    "    val_acc=0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_dataloader_):\n",
    "        optimizer.zero_grad()  # 清除梯度\n",
    "        outputs = remix(inputs) # 前向传播\n",
    "        loss = criterion(outputs.squeeze(1), targets) # 计算损失\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step() # 更新参数\n",
    "       \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_dataloader_):\n",
    "            outputs = remix(inputs)\n",
    "            val_acc+=(int)(torch.sum((outputs.squeeze(1).round()==targets)))\n",
    "# 训练模型\n",
    "        print(\"val_acc :\"+str(val_acc/(batch_size*len(val_dataloader_))))\n",
    "final_pred=[]\n",
    "final_prob=[]\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (inputs, targets) in enumerate(test_dataloader_):\n",
    "        outputs = remix(inputs)\n",
    "        final_pred+=list(outputs.squeeze(1).round().numpy())\n",
    "        final_prob+=list(outputs.squeeze(1).numpy())\n",
    "\n",
    "torch.save(remix,\"./results/your_models/combine/\"+mod_type+\".pt\")\n",
    "from utils import matrix2\n",
    "res=matrix2(final_pred,X_test.label.values,final_prob)\n",
    "pd.DataFrame(res).to_csv(\"./results/tables/\"+mod_type+\".csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans.\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif\n",
      "findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans.\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif\n",
      "findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans.\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Arial, Liberation Sans, Bitstream Vera Sans, sans-serif\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, thread = roc_curve(X_test.label.values, final_prob)\n",
    "roc_auc= auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve of '+ mod_type)\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "if os.path.exists(\"./results/tables/img\"):\n",
    "    print(\"目录已存在\")\n",
    "else:\n",
    "    os.mkdir(\"./results/tables/img\")\n",
    "plt.savefig(\"./results/tables/img/roc_curve_\"+mod_type+\".pdf\", format='pdf',bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0ba6ecac705ffa826d0bb66bcf871efc7a82d3b0b3e0fe638798478dd38494eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
